id,commit_id,file_path,status,additions,deletions,changes,patch,previous_filename,file_type,blob_before_local_path,blob_after_local_path,metadata
7e6fbb24-0195-4116-a837-edaf2b8b0055,bbc82319-7a27-46f3-b582-de74b5be8bc1,README.md,modified,1,1,2,"@@ -61,7 +61,7 @@ HelloGitHub 分享 GitHub 上有趣、入门级的开源项目。**每月 28 号
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
df1b02eb-01bd-4945-b454-34bd24e8c6bd,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub01.md,modified,1,1,2,"@@ -137,7 +137,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
f715b5e6-4e2c-44fc-83d1-a6a698f7377b,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub02.md,modified,1,1,2,"@@ -177,7 +177,7 @@ if __name__ == '__main__':
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
448044a8-34b5-4f2c-90e4-aa3816d8aec2,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub03.md,modified,1,1,2,"@@ -106,7 +106,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
eda96c26-5179-476c-90ec-4df309c4245c,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub04.md,modified,1,1,2,"@@ -118,7 +118,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
e1b327ff-9e03-4bf8-989a-c4a0ac12839b,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub05.md,modified,1,1,2,"@@ -135,7 +135,7 @@ $pinyin->convert('带着希望去旅行，比到达终点更美好', PINYIN_ASCI
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
c0a19d8e-41f4-4e33-b545-a8a3c0bafd0f,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub06.md,modified,1,1,2,"@@ -158,7 +158,7 @@ brew install mercurial
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
59ba265f-7a0f-457b-8717-bea75a3bb1d2,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub07.md,modified,1,1,2,"@@ -184,7 +184,7 @@ print langid.classify(text2)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
cf827e71-d14a-4a88-ba5b-9240e3264c20,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub08.md,modified,1,1,2,"@@ -250,7 +250,7 @@ finally:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
021fcbc4-7f43-4835-b44c-b3c0899aaca5,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub09.md,modified,1,1,2,"@@ -175,7 +175,7 @@ ngrok http 8000
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
e147dcc7-3690-452d-b0c4-2957578dbcc1,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub10.md,modified,1,1,2,"@@ -189,7 +189,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
2a025c48-4dcb-425e-b661-b7ca140ef7ed,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub100.md,modified,1,1,2,"@@ -325,7 +325,7 @@ scoop install python ruby go perl
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
304eaf09-c014-4eb4-9a7c-e077f5a25fd8,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub101.md,modified,1,1,2,"@@ -351,7 +351,7 @@ print(response)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
bfcd61fb-fed7-40cd-b481-cc4ebf663d10,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub102.md,modified,1,1,2,"@@ -313,7 +313,7 @@ output = compiled_model({0: example.numpy()})
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
2bd3f6da-8266-410b-a1a0-811af8714225,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub103.md,modified,1,1,2,"@@ -379,7 +379,7 @@ path = model.export(format=""onnx"")  # return path to exported model
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
9272b672-7a18-4c84-9c3b-58de373356cf,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub104.md,modified,1,1,2,"@@ -279,7 +279,7 @@ cv2.imwrite(""vis_image.jpg"", vis_im)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
d3a120a7-dada-4254-88dc-59c76d10a8b9,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub105.md,modified,1,1,2,"@@ -321,7 +321,7 @@ const result = await zerox({
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
9ddce36d-3206-4446-8283-d157f8774863,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub106.md,modified,1,1,2,"@@ -432,7 +432,7 @@ Render( Point(0, 0), 50, 120, 120, 300, 800, 600 )
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
310af748-baab-4a74-b5e2-126b96599b0e,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub107.md,modified,1,1,2,"@@ -252,7 +252,7 @@ if num := v.Export().(int64); num != 4 {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
594a70f1-00ac-44a0-8285-cc680955b582,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub108.md,modified,1,1,2,"@@ -289,7 +289,7 @@ ret = tf_fn(x1)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
49d70fbf-9821-4ad3-8d2c-5d799fbf7222,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub109.md,modified,1,1,2,"@@ -302,7 +302,7 @@ jsonpath ""$.id"" matches /\d{4}/     # Check the format of the id
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
d3c80458-a639-46b3-94ee-7b5c19062768,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub11.md,modified,1,1,2,"@@ -176,7 +176,7 @@ print ifconfig(""eth0"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
b37aae68-9350-45f7-bd16-541a689cd98d,bbc82319-7a27-46f3-b582-de74b5be8bc1,content/HelloGitHub110.md,modified,1,1,2,"@@ -295,7 +295,7 @@ if __name__ == ""__main__"":
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub>七牛云</sub><br>
           <sub>百万 Token 免费体验</sub>",,md,,,{}
0a96fba9-f185-4917-9b58-ae9727f8a136,93ef1de8-d00f-4f39-bcd6-3acdd46740a8,docs/zh/docs/python-types.md,modified,1,1,2,"@@ -228,7 +228,7 @@ John Doe
 
 ## Pydantic 模型
 
-<a href=""https://docs.pydantic.dev/"" class=""external-link"" target=""_blank"">Pydantic</a> 是一个用来用来执行数据校验的 Python 库。
+<a href=""https://docs.pydantic.dev/"" class=""external-link"" target=""_blank"">Pydantic</a> 是一个用来执行数据校验的 Python 库。
 
 你可以将数据的""结构""声明为具有属性的类。
 ",,md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\fastapi\fastapi\4b12a2818a7eb4ac45da8afedeb29d7030104f03\before\docs_zh_docs_python-types.md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\fastapi\fastapi\4b12a2818a7eb4ac45da8afedeb29d7030104f03\after\docs_zh_docs_python-types.md,{}
1c2162aa-2ba6-4500-913b-37012f3e2c9a,f725793f-aea1-458b-95c8-50ff777db5db,src/transformers/models/qwen2_audio/processing_qwen2_audio.py,modified,1,1,2,"@@ -248,7 +248,7 @@ def default_chat_template(self):
                     ""{{ message['content'] }}<|im_end|>\n""
                 ""{% else %}""
                     ""{% for content in message['content'] %}""
-                        ""{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}""
+                        ""{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' or content['type'] == 'audio' %}""
                             ""{% set audio_count.value = audio_count.value + 1 %}""
                             ""Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\n""
                         ""{% elif 'text' in content %}""",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\d7b87b415a5dd4a3152051e1a0abd098a02c5bfa\before\src_transformers_models_qwen2_audio_processing_qwen2_audio.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\d7b87b415a5dd4a3152051e1a0abd098a02c5bfa\after\src_transformers_models_qwen2_audio_processing_qwen2_audio.py,{}
818f0a00-e919-4cb8-b0eb-dbc6a7ab4a9b,7f59fa07-03a0-4a22-a958-7efe8faabf58,src/transformers/models/auto/modeling_auto.py,modified,0,5,5,"@@ -1529,11 +1529,6 @@
 MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(
     [
         (""sam"", ""SamModel""),
-    ]
-)
-
-MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(
-    [
         (""sam_hq"", ""SamHQModel""),
     ]
 )",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\237ff80387ef453b1bb341c0b5d6cbcb5e60758d\before\src_transformers_models_auto_modeling_auto.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\237ff80387ef453b1bb341c0b5d6cbcb5e60758d\after\src_transformers_models_auto_modeling_auto.py,{}
0898fb6b-f83f-495c-acbb-fae7a36ccc48,71efd503-edb0-4afe-909f-873cca442bb7,docs/source/en/internal/import_utils.md,modified,1,1,2,"@@ -38,7 +38,7 @@ However, no method can be called on that object:
 ```python
 >>> DetrImageProcessorFast.from_pretrained()
 ImportError: 
-DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Checkout the instructions on the
+DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 ```",,md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\docs_source_en_internal_import_utils.md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\docs_source_en_internal_import_utils.md,{}
75666937-25dc-4a34-af3c-506d670dbcfe,71efd503-edb0-4afe-909f-873cca442bb7,examples/flax/question-answering/run_qa.py,modified,1,1,2,"@@ -546,7 +546,7 @@ def main():
     # region Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_flax_question-answering_run_qa.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_flax_question-answering_run_qa.py,{}
56fd46e2-eeb1-44ad-b56c-e2c157ae65de,71efd503-edb0-4afe-909f-873cca442bb7,examples/modular-transformers/configuration_my_new_model.py,modified,1,1,2,"@@ -36,7 +36,7 @@ class MyNewModelConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_modular-transformers_configuration_my_new_model.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_modular-transformers_configuration_my_new_model.py,{}
559e461b-58ea-45d6-ada1-839ecacc2634,71efd503-edb0-4afe-909f-873cca442bb7,examples/modular-transformers/configuration_new_model.py,modified,1,1,2,"@@ -34,7 +34,7 @@ class NewModelConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_modular-transformers_configuration_new_model.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_modular-transformers_configuration_new_model.py,{}
bb5f7f30-f1dd-44f8-8512-462d0fc60055,71efd503-edb0-4afe-909f-873cca442bb7,examples/pytorch/question-answering/run_qa.py,modified,1,1,2,"@@ -357,7 +357,7 @@ def main():
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_pytorch_question-answering_run_qa.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_pytorch_question-answering_run_qa.py,{}
a38030f0-3349-4b74-aa11-755fe95ede66,71efd503-edb0-4afe-909f-873cca442bb7,examples/pytorch/token-classification/run_ner.py,modified,1,1,2,"@@ -399,7 +399,7 @@ def get_label_list(labels):
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_pytorch_token-classification_run_ner.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_pytorch_token-classification_run_ner.py,{}
8b562743-87b1-484a-8be9-99135750a9b1,71efd503-edb0-4afe-909f-873cca442bb7,examples/tensorflow/question-answering/run_qa.py,modified,1,1,2,"@@ -378,7 +378,7 @@ def main():
     # region Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_tensorflow_question-answering_run_qa.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_tensorflow_question-answering_run_qa.py,{}
aea7fef9-a479-424b-955e-f2ae3e08001f,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/aria/configuration_aria.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class AriaTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_aria_configuration_aria.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_aria_configuration_aria.py,{}
643e145d-6334-4c20-adb5-0e4ff42765d5,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/aria/modular_aria.py,modified,1,1,2,"@@ -120,7 +120,7 @@ class AriaTextConfig(LlamaConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_aria_modular_aria.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_aria_modular_aria.py,{}
d36e463a-25fc-4c5a-a462-4940672a4a91,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/bamba/configuration_bamba.py,modified,1,1,2,"@@ -53,7 +53,7 @@ class BambaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_bamba_configuration_bamba.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_bamba_configuration_bamba.py,{}
96854a4c-a1f0-411a-bead-b6c82fade780,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/bitnet/configuration_bitnet.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class BitNetConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""relu2""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_bitnet_configuration_bitnet.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_bitnet_configuration_bitnet.py,{}
fc5fa74d-f173-4984-a79d-d3a4cf3e9fe9,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/chameleon/configuration_chameleon.py,modified,1,1,2,"@@ -125,7 +125,7 @@ class ChameleonConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_chameleon_configuration_chameleon.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_chameleon_configuration_chameleon.py,{}
06282318-b124-431c-9f8d-1f6ac3d76d75,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py,modified,1,1,2,"@@ -446,7 +446,7 @@ def main():
         ""--model_size"",
         choices=[""7B"", ""30B""],
         help=""""
-        "" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, checkout the original repo: https://github.com/facebookresearch/chameleon"",
+        "" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, check out the original repo: https://github.com/facebookresearch/chameleon"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_chameleon_convert_chameleon_weights_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_chameleon_convert_chameleon_weights_to_hf.py,{}
d9b67e22-7723-4a4f-83f6-ec0dda842b93,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/cohere/configuration_cohere.py,modified,1,1,2,"@@ -56,7 +56,7 @@ class CohereConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere_configuration_cohere.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere_configuration_cohere.py,{}
d861f512-9358-4051-8d8a-73c400b4c3f8,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/cohere2/configuration_cohere2.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class Cohere2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere2_configuration_cohere2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere2_configuration_cohere2.py,{}
e012ed9c-3934-4994-8a26-e0c3db8394c2,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/cohere2/modular_cohere2.py,modified,1,1,2,"@@ -74,7 +74,7 @@ class Cohere2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere2_modular_cohere2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere2_modular_cohere2.py,{}
2cdbb633-4f20-4eb8-80a0-83886c6e335d,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/csm/configuration_csm.py,modified,2,2,4,"@@ -54,7 +54,7 @@ class CsmDepthDecoderConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
@@ -235,7 +235,7 @@ class CsmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the backbone model Transformer decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_csm_configuration_csm.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_csm_configuration_csm.py,{}
fe4b324e-b8d1-4e39-8edc-51683bb0e8fe,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/deepseek_v3/configuration_deepseek_v3.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class DeepseekV3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         n_shared_experts (`int`, *optional*, defaults to 1):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_deepseek_v3_configuration_deepseek_v3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_deepseek_v3_configuration_deepseek_v3.py,{}
5106ab94-994c-440b-87ff-5b0759f7f160,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/diffllama/configuration_diffllama.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class DiffLlamaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_diffllama_configuration_diffllama.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_diffllama_configuration_diffllama.py,{}
f09f0bc9-31e0-4a01-9512-dc6e4a870a10,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/emu3/configuration_emu3.py,modified,1,1,2,"@@ -138,7 +138,7 @@ class Emu3TextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_emu3_configuration_emu3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_emu3_configuration_emu3.py,{}
066a53f9-edf3-4ca2-ab8b-bc923fd6b519,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/falcon_h1/configuration_falcon_h1.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class FalconH1Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_falcon_h1_configuration_falcon_h1.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_falcon_h1_configuration_falcon_h1.py,{}
b1dd41a9-a7c9-4d77-8d31-244ec386ae37,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma/configuration_gemma.py,modified,1,1,2,"@@ -47,7 +47,7 @@ class GemmaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_configuration_gemma.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_configuration_gemma.py,{}
7dcd80c0-6710-4230-b8b3-9f543073789a,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma/convert_gemma_weights_to_hf.py,modified,1,1,2,"@@ -151,7 +151,7 @@ def main():
         ""--model_size"",
         default=""7B"",
         choices=[""2B"", ""7B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_convert_gemma_weights_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_convert_gemma_weights_to_hf.py,{}
4028ed5b-9cc6-453b-b08e-524d22616c4d,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma/modular_gemma.py,modified,1,1,2,"@@ -74,7 +74,7 @@ class GemmaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_modular_gemma.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_modular_gemma.py,{}
419dfd07-62f5-4ae4-8b2a-18096d68cab2,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma2/configuration_gemma2.py,modified,1,1,2,"@@ -47,7 +47,7 @@ class Gemma2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_configuration_gemma2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_configuration_gemma2.py,{}
5c442ade-aa95-4b9e-a8a3-f38f69d9cea1,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py,modified,1,1,2,"@@ -184,7 +184,7 @@ def main():
         ""--model_size"",
         default=""9B"",
         choices=[""9B"", ""27B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_convert_gemma2_weights_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_convert_gemma2_weights_to_hf.py,{}
ea8c1dd4-562f-42c2-93e9-a56777553e24,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma2/modular_gemma2.py,modified,1,1,2,"@@ -71,7 +71,7 @@ class Gemma2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_modular_gemma2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_modular_gemma2.py,{}
85f9688d-3cd3-4fca-ba19-3ee716b8e622,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma3/configuration_gemma3.py,modified,1,1,2,"@@ -55,7 +55,7 @@ class Gemma3TextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma3_configuration_gemma3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma3_configuration_gemma3.py,{}
e02d08e4-c118-4f52-92b7-e130403b08fc,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/gemma3/modular_gemma3.py,modified,1,1,2,"@@ -82,7 +82,7 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma3_modular_gemma3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma3_modular_gemma3.py,{}
a6dc226b-c7df-4044-94ee-a11073044f14,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/glm/configuration_glm.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class GlmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_glm_configuration_glm.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_glm_configuration_glm.py,{}
3c99c3e6-f782-49a7-87fc-3be7aa8a1f3e,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/glm4/configuration_glm4.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class Glm4Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_glm4_configuration_glm4.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_glm4_configuration_glm4.py,{}
6e938899-990e-4f07-a747-86f4c5312266,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/granite/configuration_granite.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granite_configuration_granite.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granite_configuration_granite.py,{}
367ed1cc-ab59-4bf2-9e3f-27517e9e741f,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/granitemoe/configuration_granitemoe.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteMoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoe_configuration_granitemoe.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoe_configuration_granitemoe.py,{}
8011454b-db6f-4de2-b605-1228adcfd012,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class GraniteMoeHybridConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoehybrid_configuration_granitemoehybrid.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoehybrid_configuration_granitemoehybrid.py,{}
e76622c8-a1c9-42ad-910a-40ebf24ef3c2,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/granitemoeshared/configuration_granitemoeshared.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteMoeSharedConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoeshared_configuration_granitemoeshared.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoeshared_configuration_granitemoeshared.py,{}
253bf958-cc96-4e04-9ef0-3a30c80f4463,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/helium/configuration_helium.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class HeliumConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 128):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_helium_configuration_helium.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_helium_configuration_helium.py,{}
63632909-4273-4e6c-b4a9-048fe5b9d9a9,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/jamba/configuration_jamba.py,modified,1,1,2,"@@ -55,7 +55,7 @@ class JambaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_jamba_configuration_jamba.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_jamba_configuration_jamba.py,{}
07c37f1c-bda4-412f-b5bd-e55e1b4fdbc0,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/llama/configuration_llama.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class LlamaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_llama_configuration_llama.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_llama_configuration_llama.py,{}
96772a2d-34bd-4542-919e-d5b282eca93b,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/llama/convert_llama_weights_to_hf.py,modified,1,1,2,"@@ -528,7 +528,7 @@ def main():
     parser.add_argument(
         ""--model_size"",
         default=None,
-        help=""'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, checkout the original repo: https://huggingface.co/meta-llama"",
+        help=""'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, check out the original repo: https://huggingface.co/meta-llama"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_llama_convert_llama_weights_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_llama_convert_llama_weights_to_hf.py,{}
9755831c-07ed-4107-922f-c4861439e566,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/mimi/configuration_mimi.py,modified,1,1,2,"@@ -95,7 +95,7 @@ class MimiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mimi_configuration_mimi.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mimi_configuration_mimi.py,{}
ada1e197-d608-4401-855c-8c209cb6e1a8,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/minimax/configuration_minimax.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MiniMaxConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_minimax_configuration_minimax.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_minimax_configuration_minimax.py,{}
f341154b-aaa9-4819-b0a3-afac818fb237,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/minimax/modular_minimax.py,modified,1,1,2,"@@ -76,7 +76,7 @@ class MiniMaxConfig(MixtralConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_minimax_modular_minimax.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_minimax_modular_minimax.py,{}
7dc815c4-88fc-4719-b9ae-6c792fb4f88d,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/mistral/configuration_mistral.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MistralConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mistral_configuration_mistral.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mistral_configuration_mistral.py,{}
56dc105b-fb7a-41ee-a158-fb3b7b4b01c9,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/mixtral/configuration_mixtral.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MixtralConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mixtral_configuration_mixtral.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mixtral_configuration_mixtral.py,{}
6cc532bb-791b-452a-8134-8ba9da3d1ea5,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py,modified,1,1,2,"@@ -227,7 +227,7 @@ def main():
     parser.add_argument(
         ""--model_size"",
         choices=[""7B""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, checkout the original repo: https://huggingface.co/mistral-ai"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, check out the original repo: https://huggingface.co/mistral-ai"",
         default=""7B"",
     )
     parser.add_argument(""--output_dir"", help=""Location to write HF model"", required=True)",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mixtral_convert_mixtral_weights_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mixtral_convert_mixtral_weights_to_hf.py,{}
2d67f3ab-e6dc-4167-9772-83cc84929628,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/moonshine/configuration_moonshine.py,modified,2,2,4,"@@ -53,15 +53,15 @@ class MoonshineConfig(PretrainedConfig):
             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         decoder_num_key_value_heads (`int`, *optional*):
             This is the number of key_value heads that should be used to implement Grouped Query Attention. If
             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `decoder_num_attention_heads`.
         pad_head_dim_to_multiple_of (`int`, *optional*):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moonshine_configuration_moonshine.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moonshine_configuration_moonshine.py,{}
10d18baa-8acb-4e4e-a372-1d5575761ffa,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/moonshine/modular_moonshine.py,modified,2,2,4,"@@ -75,15 +75,15 @@ class MoonshineConfig(PretrainedConfig):
             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         decoder_num_key_value_heads (`int`, *optional*):
             This is the number of key_value heads that should be used to implement Grouped Query Attention. If
             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `decoder_num_attention_heads`.
         pad_head_dim_to_multiple_of (`int`, *optional*):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moonshine_modular_moonshine.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moonshine_modular_moonshine.py,{}
8d597fe7-00fb-49a7-8b9d-aeff323671ce,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/moshi/configuration_moshi.py,modified,2,2,4,"@@ -47,7 +47,7 @@ class MoshiDepthConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.
         audio_vocab_size (`int`, *optional*, defaults to 2048):
             Vocabulary size of the audio part of model. Defines the number of different tokens that can be
@@ -171,7 +171,7 @@ class MoshiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.
         audio_vocab_size (`int`, *optional*):
             Vocabulary size of the audio part of model. Defines the number of different tokens that can be",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moshi_configuration_moshi.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moshi_configuration_moshi.py,{}
6aa2898c-18de-4602-a091-c5199b199429,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/nemotron/configuration_nemotron.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class NemotronConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""relu2""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_nemotron_configuration_nemotron.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_nemotron_configuration_nemotron.py,{}
8abc2e6b-dd03-4f10-a56e-e3cf4f6794b3,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/olmo/configuration_olmo.py,modified,1,1,2,"@@ -53,7 +53,7 @@ class OlmoConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo_configuration_olmo.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo_configuration_olmo.py,{}
8c68f22c-645c-49c9-b258-c35d08291a82,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/olmo2/configuration_olmo2.py,modified,1,1,2,"@@ -35,7 +35,7 @@ class Olmo2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo2_configuration_olmo2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo2_configuration_olmo2.py,{}
7ebc2758-34c7-45e6-a630-24d55b0c19e0,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/olmo2/modular_olmo2.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Olmo2Config(OlmoConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo2_modular_olmo2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo2_modular_olmo2.py,{}
ad07a4c2-26d0-4ed5-af73-eb4ca8dd82f8,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/olmoe/configuration_olmoe.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class OlmoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmoe_configuration_olmoe.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmoe_configuration_olmoe.py,{}
51f35fdc-94aa-4f2d-b4e7-27318134633e,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/phi/configuration_phi.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class PhiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi_configuration_phi.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi_configuration_phi.py,{}
04622aed-42ac-49b7-b16f-d00b1fed058b,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/phi3/configuration_phi3.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Phi3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi3_configuration_phi3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi3_configuration_phi3.py,{}
ca69ba2a-6364-42a2-b9ab-01e019151cf1,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py,modified,1,1,2,"@@ -268,7 +268,7 @@ class Phi4MultimodalConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi4_multimodal_configuration_phi4_multimodal.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi4_multimodal_configuration_phi4_multimodal.py,{}
b45c840c-bb2b-4050-a4ed-8870853a2640,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py,modified,1,1,2,"@@ -304,7 +304,7 @@ class Phi4MultimodalConfig(Phi3Config):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi4_multimodal_modular_phi4_multimodal.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi4_multimodal_modular_phi4_multimodal.py,{}
c200e8df-7470-461f-ac99-14493cfb8f90,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/phimoe/configuration_phimoe.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class PhimoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phimoe_configuration_phimoe.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phimoe_configuration_phimoe.py,{}
b8426fda-f3df-4e47-9602-030e96849d8d,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2/configuration_qwen2.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Qwen2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_configuration_qwen2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_configuration_qwen2.py,{}
af6fecae-cb2a-4a01-bb4f-9a423a55d08e,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py,modified,2,2,4,"@@ -238,7 +238,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.
@@ -584,7 +584,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_omni_configuration_qwen2_5_omni.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_omni_configuration_qwen2_5_omni.py,{}
4544137b-3c21-49db-b929-8e7b838c9747,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py,modified,2,2,4,"@@ -277,7 +277,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.
@@ -623,7 +623,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_omni_modular_qwen2_5_omni.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_omni_modular_qwen2_5_omni.py,{}
4a066085-513b-4d8c-b276-e9f76fde5d81,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py,modified,1,1,2,"@@ -94,7 +94,7 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_vl_configuration_qwen2_5_vl.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_vl_configuration_qwen2_5_vl.py,{}
01d25576-7b75-4950-8027-89393c4dca3f,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2_moe/configuration_qwen2_moe.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Qwen2MoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_moe_configuration_qwen2_moe.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_moe_configuration_qwen2_moe.py,{}
5b0e91c1-b3a9-4c07-9755-29c9d5d1460d,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen2_vl/configuration_qwen2_vl.py,modified,1,1,2,"@@ -83,7 +83,7 @@ class Qwen2VLTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_vl_configuration_qwen2_vl.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_vl_configuration_qwen2_vl.py,{}
d85ace7e-1ce3-40b8-ae2d-38b658265125,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen3/configuration_qwen3.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Qwen3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         head_dim (`int`, *optional*, defaults to 128):
             The attention head dimension.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen3_configuration_qwen3.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen3_configuration_qwen3.py,{}
05b81e54-6147-4126-bf7a-aa13443c6f12,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/qwen3_moe/configuration_qwen3_moe.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Qwen3MoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen3_moe_configuration_qwen3_moe.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen3_moe_configuration_qwen3_moe.py,{}
5226b5ab-f563-4be4-8b1e-2db8001c8602,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py,modified,1,1,2,"@@ -167,7 +167,7 @@ def main():
         ""--model_size"",
         default=""2B"",
         choices=[""2B"", ""7B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_recurrent_gemma_convert_recurrent_gemma_to_hf.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_recurrent_gemma_convert_recurrent_gemma_to_hf.py,{}
9e06b7e5-f33d-409a-9cac-781916d932e6,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/stablelm/configuration_stablelm.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class StableLmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_stablelm_configuration_stablelm.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_stablelm_configuration_stablelm.py,{}
cefb3452-da6f-4638-a30e-160b9dbba592,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/starcoder2/configuration_starcoder2.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Starcoder2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""gelu_pytorch_tanh""`):
             The non-linear activation function (function or string) in the decoder.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_starcoder2_configuration_starcoder2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_starcoder2_configuration_starcoder2.py,{}
4340d6c0-fbed-49f4-8ef9-4b38bf8a3c17,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/zamba/configuration_zamba.py,modified,1,1,2,"@@ -59,7 +59,7 @@ class ZambaConfig(PretrainedConfig):
             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         n_mamba_heads (`int`, *optional*, defaults to 2):
             Number of mamba heads for each mamba layer.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_zamba_configuration_zamba.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_zamba_configuration_zamba.py,{}
922762e4-d689-4675-a8d1-f23de642002f,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/models/zamba2/configuration_zamba2.py,modified,1,1,2,"@@ -79,7 +79,7 @@ class Zamba2Config(PretrainedConfig):
             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         attention_dropout (`float`, *optional*, defaults to 0.0):
             The dropout ratio for the attention probabilities.",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_zamba2_configuration_zamba2.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_zamba2_configuration_zamba2.py,{}
c6a2a71b-78ea-4c61-9d16-89b86a3a6f7f,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/quantizers/quantizer_bitnet.py,modified,1,1,2,"@@ -34,7 +34,7 @@ class BitNetHfQuantizer(HfQuantizer):
     1.58-bit quantization from BitNet quantization method:
     Before loading: it converts the linear layers into BitLinear layers during loading.
 
-    Checkout the paper introducing this method : https://arxiv.org/pdf/2402.17764
+    Check out the paper introducing this method : https://arxiv.org/pdf/2402.17764
     """"""
 
     requires_parameters_quantization = False",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_quantizers_quantizer_bitnet.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_quantizers_quantizer_bitnet.py,{}
5eb0fd5a-7edd-460f-859d-043bc479d242,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/utils/hub.py,modified,2,2,4,"@@ -90,7 +90,7 @@ def is_offline_mode():
 default_cache_path = constants.default_cache_path
 
 # Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.
-# The best way to set the cache path is with the environment variable HF_HOME. For more details, checkout this
+# The best way to set the cache path is with the environment variable HF_HOME. For more details, check out this
 # documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables.
 #
 # In code, use `HF_HUB_CACHE` as the default cache path. This variable is set by the library and is guaranteed
@@ -542,7 +542,7 @@ def cached_files(
             elif _raise_exceptions_for_missing_entries:
                 raise OSError(
                     f""We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the""
-                    f"" cached files.\nCheckout your internet connection or see how to run the library in offline mode at""
+                    f"" cached files.\nCheck your internet connection or see how to run the library in offline mode at""
                     "" 'https://huggingface.co/docs/transformers/installation#offline-mode'.""
                 ) from e
         # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_utils_hub.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_utils_hub.py,{}
f96c1e60-6404-4e96-a8fa-2c47f15e79bf,71efd503-edb0-4afe-909f-873cca442bb7,src/transformers/utils/import_utils.py,modified,9,9,18,"@@ -1492,39 +1492,39 @@ def check_torch_load_is_safe():
 
 # docstyle-ignore
 SENTENCEPIECE_IMPORT_ERROR = """"""
-{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
+{0} requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 PROTOBUF_IMPORT_ERROR = """"""
-{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the
+{0} requires the protobuf library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 FAISS_IMPORT_ERROR = """"""
-{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the
+{0} requires the faiss library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 PYTORCH_IMPORT_ERROR = """"""
-{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
+{0} requires the PyTorch library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 TORCHVISION_IMPORT_ERROR = """"""
-{0} requires the Torchvision library but it was not found in your environment. Checkout the instructions on the
+{0} requires the Torchvision library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
@@ -1576,30 +1576,30 @@ def check_torch_load_is_safe():
 
 # docstyle-ignore
 TENSORFLOW_IMPORT_ERROR = """"""
-{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the
+{0} requires the TensorFlow library but it was not found in your environment. Check out the instructions on the
 installation page: https://www.tensorflow.org/install and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 DETECTRON2_IMPORT_ERROR = """"""
-{0} requires the detectron2 library but it was not found in your environment. Checkout the instructions on the
+{0} requires the detectron2 library but it was not found in your environment. Check out the instructions on the
 installation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 FLAX_IMPORT_ERROR = """"""
-{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the
+{0} requires the FLAX library but it was not found in your environment. Check out the instructions on the
 installation page: https://github.com/google/flax and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 # docstyle-ignore
 FTFY_IMPORT_ERROR = """"""
-{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the
+{0} requires the ftfy library but it was not found in your environment. Check out the instructions on the
 installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""",,py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_utils_import_utils.py,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_utils_import_utils.py,{}
dba41f2f-7b30-4afb-ac59-11018c8d6a95,71efd503-edb0-4afe-909f-873cca442bb7,templates/adding_a_new_model/README.md,modified,2,2,4,"@@ -19,5 +19,5 @@ limitations under the License.
 This page has been updated in light of the removal of the `add_new_model` script in favor of the more complete 
 `add_new_model_like` script.
 
-We recommend you checkout the documentation of [How to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model)
-in the Hugging Face Transformers documentation for complete and up-to-date instructions.
+We recommend you check out the documentation on [how to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model) 
+for complete and up-to-date instructions.",,md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\templates_adding_a_new_model_README.md,C:\Users\Emily\PycharmProjects\github_collector\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\templates_adding_a_new_model_README.md,{}
