id,commit_id,file_path,status,additions,deletions,changes,patch,previous_filename,file_type,blob_before_local_path,blob_after_local_path,metadata
62850350-09e0-450a-8aeb-a7028d433523,6385c570-e56a-4e77-b505-fcde2a4591ae,tests/custom_pk/fields.py,modified,3,0,3,"@@ -19,6 +19,9 @@ def __eq__(self, other):
             return self.value == other.value
         return self.value == other
 
+    def __hash__(self):
+        return hash(self.value)
+
 
 class MyWrapperField(models.CharField):
     def __init__(self, *args, **kwargs):",,py,.\tmp\github_file_blobs\django\django\8fd21b0da35697591e86f4eab0035c4360a45144\before\tests_custom_pk_fields.py,.\tmp\github_file_blobs\django\django\8fd21b0da35697591e86f4eab0035c4360a45144\after\tests_custom_pk_fields.py,{}
45ad3cee-18bc-4464-835e-ca629b89cea1,1c464aeb-9cff-49a4-8d03-af0742ff351d,src/transformers/models/qwen2_audio/processing_qwen2_audio.py,modified,1,1,2,"@@ -248,7 +248,7 @@ def default_chat_template(self):
                     ""{{ message['content'] }}<|im_end|>\n""
                 ""{% else %}""
                     ""{% for content in message['content'] %}""
-                        ""{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}""
+                        ""{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' or content['type'] == 'audio' %}""
                             ""{% set audio_count.value = audio_count.value + 1 %}""
                             ""Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\n""
                         ""{% elif 'text' in content %}""",,py,.\tmp\github_file_blobs\huggingface\transformers\d7b87b415a5dd4a3152051e1a0abd098a02c5bfa\before\src_transformers_models_qwen2_audio_processing_qwen2_audio.py,.\tmp\github_file_blobs\huggingface\transformers\d7b87b415a5dd4a3152051e1a0abd098a02c5bfa\after\src_transformers_models_qwen2_audio_processing_qwen2_audio.py,{}
cc8b76a7-06ec-44d9-b6c8-a6308f29091c,194ba04b-345d-4efd-95fc-80ee458fc195,src/transformers/models/auto/modeling_auto.py,modified,0,5,5,"@@ -1529,11 +1529,6 @@
 MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(
     [
         (""sam"", ""SamModel""),
-    ]
-)
-
-MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(
-    [
         (""sam_hq"", ""SamHQModel""),
     ]
 )",,py,.\tmp\github_file_blobs\huggingface\transformers\237ff80387ef453b1bb341c0b5d6cbcb5e60758d\before\src_transformers_models_auto_modeling_auto.py,.\tmp\github_file_blobs\huggingface\transformers\237ff80387ef453b1bb341c0b5d6cbcb5e60758d\after\src_transformers_models_auto_modeling_auto.py,{}
5b0d1931-8c4f-4621-8306-1ced7aadbdfa,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,docs/source/en/internal/import_utils.md,modified,1,1,2,"@@ -38,7 +38,7 @@ However, no method can be called on that object:
 ```python
 >>> DetrImageProcessorFast.from_pretrained()
 ImportError: 
-DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Checkout the instructions on the
+DetrImageProcessorFast requires the Torchvision library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 ```",,md,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\docs_source_en_internal_import_utils.md,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\docs_source_en_internal_import_utils.md,{}
b745f2e2-cbad-4952-b993-a69f51b87751,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/flax/question-answering/run_qa.py,modified,1,1,2,"@@ -546,7 +546,7 @@ def main():
     # region Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_flax_question-answering_run_qa.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_flax_question-answering_run_qa.py,{}
378bd05d-0511-4a54-8bc0-8dbd16f76246,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/modular-transformers/configuration_my_new_model.py,modified,1,1,2,"@@ -36,7 +36,7 @@ class MyNewModelConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_modular-transformers_configuration_my_new_model.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_modular-transformers_configuration_my_new_model.py,{}
3967ca00-8528-4182-b8c6-6536f3a91cae,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/modular-transformers/configuration_new_model.py,modified,1,1,2,"@@ -34,7 +34,7 @@ class NewModelConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_modular-transformers_configuration_new_model.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_modular-transformers_configuration_new_model.py,{}
feb8a048-e2a5-460d-b472-5f7cb463faf8,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/pytorch/question-answering/run_qa.py,modified,1,1,2,"@@ -357,7 +357,7 @@ def main():
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_pytorch_question-answering_run_qa.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_pytorch_question-answering_run_qa.py,{}
065a17e5-8218-4aaa-aaa8-277c0d9b48ac,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/pytorch/token-classification/run_ner.py,modified,1,1,2,"@@ -399,7 +399,7 @@ def get_label_list(labels):
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_pytorch_token-classification_run_ner.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_pytorch_token-classification_run_ner.py,{}
15b06cc1-461e-45aa-8a08-487a95b214ea,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,examples/tensorflow/question-answering/run_qa.py,modified,1,1,2,"@@ -378,7 +378,7 @@ def main():
     # region Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
         raise ValueError(
-            ""This example script only works for models that have a fast tokenizer. Checkout the big table of models at""
+            ""This example script only works for models that have a fast tokenizer. Check out the big table of models at""
             "" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet""
             "" this requirement""
         )",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\examples_tensorflow_question-answering_run_qa.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\examples_tensorflow_question-answering_run_qa.py,{}
8d821965-1784-4a19-8aa1-3277c572560a,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/aria/configuration_aria.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class AriaTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_aria_configuration_aria.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_aria_configuration_aria.py,{}
d2f5eabd-2e36-40a2-bef3-28afa3fedc9d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/aria/modular_aria.py,modified,1,1,2,"@@ -120,7 +120,7 @@ class AriaTextConfig(LlamaConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_aria_modular_aria.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_aria_modular_aria.py,{}
568943ca-b61e-4128-b412-a686a59d71ec,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/bamba/configuration_bamba.py,modified,1,1,2,"@@ -53,7 +53,7 @@ class BambaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_bamba_configuration_bamba.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_bamba_configuration_bamba.py,{}
569da622-dc13-4404-bb60-0098639558ec,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/bitnet/configuration_bitnet.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class BitNetConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""relu2""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_bitnet_configuration_bitnet.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_bitnet_configuration_bitnet.py,{}
899f5023-1022-4754-a7c6-30099e0734e8,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/chameleon/configuration_chameleon.py,modified,1,1,2,"@@ -125,7 +125,7 @@ class ChameleonConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_chameleon_configuration_chameleon.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_chameleon_configuration_chameleon.py,{}
c8b58baa-dc44-423d-9525-f6091cf65788,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py,modified,1,1,2,"@@ -446,7 +446,7 @@ def main():
         ""--model_size"",
         choices=[""7B"", ""30B""],
         help=""""
-        "" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, checkout the original repo: https://github.com/facebookresearch/chameleon"",
+        "" models correspond to the finetuned versions, and are specific to the Chameleon official release. For more details on Chameleon, check out the original repo: https://github.com/facebookresearch/chameleon"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_chameleon_convert_chameleon_weights_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_chameleon_convert_chameleon_weights_to_hf.py,{}
65f0e470-ae34-4b13-9da9-730030b84f54,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/cohere/configuration_cohere.py,modified,1,1,2,"@@ -56,7 +56,7 @@ class CohereConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere_configuration_cohere.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere_configuration_cohere.py,{}
76d4afdd-9ff4-4001-ab14-258c66fef72d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/cohere2/configuration_cohere2.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class Cohere2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere2_configuration_cohere2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere2_configuration_cohere2.py,{}
74081cb5-8c36-48b4-8dca-9144ac6b8778,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/cohere2/modular_cohere2.py,modified,1,1,2,"@@ -74,7 +74,7 @@ class Cohere2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_cohere2_modular_cohere2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_cohere2_modular_cohere2.py,{}
a1fe87a4-bcb3-4c5e-9751-2b34eb5bb827,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/csm/configuration_csm.py,modified,2,2,4,"@@ -54,7 +54,7 @@ class CsmDepthDecoderConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
@@ -235,7 +235,7 @@ class CsmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the backbone model Transformer decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_csm_configuration_csm.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_csm_configuration_csm.py,{}
3cf4f7b7-03c1-4e7b-b1eb-1379d31b7b09,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/deepseek_v3/configuration_deepseek_v3.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class DeepseekV3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         n_shared_experts (`int`, *optional*, defaults to 1):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_deepseek_v3_configuration_deepseek_v3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_deepseek_v3_configuration_deepseek_v3.py,{}
acec01ab-bf18-44e9-ad36-00ed93063e87,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/diffllama/configuration_diffllama.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class DiffLlamaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_diffllama_configuration_diffllama.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_diffllama_configuration_diffllama.py,{}
21dd6bb1-415c-45a6-8057-2c6d6c50ad2f,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/emu3/configuration_emu3.py,modified,1,1,2,"@@ -138,7 +138,7 @@ class Emu3TextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_emu3_configuration_emu3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_emu3_configuration_emu3.py,{}
0879f7d5-7166-42e0-a84b-fac9b753d8f6,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/falcon_h1/configuration_falcon_h1.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class FalconH1Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_falcon_h1_configuration_falcon_h1.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_falcon_h1_configuration_falcon_h1.py,{}
d6ebe5f2-ca1e-4266-b866-a0dd13314675,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma/configuration_gemma.py,modified,1,1,2,"@@ -47,7 +47,7 @@ class GemmaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_configuration_gemma.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_configuration_gemma.py,{}
84903ac9-a2c2-45dc-b5ea-baadaa4b8fe3,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma/convert_gemma_weights_to_hf.py,modified,1,1,2,"@@ -151,7 +151,7 @@ def main():
         ""--model_size"",
         default=""7B"",
         choices=[""2B"", ""7B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_convert_gemma_weights_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_convert_gemma_weights_to_hf.py,{}
d2ff34e4-f069-430e-8d93-d8b1a54e30b7,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma/modular_gemma.py,modified,1,1,2,"@@ -74,7 +74,7 @@ class GemmaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma_modular_gemma.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma_modular_gemma.py,{}
5d391156-30fc-498e-b002-2b6463736354,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma2/configuration_gemma2.py,modified,1,1,2,"@@ -47,7 +47,7 @@ class Gemma2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_configuration_gemma2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_configuration_gemma2.py,{}
a3f9461f-073e-435f-b15a-cdc8bea6b784,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py,modified,1,1,2,"@@ -184,7 +184,7 @@ def main():
         ""--model_size"",
         default=""9B"",
         choices=[""9B"", ""27B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma22 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_convert_gemma2_weights_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_convert_gemma2_weights_to_hf.py,{}
d50155b1-4252-4c36-ad3b-9df4b84424fd,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma2/modular_gemma2.py,modified,1,1,2,"@@ -71,7 +71,7 @@ class Gemma2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma2_modular_gemma2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma2_modular_gemma2.py,{}
e76c4431-4082-4da9-a958-b854742138f4,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma3/configuration_gemma3.py,modified,1,1,2,"@@ -55,7 +55,7 @@ class Gemma3TextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma3_configuration_gemma3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma3_configuration_gemma3.py,{}
5dee1b10-d9f3-40fb-aa47-57a772290c4e,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/gemma3/modular_gemma3.py,modified,1,1,2,"@@ -82,7 +82,7 @@ class Gemma3TextConfig(Gemma2Config, PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 256):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_gemma3_modular_gemma3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_gemma3_modular_gemma3.py,{}
fbfdd913-e725-4c24-8ba4-f0d4c9a6331a,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/glm/configuration_glm.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class GlmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_glm_configuration_glm.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_glm_configuration_glm.py,{}
08fdfe0a-756a-4342-9e2b-4821bc75ebd6,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/glm4/configuration_glm4.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class Glm4Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_glm4_configuration_glm4.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_glm4_configuration_glm4.py,{}
a4c7dd04-8116-423c-9adc-73b176ed50df,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/granite/configuration_granite.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granite_configuration_granite.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granite_configuration_granite.py,{}
35eb9000-0fab-48ef-ab92-76431a6626e5,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/granitemoe/configuration_granitemoe.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteMoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoe_configuration_granitemoe.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoe_configuration_granitemoe.py,{}
15d0532c-e2f1-45e6-89c7-260464360141,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class GraniteMoeHybridConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoehybrid_configuration_granitemoehybrid.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoehybrid_configuration_granitemoehybrid.py,{}
4aa298f6-b7a3-415e-9517-a8423344a58f,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/granitemoeshared/configuration_granitemoeshared.py,modified,1,1,2,"@@ -54,7 +54,7 @@ class GraniteMoeSharedConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_granitemoeshared_configuration_granitemoeshared.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_granitemoeshared_configuration_granitemoeshared.py,{}
2d7e4b9b-1b28-48ea-9a2c-d26f6395cbe8,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/helium/configuration_helium.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class HeliumConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         head_dim (`int`, *optional*, defaults to 128):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_helium_configuration_helium.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_helium_configuration_helium.py,{}
6511e1eb-2dc8-4218-8dd8-0f9271ded651,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/jamba/configuration_jamba.py,modified,1,1,2,"@@ -55,7 +55,7 @@ class JambaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_jamba_configuration_jamba.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_jamba_configuration_jamba.py,{}
71ff09e0-1595-469d-95a0-ddd0aec793cc,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/llama/configuration_llama.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class LlamaConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_llama_configuration_llama.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_llama_configuration_llama.py,{}
beba2d45-c2ee-413b-8ee2-43ac7ab9f65c,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/llama/convert_llama_weights_to_hf.py,modified,1,1,2,"@@ -528,7 +528,7 @@ def main():
     parser.add_argument(
         ""--model_size"",
         default=None,
-        help=""'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, checkout the original repo: https://huggingface.co/meta-llama"",
+        help=""'f' Deprecated in favor of `num_shards`: models correspond to the finetuned versions, and are specific to the Llama2 official release. For more details on Llama2, check out the original repo: https://huggingface.co/meta-llama"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_llama_convert_llama_weights_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_llama_convert_llama_weights_to_hf.py,{}
f0ce403e-68db-4357-ba93-a10c35a74d11,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/mimi/configuration_mimi.py,modified,1,1,2,"@@ -95,7 +95,7 @@ class MimiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mimi_configuration_mimi.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mimi_configuration_mimi.py,{}
79d60ea1-5fbd-496e-a89d-5214048df063,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/minimax/configuration_minimax.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MiniMaxConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_minimax_configuration_minimax.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_minimax_configuration_minimax.py,{}
8e6be7d6-155e-4777-a4a3-4ac277fc17a8,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/minimax/modular_minimax.py,modified,1,1,2,"@@ -76,7 +76,7 @@ class MiniMaxConfig(MixtralConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_minimax_modular_minimax.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_minimax_modular_minimax.py,{}
65579154-e000-4150-9c37-df867f940197,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/mistral/configuration_mistral.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MistralConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mistral_configuration_mistral.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mistral_configuration_mistral.py,{}
b8b3d190-6ff5-4c05-9c83-18250087433d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/mixtral/configuration_mixtral.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class MixtralConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mixtral_configuration_mixtral.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mixtral_configuration_mixtral.py,{}
e0da6a25-0ccb-4dde-8d8e-f2bfc446f0f2,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py,modified,1,1,2,"@@ -227,7 +227,7 @@ def main():
     parser.add_argument(
         ""--model_size"",
         choices=[""7B""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, checkout the original repo: https://huggingface.co/mistral-ai"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Mixtral official release. For more details on Mixtral, check out the original repo: https://huggingface.co/mistral-ai"",
         default=""7B"",
     )
     parser.add_argument(""--output_dir"", help=""Location to write HF model"", required=True)",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_mixtral_convert_mixtral_weights_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_mixtral_convert_mixtral_weights_to_hf.py,{}
a96fed19-b6d0-4609-8548-3ce366adc02d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/moonshine/configuration_moonshine.py,modified,2,2,4,"@@ -53,15 +53,15 @@ class MoonshineConfig(PretrainedConfig):
             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         decoder_num_key_value_heads (`int`, *optional*):
             This is the number of key_value heads that should be used to implement Grouped Query Attention. If
             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `decoder_num_attention_heads`.
         pad_head_dim_to_multiple_of (`int`, *optional*):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moonshine_configuration_moonshine.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moonshine_configuration_moonshine.py,{}
4ecfccf6-e5d0-4aa0-97a8-d37cd72d1b59,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/moonshine/modular_moonshine.py,modified,2,2,4,"@@ -75,15 +75,15 @@ class MoonshineConfig(PretrainedConfig):
             `encoder_num_key_value_heads=encoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `encoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         decoder_num_key_value_heads (`int`, *optional*):
             This is the number of key_value heads that should be used to implement Grouped Query Attention. If
             `decoder_num_key_value_heads=decoder_num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `decoder_num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `decoder_num_attention_heads`.
         pad_head_dim_to_multiple_of (`int`, *optional*):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moonshine_modular_moonshine.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moonshine_modular_moonshine.py,{}
ceebeda7-0e7c-4553-8187-129909734fb7,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/moshi/configuration_moshi.py,modified,2,2,4,"@@ -47,7 +47,7 @@ class MoshiDepthConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.
         audio_vocab_size (`int`, *optional*, defaults to 2048):
             Vocabulary size of the audio part of model. Defines the number of different tokens that can be
@@ -171,7 +171,7 @@ class MoshiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.
         audio_vocab_size (`int`, *optional*):
             Vocabulary size of the audio part of model. Defines the number of different tokens that can be",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_moshi_configuration_moshi.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_moshi_configuration_moshi.py,{}
a383ac15-7f1c-4dcb-aecc-75fcb99ba7df,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/nemotron/configuration_nemotron.py,modified,1,1,2,"@@ -52,7 +52,7 @@ class NemotronConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""relu2""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_nemotron_configuration_nemotron.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_nemotron_configuration_nemotron.py,{}
d28b76e7-2e9a-4ee3-8410-b6525bae985a,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/olmo/configuration_olmo.py,modified,1,1,2,"@@ -53,7 +53,7 @@ class OlmoConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo_configuration_olmo.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo_configuration_olmo.py,{}
1ad8e138-dbf9-48a1-a781-b0b7f829076c,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/olmo2/configuration_olmo2.py,modified,1,1,2,"@@ -35,7 +35,7 @@ class Olmo2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo2_configuration_olmo2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo2_configuration_olmo2.py,{}
905bbda2-b10f-4b06-8ebd-36dcf0e3f0a2,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/olmo2/modular_olmo2.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Olmo2Config(OlmoConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmo2_modular_olmo2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmo2_modular_olmo2.py,{}
d9fe847c-b2eb-4d3d-99b7-c2deae1b8b6e,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/olmoe/configuration_olmoe.py,modified,1,1,2,"@@ -42,7 +42,7 @@ class OlmoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_olmoe_configuration_olmoe.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_olmoe_configuration_olmoe.py,{}
e3b6c50e-624d-4387-ac27-116441827d84,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/phi/configuration_phi.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class PhiConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi_configuration_phi.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi_configuration_phi.py,{}
8d6c4396-3ecd-46b5-adc8-6ca3e80280c2,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/phi3/configuration_phi3.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Phi3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi3_configuration_phi3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi3_configuration_phi3.py,{}
35caf82a-0d52-4db3-ad4b-0f21a5340f7a,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py,modified,1,1,2,"@@ -268,7 +268,7 @@ class Phi4MultimodalConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi4_multimodal_configuration_phi4_multimodal.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi4_multimodal_configuration_phi4_multimodal.py,{}
c22dc136-a611-4aa1-9d9f-ef8dbbddb869,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py,modified,1,1,2,"@@ -304,7 +304,7 @@ class Phi4MultimodalConfig(Phi3Config):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         resid_pdrop (`float`, *optional*, defaults to 0.0):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phi4_multimodal_modular_phi4_multimodal.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phi4_multimodal_modular_phi4_multimodal.py,{}
de01f7a3-9c57-4b5b-afe4-534700478c1d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/phimoe/configuration_phimoe.py,modified,1,1,2,"@@ -48,7 +48,7 @@ class PhimoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_phimoe_configuration_phimoe.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_phimoe_configuration_phimoe.py,{}
e57165b3-9390-4367-8219-9c5bc4726b55,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2/configuration_qwen2.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Qwen2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_configuration_qwen2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_configuration_qwen2.py,{}
5c776f27-37b1-40fc-b86e-9e1c4e21aedd,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py,modified,2,2,4,"@@ -238,7 +238,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.
@@ -584,7 +584,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_omni_configuration_qwen2_5_omni.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_omni_configuration_qwen2_5_omni.py,{}
8dd4ca15-097c-4810-9082-fae59a328758,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py,modified,2,2,4,"@@ -277,7 +277,7 @@ class Qwen2_5OmniTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.
@@ -623,7 +623,7 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_omni_modular_qwen2_5_omni.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_omni_modular_qwen2_5_omni.py,{}
e83d59cd-ab3f-4f86-bfc2-331dc82460c1,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py,modified,1,1,2,"@@ -94,7 +94,7 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_5_vl_configuration_qwen2_5_vl.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_5_vl_configuration_qwen2_5_vl.py,{}
89179707-db30-4cdb-ac6a-cf3d2eead8f8,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2_moe/configuration_qwen2_moe.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Qwen2MoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_moe_configuration_qwen2_moe.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_moe_configuration_qwen2_moe.py,{}
29d16438-95cb-438e-b7c6-4f6aceb5a85b,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen2_vl/configuration_qwen2_vl.py,modified,1,1,2,"@@ -83,7 +83,7 @@ class Qwen2VLTextConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen2_vl_configuration_qwen2_vl.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen2_vl_configuration_qwen2_vl.py,{}
9117deff-4b93-42e3-9b33-8e4c7b6bcd61,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen3/configuration_qwen3.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Qwen3Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         head_dim (`int`, *optional*, defaults to 128):
             The attention head dimension.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen3_configuration_qwen3.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen3_configuration_qwen3.py,{}
f3f11655-36d8-4b85-8024-358227d18073,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/qwen3_moe/configuration_qwen3_moe.py,modified,1,1,2,"@@ -49,7 +49,7 @@ class Qwen3MoeConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_qwen3_moe_configuration_qwen3_moe.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_qwen3_moe_configuration_qwen3_moe.py,{}
2992001f-9d23-40ad-a64f-7b1b905ccb05,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py,modified,1,1,2,"@@ -167,7 +167,7 @@ def main():
         ""--model_size"",
         default=""2B"",
         choices=[""2B"", ""7B"", ""tokenizer_only""],
-        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, checkout the original repo: https://huggingface.co/google/gemma-7b"",
+        help=""'f' models correspond to the finetuned versions, and are specific to the Gemma2 official release. For more details on Gemma2, check out the original repo: https://huggingface.co/google/gemma-7b"",
     )
     parser.add_argument(
         ""--output_dir"",",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_recurrent_gemma_convert_recurrent_gemma_to_hf.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_recurrent_gemma_convert_recurrent_gemma_to_hf.py,{}
f17d95a6-ae85-48bf-99ec-a4a629fc8773,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/stablelm/configuration_stablelm.py,modified,1,1,2,"@@ -51,7 +51,7 @@ class StableLmConfig(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
             `num_attention_heads`.
         hidden_act (`str` or `function`, *optional*, defaults to `""silu""`):",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_stablelm_configuration_stablelm.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_stablelm_configuration_stablelm.py,{}
69882248-e14b-4db4-8702-e7e33ad67830,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/starcoder2/configuration_starcoder2.py,modified,1,1,2,"@@ -50,7 +50,7 @@ class Starcoder2Config(PretrainedConfig):
             `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `8`.
         hidden_act (`str` or `function`, *optional*, defaults to `""gelu_pytorch_tanh""`):
             The non-linear activation function (function or string) in the decoder.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_starcoder2_configuration_starcoder2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_starcoder2_configuration_starcoder2.py,{}
107f9380-fe9c-4a74-8f03-81b8f29ac1de,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/zamba/configuration_zamba.py,modified,1,1,2,"@@ -59,7 +59,7 @@ class ZambaConfig(PretrainedConfig):
             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         n_mamba_heads (`int`, *optional*, defaults to 2):
             Number of mamba heads for each mamba layer.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_zamba_configuration_zamba.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_zamba_configuration_zamba.py,{}
cc37d0f6-e770-4748-bbfe-5b2c5c51507d,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/models/zamba2/configuration_zamba2.py,modified,1,1,2,"@@ -79,7 +79,7 @@ class Zamba2Config(PretrainedConfig):
             `num_key_value_heads=None`, the model will use Multi Head Attention (MHA), if
             `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
             converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
-            by meanpooling all the original heads within that group. For more details checkout [this
+            by meanpooling all the original heads within that group. For more details, check out [this
             paper](https://arxiv.org/pdf/2305.13245.pdf).
         attention_dropout (`float`, *optional*, defaults to 0.0):
             The dropout ratio for the attention probabilities.",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_models_zamba2_configuration_zamba2.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_models_zamba2_configuration_zamba2.py,{}
99ec5fb5-5ad8-4f0f-bad8-732942ab7747,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/quantizers/quantizer_bitnet.py,modified,1,1,2,"@@ -34,7 +34,7 @@ class BitNetHfQuantizer(HfQuantizer):
     1.58-bit quantization from BitNet quantization method:
     Before loading: it converts the linear layers into BitLinear layers during loading.
 
-    Checkout the paper introducing this method : https://arxiv.org/pdf/2402.17764
+    Check out the paper introducing this method : https://arxiv.org/pdf/2402.17764
     """"""
 
     requires_parameters_quantization = False",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_quantizers_quantizer_bitnet.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_quantizers_quantizer_bitnet.py,{}
3014a43e-46d6-47f2-9389-92d887e6aff6,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/utils/hub.py,modified,2,2,4,"@@ -90,7 +90,7 @@ def is_offline_mode():
 default_cache_path = constants.default_cache_path
 
 # Determine default cache directory. Lots of legacy environment variables to ensure backward compatibility.
-# The best way to set the cache path is with the environment variable HF_HOME. For more details, checkout this
+# The best way to set the cache path is with the environment variable HF_HOME. For more details, check out this
 # documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables.
 #
 # In code, use `HF_HUB_CACHE` as the default cache path. This variable is set by the library and is guaranteed
@@ -542,7 +542,7 @@ def cached_files(
             elif _raise_exceptions_for_missing_entries:
                 raise OSError(
                     f""We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load the files, and couldn't find them in the""
-                    f"" cached files.\nCheckout your internet connection or see how to run the library in offline mode at""
+                    f"" cached files.\nCheck your internet connection or see how to run the library in offline mode at""
                     "" 'https://huggingface.co/docs/transformers/installation#offline-mode'.""
                 ) from e
         # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_utils_hub.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_utils_hub.py,{}
7d07aceb-eb4b-479d-9d91-f10a480f9c69,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,src/transformers/utils/import_utils.py,modified,9,9,18,"@@ -1492,39 +1492,39 @@ def check_torch_load_is_safe():
 
 # docstyle-ignore
 SENTENCEPIECE_IMPORT_ERROR = """"""
-{0} requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
+{0} requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 PROTOBUF_IMPORT_ERROR = """"""
-{0} requires the protobuf library but it was not found in your environment. Checkout the instructions on the
+{0} requires the protobuf library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 FAISS_IMPORT_ERROR = """"""
-{0} requires the faiss library but it was not found in your environment. Checkout the instructions on the
+{0} requires the faiss library but it was not found in your environment. Check out the instructions on the
 installation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 PYTORCH_IMPORT_ERROR = """"""
-{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
+{0} requires the PyTorch library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 TORCHVISION_IMPORT_ERROR = """"""
-{0} requires the Torchvision library but it was not found in your environment. Checkout the instructions on the
+{0} requires the Torchvision library but it was not found in your environment. Check out the instructions on the
 installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
@@ -1576,30 +1576,30 @@ def check_torch_load_is_safe():
 
 # docstyle-ignore
 TENSORFLOW_IMPORT_ERROR = """"""
-{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the
+{0} requires the TensorFlow library but it was not found in your environment. Check out the instructions on the
 installation page: https://www.tensorflow.org/install and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 DETECTRON2_IMPORT_ERROR = """"""
-{0} requires the detectron2 library but it was not found in your environment. Checkout the instructions on the
+{0} requires the detectron2 library but it was not found in your environment. Check out the instructions on the
 installation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""
 
 
 # docstyle-ignore
 FLAX_IMPORT_ERROR = """"""
-{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the
+{0} requires the FLAX library but it was not found in your environment. Check out the instructions on the
 installation page: https://github.com/google/flax and follow the ones that match your environment.
 Please note that you may need to restart your runtime after installation.
 """"""
 
 # docstyle-ignore
 FTFY_IMPORT_ERROR = """"""
-{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the
+{0} requires the ftfy library but it was not found in your environment. Check out the instructions on the
 installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones
 that match your environment. Please note that you may need to restart your runtime after installation.
 """"""",,py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\src_transformers_utils_import_utils.py,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\src_transformers_utils_import_utils.py,{}
f43b2908-fd20-4c57-aa3a-274b3cf90dbf,1be8688e-07fb-4ef0-8412-040a1aaaa8f3,templates/adding_a_new_model/README.md,modified,2,2,4,"@@ -19,5 +19,5 @@ limitations under the License.
 This page has been updated in light of the removal of the `add_new_model` script in favor of the more complete 
 `add_new_model_like` script.
 
-We recommend you checkout the documentation of [How to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model)
-in the Hugging Face Transformers documentation for complete and up-to-date instructions.
+We recommend you check out the documentation on [how to add a model](https://huggingface.co/docs/transformers/main/en/add_new_model) 
+for complete and up-to-date instructions.",,md,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\before\templates_adding_a_new_model_README.md,.\tmp\github_file_blobs\huggingface\transformers\19224c3642705c5b6988c9f5f4251f83323d05ae\after\templates_adding_a_new_model_README.md,{}
04354a89-46b8-4b03-aabd-6869f91a61b9,3b24a9bf-b583-42d8-b059-9309d17a0a08,src/transformers/modeling_attn_mask_utils.py,modified,1,1,2,"@@ -193,7 +193,7 @@ def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]
 
         expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
 
-        inverted_mask = 1.0 - expanded_mask
+        inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask
 
         return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)
 ",,py,.\tmp\github_file_blobs\huggingface\transformers\282d6684dc0a611e97dff464aa9fa9e4adaed092\before\src_transformers_modeling_attn_mask_utils.py,.\tmp\github_file_blobs\huggingface\transformers\282d6684dc0a611e97dff464aa9fa9e4adaed092\after\src_transformers_modeling_attn_mask_utils.py,{}
31183e7c-b59c-4713-9452-560d1de0c33d,278a8e57-0441-403b-be5d-367bfd69e57a,src/transformers/models/__init__.py,modified,5,0,5,"@@ -80,6 +80,7 @@
     from .deberta import *
     from .deberta_v2 import *
     from .decision_transformer import *
+    from .deepseek_v3 import *
     from .deformable_detr import *
     from .deit import *
     from .deprecated import *
@@ -239,6 +240,7 @@
     from .plbart import *
     from .poolformer import *
     from .pop2piano import *
+    from .prompt_depth_anything import *
     from .prophetnet import *
     from .pvt import *
     from .pvt_v2 import *
@@ -247,6 +249,8 @@
     from .qwen2_audio import *
     from .qwen2_moe import *
     from .qwen2_vl import *
+    from .qwen3 import *
+    from .qwen3_moe import *
     from .rag import *
     from .recurrent_gemma import *
     from .reformer import *
@@ -268,6 +272,7 @@
     from .seggpt import *
     from .sew import *
     from .sew_d import *
+    from .shieldgemma2 import *
     from .siglip import *
     from .siglip2 import *
     from .smolvlm import *",,py,.\tmp\github_file_blobs\huggingface\transformers\b31d462c61b19cd00a08d1cc85cecfe5f59cdde3\before\src_transformers_models___init__.py,.\tmp\github_file_blobs\huggingface\transformers\b31d462c61b19cd00a08d1cc85cecfe5f59cdde3\after\src_transformers_models___init__.py,{}
3380067b-0144-482f-a226-18c982d5c04e,2745b775-3c14-408f-a199-e5988046480c,src/transformers/models/rag/retrieval_rag.py,modified,9,5,14,"@@ -104,6 +104,7 @@ class LegacyIndex(Index):
     PASSAGE_FILENAME = ""psgs_w100.tsv.pkl""
 
     def __init__(self, vector_size, index_path):
+        requires_backends(self, [""faiss""])
         self.index_id_to_db_id = []
         self.index_path = index_path
         self.passages = self._load_passages()
@@ -197,6 +198,7 @@ def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np
 
 class HFIndexBase(Index):
     def __init__(self, vector_size, dataset, index_initialized=False):
+        requires_backends(self, [""faiss""])
         self.vector_size = vector_size
         self.dataset = dataset
         self._index_initialized = index_initialized
@@ -269,6 +271,7 @@ def __init__(
         use_dummy_dataset=False,
         dataset_revision=None,
     ):
+        requires_backends(self, [""faiss""])
         if int(index_path is None) + int(index_name is None) != 1:
             raise ValueError(""Please provide `index_name` or `index_path`."")
         self.dataset_name = dataset_name
@@ -321,6 +324,7 @@ class CustomHFIndex(HFIndexBase):
     """"""
 
     def __init__(self, vector_size: int, dataset, index_path=None):
+        requires_backends(self, [""faiss""])
         super().__init__(vector_size, dataset, index_initialized=index_path is None)
         self.index_path = index_path
 
@@ -375,14 +379,14 @@ class RagRetriever:
 
     >>> dataset = (
     ...     ...
-    ... )  # dataset must be a datasets.Datasets object with columns ""title"", ""text"" and ""embeddings"", and it must have a faiss index
+    ... )  # dataset must be a datasets.Datasets object with columns ""title"", ""text"" and ""embeddings"", and it must have a supported index (e.g., Faiss or other index types depending on your setup)
     >>> retriever = RagRetriever.from_pretrained(""facebook/dpr-ctx_encoder-single-nq-base"", indexed_dataset=dataset)
 
     >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py
     >>> from transformers import RagRetriever
 
     >>> dataset_path = ""path/to/my/dataset""  # dataset saved via *dataset.save_to_disk(...)*
-    >>> index_path = ""path/to/my/index.faiss""  # faiss index saved via *dataset.get_index(""embeddings"").save(...)*
+    >>> index_path = ""path/to/my/index""  # index saved via *dataset.get_index(""embeddings"").save(...)*
     >>> retriever = RagRetriever.from_pretrained(
     ...     ""facebook/dpr-ctx_encoder-single-nq-base"",
     ...     index_name=""custom"",
@@ -398,7 +402,7 @@ class RagRetriever:
 
     def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):
         self._init_retrieval = init_retrieval
-        requires_backends(self, [""datasets"", ""faiss""])
+        requires_backends(self, [""datasets""])
         super().__init__()
         self.index = index or self._build_index(config)
         self.generator_tokenizer = generator_tokenizer
@@ -440,7 +444,7 @@ def _build_index(config):
 
     @classmethod
     def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):
-        requires_backends(cls, [""datasets"", ""faiss""])
+        requires_backends(cls, [""datasets""])
         config = kwargs.pop(""config"", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)
         rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)
         question_encoder_tokenizer = rag_tokenizer.question_encoder
@@ -557,7 +561,7 @@ def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tup
             np.array(vectors_batched),
         )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)
 
-    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:
+    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray, List[dict]]:
         """"""
         Retrieves documents for specified `question_hidden_states`.
 ",,py,.\tmp\github_file_blobs\huggingface\transformers\11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390\before\src_transformers_models_rag_retrieval_rag.py,.\tmp\github_file_blobs\huggingface\transformers\11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390\after\src_transformers_models_rag_retrieval_rag.py,{}
a32a2cde-4c8d-493a-bbee-cc61f3efd6cd,18658280-bb45-4d7c-930e-3ef7ec7a28e4,src/transformers/models/esm/modeling_esm.py,modified,6,0,6,"@@ -1023,6 +1023,8 @@ def __init__(self, config):
 
         self.init_weights()
 
+        self.post_init()
+
     def get_output_embeddings(self):
         return self.lm_head.decoder
 
@@ -1127,6 +1129,8 @@ def __init__(self, config):
 
         self.init_weights()
 
+        self.post_init()
+
     @auto_docstring
     def forward(
         self,
@@ -1210,6 +1214,8 @@ def __init__(self, config):
 
         self.init_weights()
 
+        self.post_init()
+
     @auto_docstring
     def forward(
         self,",,py,.\tmp\github_file_blobs\huggingface\transformers\b9faf2f93085e3cf2c65184a69d1d9e502f95786\before\src_transformers_models_esm_modeling_esm.py,.\tmp\github_file_blobs\huggingface\transformers\b9faf2f93085e3cf2c65184a69d1d9e502f95786\after\src_transformers_models_esm_modeling_esm.py,{}
7fc5b459-0df6-4365-83a3-2de21b798ffa,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,README.md,modified,1,1,2,"@@ -61,7 +61,7 @@ HelloGitHub  GitHub ** 28 
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\README.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\README.md,{}
36018ef2-a87b-4429-8ed2-7762bf107852,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub01.md,modified,1,1,2,"@@ -137,7 +137,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub01.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub01.md,{}
4334e667-d1f5-46f9-86db-99dd29246b05,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub02.md,modified,1,1,2,"@@ -177,7 +177,7 @@ if __name__ == '__main__':
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub02.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub02.md,{}
941ae092-a34a-48ba-9048-7af7b9f9695d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub03.md,modified,1,1,2,"@@ -106,7 +106,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub03.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub03.md,{}
2a5bbe7e-b065-4cc1-b952-dcd05fea0f97,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub04.md,modified,1,1,2,"@@ -118,7 +118,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub04.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub04.md,{}
79288ae5-f4d5-4e5a-a654-8e1689e60b5c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub05.md,modified,1,1,2,"@@ -135,7 +135,7 @@ $pinyin->convert('', PINYIN_ASCI
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub05.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub05.md,{}
06931743-e9d8-44dd-8508-7d94b7dc414f,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub06.md,modified,1,1,2,"@@ -158,7 +158,7 @@ brew install mercurial
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub06.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub06.md,{}
2ed1bcf0-e70d-4994-8a61-1575fffeb70d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub07.md,modified,1,1,2,"@@ -184,7 +184,7 @@ print langid.classify(text2)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub07.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub07.md,{}
1a864631-f538-4075-bb27-ab1235ce499b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub08.md,modified,1,1,2,"@@ -250,7 +250,7 @@ finally:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub08.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub08.md,{}
51a35030-0a56-417a-94af-dba17dec3ebb,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub09.md,modified,1,1,2,"@@ -175,7 +175,7 @@ ngrok http 8000
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub09.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub09.md,{}
3e956bf4-569c-41b9-ac88-ab8af1944d4e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub10.md,modified,1,1,2,"@@ -189,7 +189,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub10.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub10.md,{}
899dfc40-bc93-489d-9bc6-a02e913d795b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub100.md,modified,1,1,2,"@@ -325,7 +325,7 @@ scoop install python ruby go perl
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub100.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub100.md,{}
289496b7-698d-4a9c-b473-f40b4084e7b2,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub101.md,modified,1,1,2,"@@ -351,7 +351,7 @@ print(response)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub101.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub101.md,{}
420784b3-d005-4f06-b7d8-827d21595cce,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub102.md,modified,1,1,2,"@@ -313,7 +313,7 @@ output = compiled_model({0: example.numpy()})
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub102.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub102.md,{}
a9ce23da-29d8-4a77-8928-f9c2233d5e9b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub103.md,modified,1,1,2,"@@ -379,7 +379,7 @@ path = model.export(format=""onnx"")  # return path to exported model
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub103.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub103.md,{}
219ce001-b4d1-44f2-9542-090d862c9876,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub104.md,modified,1,1,2,"@@ -279,7 +279,7 @@ cv2.imwrite(""vis_image.jpg"", vis_im)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub104.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub104.md,{}
343d5594-160f-4098-b922-67d80c3a5c5a,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub105.md,modified,1,1,2,"@@ -321,7 +321,7 @@ const result = await zerox({
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub105.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub105.md,{}
4c56b387-e63f-4fa8-a0a2-e7fe1f8b4759,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub106.md,modified,1,1,2,"@@ -432,7 +432,7 @@ Render( Point(0, 0), 50, 120, 120, 300, 800, 600 )
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub106.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub106.md,{}
c29801ee-7619-4677-947b-472825ddfd0e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub107.md,modified,1,1,2,"@@ -252,7 +252,7 @@ if num := v.Export().(int64); num != 4 {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub107.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub107.md,{}
0cf0acbb-48ec-4b5a-a2a4-23be0cb8b887,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub108.md,modified,1,1,2,"@@ -289,7 +289,7 @@ ret = tf_fn(x1)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub108.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub108.md,{}
6a48bd6b-5cd9-4260-8c48-fc9750cbfcf0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub109.md,modified,1,1,2,"@@ -302,7 +302,7 @@ jsonpath ""$.id"" matches /\d{4}/     # Check the format of the id
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub109.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub109.md,{}
3fbfac51-04b7-4bfb-8f9b-28d6808f2912,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub11.md,modified,1,1,2,"@@ -176,7 +176,7 @@ print ifconfig(""eth0"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub11.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub11.md,{}
23d7bf23-ff1f-4cbe-8d56-e207adb10b51,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub110.md,modified,1,1,2,"@@ -295,7 +295,7 @@ if __name__ == ""__main__"":
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub110.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub110.md,{}
95fb6941-3a02-499e-86dc-45c4561c9bf1,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub12.md,modified,1,1,2,"@@ -196,7 +196,7 @@ Hello Google!
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub12.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub12.md,{}
ba67c839-9884-4334-9403-d6fd3ab6bcad,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub13.md,modified,1,1,2,"@@ -241,7 +241,7 @@ Detailed:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub13.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub13.md,{}
3288deb7-b5b8-4f40-a270-8d79c878a02e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub14.md,modified,1,1,2,"@@ -178,7 +178,7 @@ gcc -g -O2 testSearcher.c ip2region.c
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub14.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub14.md,{}
ba9a3411-9e72-42d0-90d0-be5a887726c8,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub15.md,modified,1,1,2,"@@ -199,7 +199,7 @@ def test():
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub15.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub15.md,{}
9eceb171-c736-455f-92ca-417608fe56dc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub16.md,modified,1,1,2,"@@ -219,7 +219,7 @@ namespace WeixinSDK.Test.Fake
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub16.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub16.md,{}
f06bbafe-821b-4785-bc27-0242df87b1bb,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub17.md,modified,1,1,2,"@@ -190,7 +190,7 @@
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub17.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub17.md,{}
d785899b-515e-48bc-a988-1b8d0ff8b7e7,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub18.md,modified,1,1,2,"@@ -308,7 +308,7 @@ datetime.datetime(2013, 12, 30, 0, 0)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub18.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub18.md,{}
35aebf09-6c4a-4111-bf42-a5fb213ef7ee,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub19.md,modified,1,1,2,"@@ -192,7 +192,7 @@ browser.submit(upload_form)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub19.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub19.md,{}
1e5c0426-50c3-4179-9d37-414c03ba0e75,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub20.md,modified,1,1,2,"@@ -227,7 +227,7 @@ with open('report.xls', 'wb') as f:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub20.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub20.md,{}
f4c36be8-57c6-4c52-bebf-1a1d4ba197e8,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub21.md,modified,1,1,2,"@@ -230,7 +230,7 @@ format code  extension  resolution note
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub21.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub21.md,{}
d50eb320-999b-40cc-a685-7306be435320,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub22.md,modified,1,1,2,"@@ -191,7 +191,7 @@ for i in tqdm(range(10000)):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub22.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub22.md,{}
636b5a8c-851e-4e64-b726-b9af70c361b7,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub23.md,modified,1,1,2,"@@ -173,7 +173,7 @@ foo(
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub23.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub23.md,{}
aed157a5-b54a-4219-8ca1-5f5e7701eea5,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub24.md,modified,1,1,2,"@@ -232,7 +232,7 @@ ws_api.get_gzh_info('')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub24.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub24.md,{}
59a7a387-7759-422e-91bc-1ede29083f1d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub25.md,modified,1,1,2,"@@ -213,7 +213,7 @@ synonyms.seg("""")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub25.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub25.md,{}
85d4ecb6-a7fc-4c2f-8da0-23bb5da52e88,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub26.md,modified,1,1,2,"@@ -306,7 +306,7 @@ public func print<T>(file: String = #file, function: String = #function, line: I
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub26.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub26.md,{}
8a141730-72d7-4113-97fa-bea5f5a613cc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub27.md,modified,1,1,2,"@@ -224,7 +224,7 @@ def get_data(url):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub27.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub27.md,{}
d6b903b7-8378-4f57-bc28-2202488764cc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub28.md,modified,1,1,2,"@@ -237,7 +237,7 @@ export default class Index extends Component {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub28.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub28.md,{}
b3e76fca-9ce1-4711-b247-2b719d5aa1b0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub29.md,modified,1,1,2,"@@ -267,7 +267,7 @@ ReactDOM.render(
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub29.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub29.md,{}
048f001f-8370-4a57-9096-a5d15ce6a295,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub30.md,modified,1,1,2,"@@ -195,7 +195,7 @@ print(s)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub30.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub30.md,{}
1a7d2ec4-b34c-4b0d-ac03-de893f50c04d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub31.md,modified,1,1,2,"@@ -265,7 +265,7 @@ cup
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub31.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub31.md,{}
143c7058-2227-4630-bcb8-8a969da172ed,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub32.md,modified,1,1,2,"@@ -329,7 +329,7 @@ learn.fit(1)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub32.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub32.md,{}
255da22c-abdd-49e5-a61f-26b746c644b6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub33.md,modified,1,1,2,"@@ -257,7 +257,7 @@ _attrLabel = [NudeIn make:^(NUDTextMaker *make) {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub33.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub33.md,{}
519252fe-0d04-4016-95cb-bd36ac08af4e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub34.md,modified,1,1,2,"@@ -378,7 +378,7 @@ segments_tensors = torch.tensor([segments_ids])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub34.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub34.md,{}
8a2aebb6-d49b-4a4d-aba1-e903594654dd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub35.md,modified,1,1,2,"@@ -303,7 +303,7 @@ doc.sentences[0].print_dependencies()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub35.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub35.md,{}
034a953e-551f-460a-8fc2-0801f9c309ca,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub36.md,modified,1,1,2,"@@ -341,7 +341,7 @@ render(<Player sources={sources} />)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub36.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub36.md,{}
faec229c-af6c-4788-933b-0ce07d0b162e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub37.md,modified,1,1,2,"@@ -266,7 +266,7 @@ pyxel.run(update, draw)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub37.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub37.md,{}
524c5579-2cb2-45b6-b5c3-275860fcb114,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub38.md,modified,1,1,2,"@@ -318,7 +318,7 @@ class IrisClassifier(BentoService):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub38.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub38.md,{}
15701f8c-4a93-4d14-81aa-16776928b633,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub39.md,modified,1,1,2,"@@ -331,7 +331,7 @@ dropout(input_) # RETURNS: torch.FloatTensor (6x3x10)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub39.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub39.md,{}
7f8f236c-6a7e-4afc-a03e-9ef66b102654,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub40.md,modified,1,1,2,"@@ -352,7 +352,7 @@ forward(input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=No
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub40.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub40.md,{}
ccabea71-7748-438c-8b4e-7ffcf5a8cec8,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub41.md,modified,1,1,2,"@@ -309,7 +309,7 @@ RestClient.post( url,
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub41.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub41.md,{}
4c7c908d-f2ec-4b46-bcec-4999a9747576,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub42.md,modified,1,1,2,"@@ -302,7 +302,7 @@ array([[4.43336608e-03, 9.95215198e-01, 3.51419231e-04, 1.68657851e-08]])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub42.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub42.md,{}
2c5e2bff-eeb0-4638-9d0c-79f60d238d35,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub43.md,modified,1,1,2,"@@ -342,7 +342,7 @@ import Percent
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub43.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub43.md,{}
9f63dcaa-a1f3-4ba5-baad-2f1a04950114,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub44.md,modified,1,1,2,"@@ -272,7 +272,7 @@ do {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub44.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub44.md,{}
942fd662-8117-4224-a288-822b9cbcee67,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub45.md,modified,1,1,2,"@@ -457,7 +457,7 @@ trainer(net, train_dataset)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub45.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub45.md,{}
ecc4b2bd-1258-4a6f-8a94-fadccef3d0d0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub46.md,modified,1,1,2,"@@ -339,7 +339,7 @@ fn main() {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub46.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub46.md,{}
108dc35f-4aa1-4cfd-bb3c-497fa72c714e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub47.md,modified,1,1,2,"@@ -233,7 +233,7 @@ print(bond_df)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub47.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub47.md,{}
8abbde8f-e34b-4dc2-b091-5495bc23cf9b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub48.md,modified,1,1,2,"@@ -265,7 +265,7 @@ results = clf.predict(x_test)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub48.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub48.md,{}
734b48c3-2da4-4424-9aab-edce71f897b3,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub49.md,modified,1,1,2,"@@ -236,7 +236,7 @@ let _ = ""19 Nov 2015 22:20:40 +0100"".toRSS(alt: true)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub49.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub49.md,{}
f1ada77c-531f-468c-9ee1-ab1110020d4a,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub50.md,modified,1,1,2,"@@ -285,7 +285,7 @@ best_points, best_distance = ga_tsp.run()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub50.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub50.md,{}
f50b0b6f-1883-46d5-a5e9-8c2c109b09a0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub51.md,modified,1,1,2,"@@ -260,7 +260,7 @@ python py2sec.py -d example/ -m test1.py,bbb/
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub51.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub51.md,{}
a77d2461-0564-496d-8de2-8c07164b061b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub52.md,modified,1,1,2,"@@ -269,7 +269,7 @@ DATABASE_PASSWORD = os.getenv(""DATABASE_PASSWORD"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub52.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub52.md,{}
7e8c64d2-fe5d-4b16-a3df-59c9f000809d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub53.md,modified,1,1,2,"@@ -265,7 +265,7 @@ points & bars
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub53.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub53.md,{}
00ad6582-c289-4caf-85f8-fbcb06e8c2ef,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub54.md,modified,1,1,2,"@@ -394,7 +394,7 @@ ac.showPhotoLibrary(sender: self)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub54.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub54.md,{}
763f6cd7-e673-47c9-85a9-ad81f0870e0f,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub55.md,modified,1,1,2,"@@ -332,7 +332,7 @@ s.sentiments    # 0.9769663402895832 positive
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub55.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub55.md,{}
4a829e15-1919-4110-bc0b-3803d5edeca0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub56.md,modified,1,1,2,"@@ -389,7 +389,7 @@ result = reader.readtext('chinese.jpg')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub56.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub56.md,{}
71c202b0-f8b6-4288-a1db-929c01cf8041,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub57.md,modified,1,1,2,"@@ -297,7 +297,7 @@ HELLO
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub57.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub57.md,{}
b59d5b19-622c-4ef7-82c8-c7d9925b2b80,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub58.md,modified,1,1,2,"@@ -363,7 +363,7 @@ Lua 5.3.5 Copyright (C) 1994-2018 Lua.org, PUC-Rio
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub58.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub58.md,{}
f2208cad-b35b-4a1f-8475-1b92b6b8a379,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub59.md,modified,1,1,2,"@@ -374,7 +374,7 @@ try db.run(users.create { t in
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub59.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub59.md,{}
2ba08491-7632-4340-90ac-e70e7e3fcc14,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub60.md,modified,1,1,2,"@@ -470,7 +470,7 @@ docker run -it \
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub60.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub60.md,{}
262faef8-b586-4798-b6c3-7ed5b0be65ca,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub61.md,modified,1,1,2,"@@ -308,7 +308,7 @@ fselect hsize, abspath from ./tmp where size lt 8k
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub61.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub61.md,{}
aaad4679-5a27-4af4-9698-4c805d20f41d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub62.md,modified,1,1,2,"@@ -283,7 +283,7 @@ x <- 42
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub62.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub62.md,{}
75e0b92d-57d9-4a66-9bee-189330270e9e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub63.md,modified,1,1,2,"@@ -361,7 +361,7 @@ docker exec aind cat /home/user/.vnc/passwdfile
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub63.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub63.md,{}
10c5e314-a69d-435b-a00d-6cc0fbd4d0e5,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub64.md,modified,1,1,2,"@@ -357,7 +357,7 @@ alert.show()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub64.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub64.md,{}
daa8629d-be42-4639-b19b-203cc769fdce,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub65.md,modified,1,1,2,"@@ -357,7 +357,7 @@ $ python3 -m weibo_spider
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub65.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub65.md,{}
bd943775-8477-4cea-a0ce-aaf1ba0eefe6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub66.md,modified,1,1,2,"@@ -342,7 +342,7 @@ static void take_gil(PyThreadState *tstate)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub66.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub66.md,{}
e6888d65-8d92-4946-b86d-35fc6faf470f,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub67.md,modified,1,1,2,"@@ -319,7 +319,7 @@ df = q.collect()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub67.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub67.md,{}
aa752f1a-748c-45b5-8ed7-6abde4712e43,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub68.md,modified,1,1,2,"@@ -316,7 +316,7 @@ RedisList[1, 4, 9, 16, 25]
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub68.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub68.md,{}
d1c0bb47-6646-475c-bf9a-f2c856e45f71,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub69.md,modified,1,1,2,"@@ -377,7 +377,7 @@ spleeter separate -p spleeter:2stems -o output .mp3
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub69.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub69.md,{}
8c1c7382-8674-44b7-a9fc-7a89e63c1f4b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub70.md,modified,1,1,2,"@@ -364,7 +364,7 @@ end
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub70.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub70.md,{}
85ff5022-f810-498e-8389-c7cc987de670,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub71.md,modified,1,1,2,"@@ -391,7 +391,7 @@ try app.run()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub71.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub71.md,{}
81ec0d17-c321-40f7-b7c7-0658baec94d3,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub72.md,modified,1,1,2,"@@ -425,7 +425,7 @@ class PreprocImg(Executor):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub72.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub72.md,{}
33d61c19-abc5-4e90-9d66-cb9b4d0a3a0e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub73.md,modified,1,1,2,"@@ -379,7 +379,7 @@ with pikepdf.open('input.pdf') as pdf:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub73.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub73.md,{}
812071f9-3e8a-4d38-98df-a4b59dcca2f9,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub74.md,modified,1,1,2,"@@ -365,7 +365,7 @@ xonshxxh anyhost +s xonsh
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub74.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub74.md,{}
c37d86e6-2987-482b-ab41-79a9ab24a5bd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub75.md,modified,1,1,2,"@@ -375,7 +375,7 @@ struct ContentView: View {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub75.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub75.md,{}
f4d2be3b-4847-4616-9aea-29861de6b3c0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub76.md,modified,1,1,2,"@@ -389,7 +389,7 @@ tagger.predict(sentence)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub76.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub76.md,{}
be80b4ed-d2a2-4e35-a38a-49fc57db0cf8,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub77.md,modified,1,1,2,"@@ -342,7 +342,7 @@ mackup restore
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub77.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub77.md,{}
5b5ec6a2-004f-402e-89ba-ebb1b07f7657,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub78.md,modified,1,1,2,"@@ -389,7 +389,7 @@ image.save(""astronaut_rides_horse.png"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub78.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub78.md,{}
00040b77-f57f-418f-8867-ddf7d47e7cb4,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub79.md,modified,1,1,2,"@@ -387,7 +387,7 @@ code2flow 
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub79.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub79.md,{}
3c121a38-a18d-40d7-8111-50318ea1d61b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub80.md,modified,1,1,2,"@@ -291,7 +291,7 @@ ABC
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub80.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub80.md,{}
cfe31c3d-53ba-415b-91c5-e90501085b58,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub81.md,modified,1,1,2,"@@ -296,7 +296,7 @@ for i in range(10):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub81.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub81.md,{}
39308862-426d-4cbb-aa94-e620b4d55587,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub82.md,modified,1,1,2,"@@ -355,7 +355,7 @@ git clone https://github.com/Botspot/pi-apps
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub82.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub82.md,{}
b0a96148-82d0-440b-afa4-35b3b7e022a2,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub83.md,modified,1,1,2,"@@ -434,7 +434,7 @@ $ curl http://localhost:5000/predictions -X POST \
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub83.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub83.md,{}
cb09a4b1-00be-45ae-a560-22c396c3defb,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub84.md,modified,1,1,2,"@@ -314,7 +314,7 @@ john = table.find_one(name='John Doe')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub84.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub84.md,{}
e03c99d8-6a32-42fd-baaf-735daad04bb4,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub85.md,modified,1,1,2,"@@ -282,7 +282,7 @@ Audio(audio_array, rate=SAMPLE_RATE)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub85.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub85.md,{}
7fb970d3-c716-47e2-bf6e-6ae0fd2b4b2f,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub86.md,modified,2,2,4,"@@ -203,7 +203,7 @@ play(guitar, bpm=100, instrument=25)
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/86/379429942.gif' style=""max-width:80%; max-height=80%;""></img></p>
 
-31[oxipng](https://hellogithub.com/periodical/statistics/click?target=https://github.com/shssoichiro/oxipng) PNG  Rust  PNG  Rust 
+31[oxipng](https://hellogithub.com/periodical/statistics/click?target=https://github.com/oxipng/oxipng) PNG  Rust  PNG  Rust 
 
 ### Swift 
 32[fsnotes](https://hellogithub.com/periodical/statistics/click?target=https://github.com/glushchenko/fsnotes)macOS/iOS  macOS  iOS  MarkdownTouchBar  170 iCloud Drive  Dropbox 
@@ -294,7 +294,7 @@ play(guitar, bpm=100, instrument=25)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub86.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub86.md,{}
af383bd1-3650-4e6a-99f4-51619b0da554,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub87.md,modified,1,1,2,"@@ -295,7 +295,7 @@ panorama = stitcher.stitch([""img?.jpg""])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub87.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub87.md,{}
c7fafb93-bd29-4c72-a857-250abf62a8e7,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub88.md,modified,1,1,2,"@@ -347,7 +347,7 @@ async def on_hello_world(msg: HelloWorld):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub88.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub88.md,{}
f9098679-d7ba-4738-aff7-badb54a9837e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub89.md,modified,1,1,2,"@@ -306,7 +306,7 @@ for div in divs:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub89.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub89.md,{}
5ea1be9c-46a8-4944-9972-4cd5a82d3b4c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub90.md,modified,1,1,2,"@@ -343,7 +343,7 @@ print(grad(grad(grad(tanh)))(1.0))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub90.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub90.md,{}
b005c4b0-6d3c-4b95-af02-1f653e202e23,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub91.md,modified,2,2,4,"@@ -250,7 +250,7 @@ async fn main() -> Result<(), Box<dyn std::error::Error>> {
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/91/31883122.png' style=""max-width:80%; max-height=80%;""></img></p>
 
 ### 
-32[AISystem](https://hellogithub.com/periodical/statistics/click?target=https://github.com/chenzomi12/AISystem)AI  &  AI AI AI AI 
+32[aisystem](https://hellogithub.com/periodical/statistics/click?target=https://github.com/chenzomi12/aisystem)AI  &  AI AI AI AI 
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/91/539888500.png' style=""max-width:80%; max-height=80%;""></img></p>
 
@@ -329,7 +329,7 @@ async fn main() -> Result<(), Box<dyn std::error::Error>> {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub91.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub91.md,{}
0d5abdb1-5961-49f5-838c-351ed677e17a,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub92.md,modified,1,1,2,"@@ -273,7 +273,7 @@ print(model(img))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub92.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub92.md,{}
01adfad6-7eb0-438d-b5f9-4d403eb6f3dc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub93.md,modified,1,1,2,"@@ -262,7 +262,7 @@ print(len(detections))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub93.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub93.md,{}
5d731b6b-6c90-460c-a2b7-a8226610eafc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub94.md,modified,1,1,2,"@@ -308,7 +308,7 @@ print(text_with_wm)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub94.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub94.md,{}
89dae3e2-aa2b-42cb-aed6-71ac475cc6ee,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub95.md,modified,1,1,2,"@@ -337,7 +337,7 @@ df.chat('Which are the 5 happiest countries?')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub95.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub95.md,{}
8520bbd1-541f-4150-921c-1b70a51486ee,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub96.md,modified,2,2,4,"@@ -126,7 +126,7 @@ XXH32_hash_t hash_string(const char* string, XXH32_hash_t seed)
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/96/237222619.png' style=""max-width:80%; max-height=80%;""></img></p>
 
 ### Python 
-23[marker](https://hellogithub.com/periodical/statistics/click?target=https://github.com/VikParuchuri/marker) PDF  Markdown  PDFEPUB  MOBI  Markdown  Python  Nougat
+23[marker](https://hellogithub.com/periodical/statistics/click?target=https://github.com/datalab-to/marker) PDF  Markdown  PDFEPUB  MOBI  Markdown  Python  Nougat
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/96/712111618.png' style=""max-width:80%; max-height=80%;""></img></p>
 
@@ -275,7 +275,7 @@ Version: 2.7
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub96.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub96.md,{}
2485404c-e93e-4e2f-8773-7f642b6c5df2,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub97.md,modified,1,1,2,"@@ -317,7 +317,7 @@ docker pull bitnami/APP:[TAG]
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub97.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub97.md,{}
0fa406b6-5b4d-437b-ba4f-23d17e4872ae,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub98.md,modified,1,1,2,"@@ -305,7 +305,7 @@ print(response)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub98.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub98.md,{}
fc3ebb75-0405-447c-8e21-13e4ec250f51,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/HelloGitHub99.md,modified,1,1,2,"@@ -331,7 +331,7 @@ print(result)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_HelloGitHub99.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_HelloGitHub99.md,{}
8103e510-b668-47e6-8a29-9ea63c4f27e6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub01.md,modified,1,1,2,"@@ -127,7 +127,7 @@ Click the **Table of Contents** icon at the top-right corner to open the n
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub01.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub01.md,{}
67eab322-ad22-4659-8c39-44751bc18f38,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub02.md,modified,1,1,2,"@@ -167,7 +167,7 @@ if __name__ == '__main__':
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub02.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub02.md,{}
77279c7e-4753-459d-977e-0d179a2aab3c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub03.md,modified,1,1,2,"@@ -96,7 +96,7 @@ Click the **Table of Contents** icon at the top-right corner to open the n
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub03.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub03.md,{}
4d3e8749-2a15-413e-80a0-e0c35921cc56,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub04.md,modified,1,1,2,"@@ -106,7 +106,7 @@ Click the **Table of Contents** icon at the top-right corner to open the n
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub04.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub04.md,{}
452128b9-d37e-4fc1-a0bc-6b2a8252cbc1,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub05.md,modified,1,1,2,"@@ -125,7 +125,7 @@ $pinyin->convert('', PINYIN_ASCI
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub05.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub05.md,{}
345f8da5-382f-4c5e-9eec-3eca348a1a38,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub06.md,modified,1,1,2,"@@ -148,7 +148,7 @@ brew install mercurial
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub06.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub06.md,{}
2d1449f2-9133-4118-9a3b-f200415d6cbd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub07.md,modified,1,1,2,"@@ -174,7 +174,7 @@ print langid.classify(text2)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub07.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub07.md,{}
a079f70e-abf7-484f-a622-e597fbaccfed,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub08.md,modified,1,1,2,"@@ -240,7 +240,7 @@ finally:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub08.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub08.md,{}
684723e4-57ef-4ce7-910b-158ad27458cd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub09.md,modified,1,1,2,"@@ -165,7 +165,7 @@ ngrok http 8000
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub09.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub09.md,{}
7d353d3e-2dae-44d7-81b4-be949a644887,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub10.md,modified,1,1,2,"@@ -179,7 +179,7 @@ Click the **Table of Contents** icon at the top-right corner to open the n
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub10.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub10.md,{}
f1270fc6-2ea7-4210-bd72-718fb617e8fa,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub100.md,modified,1,1,2,"@@ -315,7 +315,7 @@ scoop install python ruby go perl
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub100.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub100.md,{}
ccc58dd3-ab5c-44a9-9f41-ce81f92a8f8c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub101.md,modified,1,1,2,"@@ -341,7 +341,7 @@ print(response)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub101.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub101.md,{}
a2ae2ad7-5703-474e-86e3-d94c825ab643,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub102.md,modified,1,1,2,"@@ -303,7 +303,7 @@ output = compiled_model({0: example.numpy()})
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub102.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub102.md,{}
710710cf-b0e1-4207-a38b-bbf1a6c8e9b3,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub103.md,modified,1,1,2,"@@ -369,7 +369,7 @@ path = model.export(format=""onnx"")  # return path to exported model
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub103.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub103.md,{}
894f12e5-584e-447f-a49b-0b0fab64778b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub104.md,modified,1,1,2,"@@ -269,7 +269,7 @@ cv2.imwrite(""vis_image.jpg"", vis_im)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub104.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub104.md,{}
141f093b-09f2-4b69-9afb-8a74aaff7a24,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub105.md,modified,1,1,2,"@@ -311,7 +311,7 @@ const result = await zerox({
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub105.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub105.md,{}
685e939e-79d5-4ed2-9e05-08f934572051,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub106.md,modified,1,1,2,"@@ -422,7 +422,7 @@ Render( Point(0, 0), 50, 120, 120, 300, 800, 600 )
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub106.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub106.md,{}
0b9fc7ba-6696-4bff-9216-4026ffc9a4e6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub107.md,modified,1,1,2,"@@ -242,7 +242,7 @@ if num := v.Export().(int64); num != 4 {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub107.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub107.md,{}
7a79d742-6056-4175-9a52-01d8bcdfcd6e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub108.md,modified,1,1,2,"@@ -279,7 +279,7 @@ ret = tf_fn(x1)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub108.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub108.md,{}
59cd4ca2-1479-435d-818f-57c506c561c0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub109.md,modified,1,1,2,"@@ -292,7 +292,7 @@ jsonpath ""$.id"" matches /\d{4}/     # Check the format of the id
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub109.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub109.md,{}
83b09a69-ac9a-4af9-a7b3-265c795032d1,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub11.md,modified,1,1,2,"@@ -166,7 +166,7 @@ print ifconfig(""eth0"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub11.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub11.md,{}
c3027305-4022-4ab3-b528-a2dd7152279f,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub110.md,modified,1,1,2,"@@ -285,7 +285,7 @@ if __name__ == ""__main__"":
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub110.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub110.md,{}
f37c3201-b43f-48c6-8796-3f73aa91ad14,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub12.md,modified,1,1,2,"@@ -186,7 +186,7 @@ Hello Google!
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub12.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub12.md,{}
99c6cac5-0050-4e09-9436-1189df0af2bd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub13.md,modified,1,1,2,"@@ -231,7 +231,7 @@ Detailed:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub13.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub13.md,{}
c1a7e627-8ab5-4cfa-b9e3-4f94174b4dad,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub14.md,modified,1,1,2,"@@ -168,7 +168,7 @@ gcc -g -O2 testSearcher.c ip2region.c
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub14.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub14.md,{}
3bfb4e31-4593-4fc0-9304-6e4b4f6af790,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub15.md,modified,1,1,2,"@@ -189,7 +189,7 @@ def test():
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub15.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub15.md,{}
952cdc59-02a5-49a0-b590-77bc0673a3c6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub16.md,modified,1,1,2,"@@ -209,7 +209,7 @@ namespace WeixinSDK.Test.Fake
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub16.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub16.md,{}
e0c8eeef-7504-4f38-a1f7-7fb7a85a5b3a,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub17.md,modified,1,1,2,"@@ -180,7 +180,7 @@ Click the **Table of Contents** icon at the top-right corner to open the n
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub17.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub17.md,{}
8adba3e9-a2e4-4e35-b180-4ee40a1bff91,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub18.md,modified,1,1,2,"@@ -298,7 +298,7 @@ datetime.datetime(2013, 12, 30, 0, 0)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub18.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub18.md,{}
de778d55-5d87-4d59-b39e-962d87673b91,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub19.md,modified,1,1,2,"@@ -182,7 +182,7 @@ browser.submit(upload_form)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub19.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub19.md,{}
ded06051-1469-4894-9120-396be178eca7,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub20.md,modified,1,1,2,"@@ -217,7 +217,7 @@ with open('report.xls', 'wb') as f:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub20.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub20.md,{}
78fb6952-dcd3-43c3-af23-3825411b14e5,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub21.md,modified,1,1,2,"@@ -220,7 +220,7 @@ format code  extension  resolution note
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub21.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub21.md,{}
4736a084-ab02-4207-8875-cc91eca05c6c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub22.md,modified,1,1,2,"@@ -181,7 +181,7 @@ for i in tqdm(range(10000)):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub22.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub22.md,{}
2fc8d00c-a069-4dc1-b71d-b7722a9f24b6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub23.md,modified,1,1,2,"@@ -163,7 +163,7 @@ foo(
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub23.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub23.md,{}
52611b02-c872-4b65-93df-faec5026e82c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub24.md,modified,1,1,2,"@@ -222,7 +222,7 @@ ws_api.get_gzh_info('')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub24.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub24.md,{}
14b0bca6-fe66-486b-9060-5bf1e40c3b59,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub25.md,modified,1,1,2,"@@ -203,7 +203,7 @@ synonyms.seg("""")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub25.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub25.md,{}
4e0d27eb-8f24-42d0-ac61-333454058bde,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub26.md,modified,1,1,2,"@@ -295,7 +295,7 @@ public func print<T>(file: String = #file, function: String = #function, line: I
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub26.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub26.md,{}
1034e46a-3282-4099-9d0d-dfe6f9f73d50,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub27.md,modified,1,1,2,"@@ -214,7 +214,7 @@ def get_data(url):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub27.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub27.md,{}
03126091-d6c4-4f77-8b2c-e31c98ae7967,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub28.md,modified,1,1,2,"@@ -227,7 +227,7 @@ export default class Index extends Component {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub28.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub28.md,{}
6da34f4d-f9bd-40d1-a391-0a89a4351bb8,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub29.md,modified,1,1,2,"@@ -257,7 +257,7 @@ ReactDOM.render(
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub29.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub29.md,{}
c0099592-0f72-4f17-a3f1-cfafce1c9f45,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub30.md,modified,1,1,2,"@@ -185,7 +185,7 @@ print(s)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub30.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub30.md,{}
0bf070c6-4eaa-41c1-adec-dccd7f67aa6b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub31.md,modified,1,1,2,"@@ -255,7 +255,7 @@ cup
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub31.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub31.md,{}
3d936c01-74bc-4334-8663-4bbc5112b5dd,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub32.md,modified,1,1,2,"@@ -319,7 +319,7 @@ learn.fit(1)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub32.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub32.md,{}
6b255018-744d-4845-9279-7b49aa96371e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub33.md,modified,1,1,2,"@@ -247,7 +247,7 @@ _attrLabel = [NudeIn make:^(NUDTextMaker *make) {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub33.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub33.md,{}
e514d9b8-6279-418e-bde8-e079152348b0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub34.md,modified,1,1,2,"@@ -368,7 +368,7 @@ segments_tensors = torch.tensor([segments_ids])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub34.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub34.md,{}
0f45e6b0-7a04-405c-8c41-b7b5c3193ae4,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub35.md,modified,1,1,2,"@@ -293,7 +293,7 @@ doc.sentences[0].print_dependencies()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub35.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub35.md,{}
87c67053-9e5e-4bc4-a820-8b7f6392f829,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub36.md,modified,1,1,2,"@@ -331,7 +331,7 @@ render(<Player sources={sources} />)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub36.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub36.md,{}
d5a06930-1ca7-41a9-8324-e11bfda83b41,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub37.md,modified,1,1,2,"@@ -256,7 +256,7 @@ pyxel.run(update, draw)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub37.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub37.md,{}
d45babb8-2a2f-4a9d-b138-eaa8718a9b67,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub38.md,modified,1,1,2,"@@ -308,7 +308,7 @@ class IrisClassifier(BentoService):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub38.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub38.md,{}
d3bcdcd5-38e3-4cff-a0b8-41fe911030e3,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub39.md,modified,1,1,2,"@@ -319,7 +319,7 @@ dropout(input_) # RETURNS: torch.FloatTensor (6x3x10)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub39.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub39.md,{}
8e30189f-c012-430a-84b2-03ffef161270,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub40.md,modified,1,1,2,"@@ -342,7 +342,7 @@ forward(input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=No
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub40.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub40.md,{}
b1c58248-2c81-4987-ae03-58553f0dfc78,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub41.md,modified,1,1,2,"@@ -299,7 +299,7 @@ RestClient.post( url,
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub41.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub41.md,{}
ea976db6-2e38-44b4-b932-2af701077c05,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub42.md,modified,1,1,2,"@@ -292,7 +292,7 @@ array([[4.43336608e-03, 9.95215198e-01, 3.51419231e-04, 1.68657851e-08]])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub42.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub42.md,{}
f8517ea2-ec93-468b-825a-b7c8d7f35450,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub43.md,modified,1,1,2,"@@ -332,7 +332,7 @@ import Percent
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub43.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub43.md,{}
2d3b7a94-16da-4390-95c0-115978c3230a,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub44.md,modified,1,1,2,"@@ -262,7 +262,7 @@ do {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub44.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub44.md,{}
f9986fcc-900f-4dc0-bd0d-8475b276e33b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub45.md,modified,1,1,2,"@@ -447,7 +447,7 @@ trainer(net, train_dataset)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub45.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub45.md,{}
30976b03-23c3-472f-9fec-5b74d634f1b6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub46.md,modified,1,1,2,"@@ -329,7 +329,7 @@ fn main() {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub46.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub46.md,{}
07834a9b-19bf-4ef1-9fda-5b73c83f6dae,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub47.md,modified,1,1,2,"@@ -223,7 +223,7 @@ print(bond_df)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub47.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub47.md,{}
33636278-7115-4f19-b0b2-2783a9f1f0bf,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub48.md,modified,1,1,2,"@@ -255,7 +255,7 @@ results = clf.predict(x_test)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub48.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub48.md,{}
4ae0c880-991f-48bc-b1dc-e552d6079d95,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub49.md,modified,1,1,2,"@@ -226,7 +226,7 @@ let _ = ""19 Nov 2015 22:20:40 +0100"".toRSS(alt: true)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub49.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub49.md,{}
5ca7b8ec-ddc3-404f-b446-e4078e95a4dc,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub50.md,modified,1,1,2,"@@ -275,7 +275,7 @@ best_points, best_distance = ga_tsp.run()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub50.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub50.md,{}
cfc36aee-6e23-45ef-abbd-eda900777134,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub51.md,modified,1,1,2,"@@ -250,7 +250,7 @@ python py2sec.py -d example/ -m test1.py,bbb/
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub51.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub51.md,{}
b4acc7e3-1acc-46cb-bf39-5e373e286e33,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub52.md,modified,1,1,2,"@@ -259,7 +259,7 @@ DATABASE_PASSWORD = os.getenv(""DATABASE_PASSWORD"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub52.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub52.md,{}
3d5a6408-e871-460e-9f70-cc823e737c98,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub53.md,modified,1,1,2,"@@ -255,7 +255,7 @@ points & bars
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub53.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub53.md,{}
e241e444-c80d-4b1d-af26-58fcd2605f91,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub54.md,modified,1,1,2,"@@ -384,7 +384,7 @@ ac.showPhotoLibrary(sender: self)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub54.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub54.md,{}
4fc022f3-d176-4126-9fb5-07b84e739b35,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub55.md,modified,1,1,2,"@@ -322,7 +322,7 @@ s.sentiments    # 0.9769663402895832 positive
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub55.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub55.md,{}
607c3e7a-f99c-43c0-8aa4-92debf2dc9f1,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub56.md,modified,1,1,2,"@@ -379,7 +379,7 @@ result = reader.readtext('chinese.jpg')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub56.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub56.md,{}
9b36bd7a-b42c-4eea-ba7f-09a1ed03a0ae,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub57.md,modified,1,1,2,"@@ -287,7 +287,7 @@ HELLO
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub57.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub57.md,{}
fa8b6deb-ba68-4dde-8aa9-bc44a31394f4,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub58.md,modified,1,1,2,"@@ -353,7 +353,7 @@ Lua 5.3.5 Copyright (C) 1994-2018 Lua.org, PUC-Rio
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub58.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub58.md,{}
87614e39-2d77-4b5d-aa72-270e46e97c6d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub59.md,modified,1,1,2,"@@ -364,7 +364,7 @@ try db.run(users.create { t in
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub59.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub59.md,{}
b167ae9b-d5d0-490f-8d22-65e7915b9672,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub60.md,modified,1,1,2,"@@ -459,7 +459,7 @@ docker run -it \
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub60.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub60.md,{}
3f889ec5-0c71-4030-a1da-59b7c5c77a9b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub61.md,modified,1,1,2,"@@ -298,7 +298,7 @@ fselect hsize, abspath from ./tmp where size lt 8k
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub61.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub61.md,{}
5f6f2418-beeb-43cc-b16d-11df5e13f7d1,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub62.md,modified,1,1,2,"@@ -272,7 +272,7 @@ x <- 42
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub62.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub62.md,{}
ed222330-e516-46dd-ac5f-c729c2159bf0,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub63.md,modified,1,1,2,"@@ -351,7 +351,7 @@ docker exec aind cat /home/user/.vnc/passwdfile
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub63.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub63.md,{}
cf2c5fca-f989-44d4-9c12-d210f894d9da,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub64.md,modified,1,1,2,"@@ -347,7 +347,7 @@ alert.show()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub64.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub64.md,{}
672e8736-1f76-4e47-86d7-92249b6672e5,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub65.md,modified,1,1,2,"@@ -347,7 +347,7 @@ $ python3 -m weibo_spider
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub65.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub65.md,{}
f2e17f18-bf4d-41b0-99ec-cf620ca99f18,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub66.md,modified,1,1,2,"@@ -332,7 +332,7 @@ static void take_gil(PyThreadState *tstate)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub66.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub66.md,{}
6bf19ab7-0769-4f70-82c2-5ad0e95cf544,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub67.md,modified,1,1,2,"@@ -309,7 +309,7 @@ df = q.collect()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub67.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub67.md,{}
ccf51225-23f4-45aa-95ce-fdf81a669f52,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub68.md,modified,1,1,2,"@@ -306,7 +306,7 @@ RedisList[1, 4, 9, 16, 25]
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub68.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub68.md,{}
24826f78-95f5-491f-93e7-83a7936d3ce6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub69.md,modified,1,1,2,"@@ -360,7 +360,7 @@ spleeter separate -p spleeter:2stems -o output .mp3
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub69.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub69.md,{}
7f36082d-9d02-4287-ae8f-82d388a26e93,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub70.md,modified,1,1,2,"@@ -354,7 +354,7 @@ end
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub70.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub70.md,{}
2832963b-348f-4b6e-8bb4-c930e54a87ad,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub71.md,modified,1,1,2,"@@ -379,7 +379,7 @@ try app.run()
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub71.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub71.md,{}
27b0d1f2-8959-4bdb-8586-5124e25004ba,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub72.md,modified,1,1,2,"@@ -415,7 +415,7 @@ class PreprocImg(Executor):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub72.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub72.md,{}
4ae4e9d6-b3a2-4ba2-b034-2db8a940cd03,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub73.md,modified,1,1,2,"@@ -369,7 +369,7 @@ with pikepdf.open('input.pdf') as pdf:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub73.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub73.md,{}
22dc2968-b3cd-4cb7-8357-2ba3fcc8affe,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub74.md,modified,1,1,2,"@@ -355,7 +355,7 @@ xonshxxh anyhost +s xonsh
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub74.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub74.md,{}
d91942d7-61a3-4a15-82be-b8c36739499d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub75.md,modified,1,1,2,"@@ -365,7 +365,7 @@ struct ContentView: View {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub75.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub75.md,{}
7201694e-e39f-4e33-ba44-46d62279b67e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub76.md,modified,1,1,2,"@@ -379,7 +379,7 @@ tagger.predict(sentence)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub76.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub76.md,{}
00db1aa7-9cab-44e2-b6b9-9286b8909b4d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub77.md,modified,1,1,2,"@@ -332,7 +332,7 @@ mackup restore
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub77.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub77.md,{}
1f010030-35e3-4dbb-a084-801801046e43,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub78.md,modified,1,1,2,"@@ -379,7 +379,7 @@ image.save(""astronaut_rides_horse.png"")
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub78.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub78.md,{}
77330d3a-29b2-4084-824c-690ec96ed285,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub79.md,modified,1,1,2,"@@ -377,7 +377,7 @@ code2flow 
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub79.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub79.md,{}
d962200d-1ae4-44e2-8ee4-fbd802e4fcfb,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub80.md,modified,1,1,2,"@@ -281,7 +281,7 @@ ABC
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub80.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub80.md,{}
69c73324-e8fa-4aa9-886d-18d637dc9dae,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub81.md,modified,1,1,2,"@@ -286,7 +286,7 @@ for i in range(10):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub81.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub81.md,{}
d62f4d99-6454-4a5c-904f-4943dc6e7e73,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub82.md,modified,1,1,2,"@@ -343,7 +343,7 @@ git clone https://github.com/Botspot/pi-apps
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub82.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub82.md,{}
31babf8d-8293-46ee-b044-26457c6e139c,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub83.md,modified,1,1,2,"@@ -424,7 +424,7 @@ $ curl http://localhost:5000/predictions -X POST \
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub83.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub83.md,{}
020cf914-58df-45dd-9f89-e7b082959ad5,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub84.md,modified,1,1,2,"@@ -311,7 +311,7 @@ john = table.find_one(name='John Doe')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub84.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub84.md,{}
0f1e1c01-fbc4-4bab-85f4-996fcf4aa684,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub85.md,modified,1,1,2,"@@ -284,7 +284,7 @@ Audio(audio_array, rate=SAMPLE_RATE)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub85.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub85.md,{}
7c5d213e-3d1a-4106-8558-5b12db806e81,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub86.md,modified,2,2,4,"@@ -196,7 +196,7 @@ play(guitar, bpm=100, instrument=25)
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/86/379429942.gif' style=""max-width:80%; max-height=80%;""></img></p>
 
-31[oxipng](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/shssoichiro/oxipng)Multithreaded PNG Image Compression Tool. This is a Rust-written command-line lossless PNG compression tool that supports multithreading for fast compression and can also be used as a Rust library.
+31[oxipng](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/oxipng/oxipng)Multithreaded PNG Image Compression Tool. This is a Rust-written command-line lossless PNG compression tool that supports multithreading for fast compression and can also be used as a Rust library.
 
 ### Swift
 32[fsnotes](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/glushchenko/fsnotes)Note Manager for macOS/iOS. This is a note management tool suitable for macOS and iOS that supports Markdown, encrypted notes, web page generation, TouchBar shortcuts, syntax highlighting for over 170 programming languages, and content synchronization via iCloud Drive or Dropbox.
@@ -284,7 +284,7 @@ play(guitar, bpm=100, instrument=25)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub86.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub86.md,{}
57ab889e-d94c-4811-847d-3af7fec52eab,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub87.md,modified,1,1,2,"@@ -285,7 +285,7 @@ panorama = stitcher.stitch([""img?.jpg""])
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub87.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub87.md,{}
7c1c2f6b-5035-463b-8790-1d2ee41d4303,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub88.md,modified,1,1,2,"@@ -337,7 +337,7 @@ async def on_hello_world(msg: HelloWorld):
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub88.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub88.md,{}
4730ec0a-0aef-4057-a15b-7b0e04c40923,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub89.md,modified,1,1,2,"@@ -296,7 +296,7 @@ for div in divs:
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub89.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub89.md,{}
d948ef5c-e1d7-42f1-972a-32f88e8d8af6,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub90.md,modified,1,1,2,"@@ -333,7 +333,7 @@ print(grad(grad(grad(tanh)))(1.0))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub90.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub90.md,{}
508c1ca5-81ea-40da-b2d1-8ffa7764b817,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub91.md,modified,2,2,4,"@@ -243,7 +243,7 @@ async fn main() -> Result<(), Box<dyn std::error::Error>> {
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/91/31883122.png' style=""max-width:80%; max-height=80%;""></img></p>
 
 ### AI
-32[AISystem](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/chenzomi12/AISystem)AI Systems & Deep Learning Tutorials. This project primarily focuses on tutorials regarding the design of artificial intelligence and deep learning systems, covering aspects such as foundational knowledge and overviews of AI, AI chips, principles of AI compilers, and core technologies of AI frameworks.
+32[aisystem](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/chenzomi12/aisystem)AI Systems & Deep Learning Tutorials. This project primarily focuses on tutorials regarding the design of artificial intelligence and deep learning systems, covering aspects such as foundational knowledge and overviews of AI, AI chips, principles of AI compilers, and core technologies of AI frameworks.
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/91/539888500.png' style=""max-width:80%; max-height=80%;""></img></p>
 
@@ -319,7 +319,7 @@ async fn main() -> Result<(), Box<dyn std::error::Error>> {
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub91.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub91.md,{}
94954e68-9b66-4227-b850-94ec3a25e954,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub92.md,modified,1,1,2,"@@ -263,7 +263,7 @@ print(model(img))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub92.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub92.md,{}
4e15fdfd-edb3-490c-89e1-b6bbfe1944e4,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub93.md,modified,1,1,2,"@@ -252,7 +252,7 @@ print(len(detections))
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub93.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub93.md,{}
a8480172-10da-4b1c-acc7-0e1b9264e426,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub94.md,modified,1,1,2,"@@ -298,7 +298,7 @@ print(text_with_wm)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub94.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub94.md,{}
e326eb4f-7dd4-4f9f-b812-8826db6a577e,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub95.md,modified,1,1,2,"@@ -327,7 +327,7 @@ df.chat('Which are the 5 happiest countries?')
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub95.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub95.md,{}
827d75d4-c511-4683-b7f0-66013eb74693,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub96.md,modified,2,2,4,"@@ -119,7 +119,7 @@ XXH32_hash_t hash_string(const char* string, XXH32_hash_t seed)
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/96/237222619.png' style=""max-width:80%; max-height=80%;""></img></p>
 
 ### Python
-23[marker](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/VikParuchuri/marker)Project for Converting PDF to Markdown File. This is a Python project capable of converting PDF, EPUB, and MOBI formatted files into Markdown files. Compared to Nougat, it offers faster speed and higher accuracy, providing optimal results when handling English content, though the processing of Chinese can be less effective.
+23[marker](https://hellogithub.com/en/periodical/statistics/click?target=https://github.com/datalab-to/marker)Project for Converting PDF to Markdown File. This is a Python project capable of converting PDF, EPUB, and MOBI formatted files into Markdown files. Compared to Nougat, it offers faster speed and higher accuracy, providing optimal results when handling English content, though the processing of Chinese can be less effective.
 
 <p align=""center""><img src='https://raw.githubusercontent.com/521xueweihan/img3/master/hellogithub/96/712111618.png' style=""max-width:80%; max-height=80%;""></img></p>
 
@@ -265,7 +265,7 @@ Version: 2.7
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub96.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub96.md,{}
fc9f2c6e-60aa-4e99-a185-b7dbfdbeb43d,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub97.md,modified,1,1,2,"@@ -307,7 +307,7 @@ docker pull bitnami/APP:[TAG]
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub97.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub97.md,{}
29e32bf2-27d7-47df-a3ff-8546967d7360,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub98.md,modified,1,1,2,"@@ -295,7 +295,7 @@ print(response)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub98.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub98.md,{}
b8001da8-28e2-4d32-a27d-373004ed848b,4e2a815c-276b-4d6d-aaaa-7c1e5caf7624,content/en/HelloGitHub99.md,modified,1,1,2,"@@ -385,7 +385,7 @@ print(result)
         </a>
       </th>
       <th align=""center"" style=""width: 80px;"">
-        <a href=""https://www.qiniu.com/?utm_source=hello"">
+        <a href=""https://www.qiniu.com/products/ai-token-api?utm_source=hello"">
           <img src=""https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg"" width=""60px""><br>
           <sub></sub><br>
           <sub> Token </sub>",,md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\before\content_en_HelloGitHub99.md,.\tmp\github_file_blobs\521xueweihan\HelloGitHub\874b2bca9676ce254b55fa61103e405ad9f3f3ef\after\content_en_HelloGitHub99.md,{}
5ab8a67b-eef2-487b-aba0-c99b46619ac9,0626ac66-21fa-4e16-b844-2cc2656e9d47,docs/source/mps.md,renamed,26,8,34,"@@ -1,8 +1,14 @@
-torch.mps
-===================================
+# torch.mps
+
+```{eval-rst}
 .. automodule:: torch.mps
+```
+
+```{eval-rst}
 .. currentmodule:: torch.mps
+```
 
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
@@ -19,9 +25,11 @@ torch.mps
     driver_allocated_memory
     recommended_max_memory
     compile_shader
+```
 
-MPS Profiler
-------------
+## MPS Profiler
+
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
@@ -33,17 +41,27 @@ MPS Profiler
     profiler.is_capturing_metal
     profiler.is_metal_capture_enabled
     profiler.metal_capture
+```
+
+## MPS Event
 
-MPS Event
-------------
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
 
     event.Event
 
+```
 
-.. This module needs to be documented. Adding here in the meantime
-.. for tracking purposes
+% This module needs to be documented. Adding here in the meantime
+
+% for tracking purposes
+
+```{eval-rst}
 .. py:module:: torch.mps.event
+```
+
+```{eval-rst}
 .. py:module:: torch.mps.profiler
+```",docs/source/mps.rst,md,,.\tmp\github_file_blobs\pytorch\pytorch\d41f62b7a06c51e4a57df4d58e7a2d86d2faa875\after\docs_source_mps.md,{}
1b7ebeb3-654c-4b58-a9c6-a6432aad636b,0626ac66-21fa-4e16-b844-2cc2656e9d47,docs/source/mtia.md,renamed,12,4,16,"@@ -1,11 +1,16 @@
-torch.mtia
-===================================
+# torch.mtia
 
 The MTIA backend is implemented out of the tree, only interfaces are be defined here.
 
+```{eval-rst}
 .. automodule:: torch.mtia
+```
+
+```{eval-rst}
 .. currentmodule:: torch.mtia
+```
 
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
@@ -32,12 +37,15 @@ The MTIA backend is implemented out of the tree, only interfaces are be defined
     set_rng_state
     get_rng_state
     DeferredMtiaCallError
+```
+
+## Streams and events
 
-Streams and events
-------------------
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
 
     Event
     Stream
+```",docs/source/mtia.rst,md,,.\tmp\github_file_blobs\pytorch\pytorch\d41f62b7a06c51e4a57df4d58e7a2d86d2faa875\after\docs_source_mtia.md,{}
33378917-10a5-43dc-bf99-95fb3356d783,0626ac66-21fa-4e16-b844-2cc2656e9d47,docs/source/mtia.memory.md,renamed,8,2,10,"@@ -1,13 +1,19 @@
-torch.mtia.memory
-===================================
+# torch.mtia.memory
 
 The MTIA backend is implemented out of the tree, only interfaces are be defined here.
 
+```{eval-rst}
 .. automodule:: torch.mtia.memory
+```
+
+```{eval-rst}
 .. currentmodule:: torch.mtia.memory
+```
 
+```{eval-rst}
 .. autosummary::
     :toctree: generated
     :nosignatures:
 
     memory_stats
+```",docs/source/mtia.memory.rst,md,,.\tmp\github_file_blobs\pytorch\pytorch\d41f62b7a06c51e4a57df4d58e7a2d86d2faa875\after\docs_source_mtia.memory.md,{}
f416f8b4-c2ff-408f-8fd5-e075dac85daf,0626ac66-21fa-4e16-b844-2cc2656e9d47,docs/source/multiprocessing.md,renamed,116,93,209,"@@ -1,136 +1,139 @@
-:orphan:
+---
+orphan: true
+---
 
-.. _multiprocessing-doc:
+(multiprocessing-doc)=
 
-Multiprocessing package - torch.multiprocessing
-===============================================
+# Multiprocessing package - torch.multiprocessing
 
+```{eval-rst}
 .. automodule:: torch.multiprocessing
-.. currentmodule:: torch.multiprocessing
+```
 
-.. warning::
+```{eval-rst}
+.. currentmodule:: torch.multiprocessing
+```
 
-    If the main process exits abruptly (e.g. because of an incoming signal),
-    Python's ``multiprocessing`` sometimes fails to clean up its children.
-    It's a known caveat, so if you're seeing any resource leaks after
-    interrupting the interpreter, it probably means that this has just happened
-    to you.
+:::{warning}
+If the main process exits abruptly (e.g. because of an incoming signal),
+Python's `multiprocessing` sometimes fails to clean up its children.
+It's a known caveat, so if you're seeing any resource leaks after
+interrupting the interpreter, it probably means that this has just happened
+to you.
+:::
 
-Strategy management
--------------------
+## Strategy management
 
+```{eval-rst}
 .. autofunction:: get_all_sharing_strategies
+```
+
+```{eval-rst}
 .. autofunction:: get_sharing_strategy
+```
+
+```{eval-rst}
 .. autofunction:: set_sharing_strategy
 
+```
 
-.. _multiprocessing-cuda-sharing-details:
+(multiprocessing-cuda-sharing-details)=
 
-Sharing CUDA tensors
---------------------
+## Sharing CUDA tensors
 
 Sharing CUDA tensors between processes is supported only in Python 3, using
-a ``spawn`` or ``forkserver`` start methods.
-
+a `spawn` or `forkserver` start methods.
 
 Unlike CPU tensors, the sending process is required to keep the original tensor
 as long as the receiving process retains a copy of the tensor. The refcounting is
 implemented under the hood but requires users to follow the next best practices.
 
-.. warning::
-    If the consumer process dies abnormally to a fatal signal, the shared tensor
-    could be forever kept in memory as long as the sending process is running.
-
+:::{warning}
+If the consumer process dies abnormally to a fatal signal, the shared tensor
+could be forever kept in memory as long as the sending process is running.
+:::
 
 1. Release memory ASAP in the consumer.
 
-::
-
-    ## Good
-    x = queue.get()
-    # do somethings with x
-    del x
+```
+## Good
+x = queue.get()
+# do somethings with x
+del x
+```
 
-::
-
-    ## Bad
-    x = queue.get()
-    # do somethings with x
-    # do everything else (producer have to keep x in memory)
+```
+## Bad
+x = queue.get()
+# do somethings with x
+# do everything else (producer have to keep x in memory)
+```
 
 2. Keep producer process running until all consumers exits. This will prevent
 the situation when the producer process releasing memory which is still in use
 by the consumer.
 
-::
-
-    ## producer
-    # send tensors, do something
-    event.wait()
-
+```
+## producer
+# send tensors, do something
+event.wait()
+```
 
-::
-
-    ## consumer
-    # receive tensors and use them
-    event.set()
+```
+## consumer
+# receive tensors and use them
+event.set()
+```
 
 3. Don't pass received tensors.
 
-::
-
-    # not going to work
-    x = queue.get()
-    queue_2.put(x)
-
+```
+# not going to work
+x = queue.get()
+queue_2.put(x)
+```
 
-::
+```
+# you need to create a process-local copy
+x = queue.get()
+x_clone = x.clone()
+queue_2.put(x_clone)
+```
 
-    # you need to create a process-local copy
-    x = queue.get()
-    x_clone = x.clone()
-    queue_2.put(x_clone)
+```
+# putting and getting from the same queue in the same process will likely end up with segfault
+queue.put(tensor)
+x = queue.get()
+```
 
-
-::
-
-    # putting and getting from the same queue in the same process will likely end up with segfault
-    queue.put(tensor)
-    x = queue.get()
-
-
-Sharing strategies
-------------------
+## Sharing strategies
 
 This section provides a brief overview into how different sharing strategies
 work. Note that it applies only to CPU tensor - CUDA tensors will always use
 the CUDA API, as that's the only way they can be shared.
 
-File descriptor - ``file_descriptor``
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-
-.. note::
+### File descriptor - `file_descriptor`
 
-    This is the default strategy (except for macOS and OS X where it's not
-    supported).
+:::{note}
+This is the default strategy (except for macOS and OS X where it's not
+supported).
+:::
 
 This strategy will use file descriptors as shared memory handles. Whenever a
-storage is moved to shared memory, a file descriptor obtained from ``shm_open``
+storage is moved to shared memory, a file descriptor obtained from `shm_open`
 is cached with the object, and when it's going to be sent to other processes,
 the file descriptor will be transferred (e.g. via UNIX sockets) to it. The
-receiver will also cache the file descriptor and ``mmap`` it, to obtain a shared
+receiver will also cache the file descriptor and `mmap` it, to obtain a shared
 view onto the storage data.
 
 Note that if there will be a lot of tensors shared, this strategy will keep a
 large number of file descriptors open most of the time. If your system has low
 limits for the number of open file descriptors, and you can't raise them, you
-should use the ``file_system`` strategy.
+should use the `file_system` strategy.
 
-File system - ``file_system``
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+### File system - `file_system`
 
-This strategy will use file names given to ``shm_open`` to identify the shared
+This strategy will use file names given to `shm_open` to identify the shared
 memory regions. This has a benefit of not requiring the implementation to cache
 the file descriptors obtained from it, but at the same time is prone to shared
 memory leaks. The file can't be deleted right after its creation, because other
@@ -139,28 +142,27 @@ crash, or are killed, and don't call the storage destructors, the files will
 remain in the system. This is very serious, because they keep using up the
 memory until the system is restarted, or they're freed manually.
 
-To counter the problem of shared memory file leaks, :mod:`torch.multiprocessing`
-will spawn a daemon named ``torch_shm_manager`` that will isolate itself from
+To counter the problem of shared memory file leaks, {mod}`torch.multiprocessing`
+will spawn a daemon named `torch_shm_manager` that will isolate itself from
 the current process group, and will keep track of all shared memory allocations.
 Once all processes connected to it exit, it will wait a moment to ensure there
 will be no new connections, and will iterate over all shared memory files
 allocated by the group. If it finds that any of them still exist, they will be
 deallocated. We've tested this method and it proved to be robust to various
-failures. Still, if your system has high enough limits, and ``file_descriptor``
+failures. Still, if your system has high enough limits, and `file_descriptor`
 is a supported strategy, we do not recommend switching to this one.
 
-Spawning subprocesses
----------------------
+## Spawning subprocesses
 
-.. note::
+:::{note}
+Available for Python >= 3.4.
 
-   Available for Python >= 3.4.
-
-   This depends on the ``spawn`` start method in Python's
-   ``multiprocessing`` package.
+This depends on the `spawn` start method in Python's
+`multiprocessing` package.
+:::
 
 Spawning a number of subprocesses to perform some function can be done
-by creating ``Process`` instances and calling ``join`` to wait for
+by creating `Process` instances and calling `join` to wait for
 their completion. This approach works fine when dealing with a single
 subprocess but presents potential issues when dealing with multiple
 processes.
@@ -170,27 +172,48 @@ sequentially. If they don't, and the first process does not terminate,
 the process termination will go unnoticed. Also, there are no native
 facilities for error propagation.
 
-The ``spawn`` function below addresses these concerns and takes care
+The `spawn` function below addresses these concerns and takes care
 of error propagation, out of order termination, and will actively
 terminate processes upon detecting an error in one of them.
 
+```{eval-rst}
 .. automodule:: torch.multiprocessing.spawn
+```
+
+```{eval-rst}
 .. currentmodule:: torch.multiprocessing.spawn
+```
 
+```{eval-rst}
 .. autofunction:: spawn
+```
 
+```{eval-rst}
 .. currentmodule:: torch.multiprocessing
 
+```
 
+```{eval-rst}
 .. class:: SpawnContext
 
    Returned by :func:`~spawn` when called with ``join=False``.
 
    .. automethod:: join
 
+```
+
+% This module needs to be documented. Adding here in the meantime
 
-.. This module needs to be documented. Adding here in the meantime
-.. for tracking purposes
+% for tracking purposes
+
+```{eval-rst}
 .. py:module:: torch.multiprocessing.pool
+```
+
+```{eval-rst}
 .. py:module:: torch.multiprocessing.queue
+```
+
+```{eval-rst}
 .. py:module:: torch.multiprocessing.reductions
+```",docs/source/multiprocessing.rst,md,,.\tmp\github_file_blobs\pytorch\pytorch\d41f62b7a06c51e4a57df4d58e7a2d86d2faa875\after\docs_source_multiprocessing.md,{}
7e454071-3931-4e99-89bf-51f26eb285a1,0626ac66-21fa-4e16-b844-2cc2656e9d47,docs/source/name_inference.md,renamed,160,162,322,"@@ -1,11 +1,12 @@
+```{eval-rst}
 .. currentmodule:: torch
+```
 
-.. _name_inference_reference-doc:
+(name_inference_reference-doc)=
 
-Named Tensors operator coverage
-===============================
+# Named Tensors operator coverage
 
-Please read :ref:`named_tensors-doc` first for an introduction to named tensors.
+Please read {ref}`named_tensors-doc` first for an introduction to named tensors.
 
 This document is a reference for *name inference*, a process that defines how
 named tensors:
@@ -17,11 +18,13 @@ Below is a list of all operations that are supported with named tensors
 and their associated name inference rules.
 
 If you don't see an operation listed here, but it would help your use case, please
-`search if an issue has already been filed <https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22>`_ and if not, `file one <https://github.com/pytorch/pytorch/issues/new/choose>`_.
+[search if an issue has already been filed](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22) and if not, [file one](https://github.com/pytorch/pytorch/issues/new/choose).
 
-.. warning::
-    The named tensor API is experimental and subject to change.
+:::{warning}
+The named tensor API is experimental and subject to change.
+:::
 
+```{eval-rst}
 .. csv-table:: Supported Operations
    :header: API, Name inference rule
    :widths: 20, 20
@@ -244,226 +247,221 @@ If you don't see an operation listed here, but it would help your use case, plea
    :meth:`Tensor.zero_`,None
    :func:`torch.zeros`,:ref:`factory-doc`
 
+```
 
-.. _keeps_input_names-doc:
+(keeps_input_names-doc)=
 
-Keeps input names
-^^^^^^^^^^^^^^^^^
+## Keeps input names
 
 All pointwise unary functions follow this rule as well as some other unary functions.
 
 - Check names: None
 - Propagate names: input tensor's names are propagated to the output.
 
-::
+```
+>>> x = torch.randn(3, 3, names=('N', 'C'))
+>>> x.abs().names
+('N', 'C')
+```
 
-    >>> x = torch.randn(3, 3, names=('N', 'C'))
-    >>> x.abs().names
-    ('N', 'C')
+(removes_dimensions-doc)=
 
-.. _removes_dimensions-doc:
+## Removes dimensions
 
-Removes dimensions
-^^^^^^^^^^^^^^^^^^
-
-All reduction ops like :meth:`~Tensor.sum` remove dimensions by reducing
-over the desired dimensions. Other operations like :meth:`~Tensor.select` and
-:meth:`~Tensor.squeeze` remove dimensions.
+All reduction ops like {meth}`~Tensor.sum` remove dimensions by reducing
+over the desired dimensions. Other operations like {meth}`~Tensor.select` and
+{meth}`~Tensor.squeeze` remove dimensions.
 
 Wherever one can pass an integer dimension index to an operator, one can also pass
 a dimension name. Functions that take lists of dimension indices can also take in a
 list of dimension names.
 
-- Check names: If :attr:`dim` or :attr:`dims` is passed in as a list of names,
-  check that those names exist in :attr:`self`.
-- Propagate names: If the dimensions of the input tensor specified by :attr:`dim`
-  or :attr:`dims` are not present in the output tensor, then the corresponding names
-  of those dimensions do not appear in ``output.names``.
-
-::
-
-    >>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))
-    >>> x.squeeze('N').names
-    ('C', 'H', 'W')
+- Check names: If {attr}`dim` or {attr}`dims` is passed in as a list of names,
+  check that those names exist in {attr}`self`.
+- Propagate names: If the dimensions of the input tensor specified by {attr}`dim`
+  or {attr}`dims` are not present in the output tensor, then the corresponding names
+  of those dimensions do not appear in `output.names`.
 
-    >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
-    >>> x.sum(['N', 'C']).names
-    ('H', 'W')
+```
+>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))
+>>> x.squeeze('N').names
+('C', 'H', 'W')
 
-    # Reduction ops with keepdim=True don't actually remove dimensions.
-    >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
-    >>> x.sum(['N', 'C'], keepdim=True).names
-    ('N', 'C', 'H', 'W')
+>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
+>>> x.sum(['N', 'C']).names
+('H', 'W')
 
+# Reduction ops with keepdim=True don't actually remove dimensions.
+>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))
+>>> x.sum(['N', 'C'], keepdim=True).names
+('N', 'C', 'H', 'W')
+```
 
-.. _unifies_names_from_inputs-doc:
+(unifies_names_from_inputs-doc)=
 
-Unifies names from inputs
-^^^^^^^^^^^^^^^^^^^^^^^^^
+## Unifies names from inputs
 
 All binary arithmetic ops follow this rule. Operations that broadcast still
 broadcast positionally from the right to preserve compatibility with unnamed
-tensors. To perform explicit broadcasting by names, use :meth:`Tensor.align_as`.
+tensors. To perform explicit broadcasting by names, use {meth}`Tensor.align_as`.
 
 - Check names: All names must match positionally from the right. i.e., in
-  ``tensor + other``, ``match(tensor.names[i], other.names[i])`` must be true for all
-  ``i`` in ``(-min(tensor.dim(), other.dim()) + 1, -1]``.
+  `tensor + other`, `match(tensor.names[i], other.names[i])` must be true for all
+  `i` in `(-min(tensor.dim(), other.dim()) + 1, -1]`.
 - Check names: Furthermore, all named dimensions must be aligned from the right.
-  During matching, if we match a named dimension ``A`` with an unnamed dimension
-  ``None``, then ``A`` must not appear in the tensor with the unnamed dimension.
+  During matching, if we match a named dimension `A` with an unnamed dimension
+  `None`, then `A` must not appear in the tensor with the unnamed dimension.
 - Propagate names: unify pairs of names from the right from both tensors to
   produce output names.
 
 For example,
 
-::
-
-    # tensor: Tensor[   N, None]
-    # other:  Tensor[None,    C]
-    >>> tensor = torch.randn(3, 3, names=('N', None))
-    >>> other = torch.randn(3, 3, names=(None, 'C'))
-    >>> (tensor + other).names
-    ('N', 'C')
+```
+# tensor: Tensor[   N, None]
+# other:  Tensor[None,    C]
+>>> tensor = torch.randn(3, 3, names=('N', None))
+>>> other = torch.randn(3, 3, names=(None, 'C'))
+>>> (tensor + other).names
+('N', 'C')
+```
 
 Check names:
 
-- ``match(tensor.names[-1], other.names[-1])`` is ``True``
-- ``match(tensor.names[-2], tensor.names[-2])`` is ``True``
-- Because we matched ``None`` in :attr:`tensor` with ``'C'``,
-  check to make sure ``'C'`` doesn't exist in :attr:`tensor` (it does not).
-- Check to make sure ``'N'`` doesn't exists in :attr:`other` (it does not).
+- `match(tensor.names[-1], other.names[-1])` is `True`
+- `match(tensor.names[-2], tensor.names[-2])` is `True`
+- Because we matched `None` in {attr}`tensor` with `'C'`,
+  check to make sure `'C'` doesn't exist in {attr}`tensor` (it does not).
+- Check to make sure `'N'` doesn't exists in {attr}`other` (it does not).
 
 Finally, the output names are computed with
-``[unify('N', None), unify(None, 'C')] = ['N', 'C']``
-
-More examples::
-
-    # Dimensions don't match from the right:
-    # tensor: Tensor[N, C]
-    # other:  Tensor[   N]
-    >>> tensor = torch.randn(3, 3, names=('N', 'C'))
-    >>> other = torch.randn(3, names=('N',))
-    >>> (tensor + other).names
-    RuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims
-    ['N']: dim 'C' and dim 'N' are at the same position from the right but do
-    not match.
-
-    # Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:
-    # tensor: Tensor[N, None]
-    # other:  Tensor[      N]
-    >>> tensor = torch.randn(3, 3, names=('N', None))
-    >>> other = torch.randn(3, names=('N',))
-    >>> (tensor + other).names
-    RuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and
-    dims ['N', None]: dim 'N' appears in a different position from the right
-    across both lists.
-
-.. note::
-
-    In both of the last examples, it is possible to align the tensors by names
-    and then perform the addition. Use :meth:`Tensor.align_as` to align
-    tensors by name or :meth:`Tensor.align_to` to align tensors to a custom
-    dimension ordering.
-
-.. _permutes_dimensions-doc:
-
-Permutes dimensions
-^^^^^^^^^^^^^^^^^^^
-
-Some operations, like :meth:`Tensor.t()`, permute the order of dimensions. Dimension names
+`[unify('N', None), unify(None, 'C')] = ['N', 'C']`
+
+More examples:
+
+```
+# Dimensions don't match from the right:
+# tensor: Tensor[N, C]
+# other:  Tensor[   N]
+>>> tensor = torch.randn(3, 3, names=('N', 'C'))
+>>> other = torch.randn(3, names=('N',))
+>>> (tensor + other).names
+RuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims
+['N']: dim 'C' and dim 'N' are at the same position from the right but do
+not match.
+
+# Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:
+# tensor: Tensor[N, None]
+# other:  Tensor[      N]
+>>> tensor = torch.randn(3, 3, names=('N', None))
+>>> other = torch.randn(3, names=('N',))
+>>> (tensor + other).names
+RuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and
+dims ['N', None]: dim 'N' appears in a different position from the right
+across both lists.
+```
+
+:::{note}
+In both of the last examples, it is possible to align the tensors by names
+and then perform the addition. Use {meth}`Tensor.align_as` to align
+tensors by name or {meth}`Tensor.align_to` to align tensors to a custom
+dimension ordering.
+:::
+
+(permutes_dimensions-doc)=
+
+## Permutes dimensions
+
+Some operations, like {meth}`Tensor.t()`, permute the order of dimensions. Dimension names
 are attached to individual dimensions so they get permuted as well.
 
-If the operator takes in positional index :attr:`dim`, it is also able to take a dimension
-name as :attr:`dim`.
+If the operator takes in positional index {attr}`dim`, it is also able to take a dimension
+name as {attr}`dim`.
 
-- Check names: If :attr:`dim` is passed as a name, check that it exists in the tensor.
+- Check names: If {attr}`dim` is passed as a name, check that it exists in the tensor.
 - Propagate names: Permute dimension names in the same way as the dimensions that are
   being permuted.
 
-::
-
-    >>> x = torch.randn(3, 3, names=('N', 'C'))
-    >>> x.transpose('N', 'C').names
-    ('C', 'N')
+```
+>>> x = torch.randn(3, 3, names=('N', 'C'))
+>>> x.transpose('N', 'C').names
+('C', 'N')
+```
 
-.. _contracts_away_dims-doc:
+(contracts_away_dims-doc)=
 
-Contracts away dims
-^^^^^^^^^^^^^^^^^^^
+## Contracts away dims
 
 Matrix multiply functions follow some variant of this. Let's go through
-:func:`torch.mm` first and then generalize the rule for batch matrix multiplication.
+{func}`torch.mm` first and then generalize the rule for batch matrix multiplication.
 
-For ``torch.mm(tensor, other)``:
+For `torch.mm(tensor, other)`:
 
 - Check names: None
-- Propagate names: result names are ``(tensor.names[-2], other.names[-1])``.
-
-::
+- Propagate names: result names are `(tensor.names[-2], other.names[-1])`.
 
-    >>> x = torch.randn(3, 3, names=('N', 'D'))
-    >>> y = torch.randn(3, 3, names=('in', 'out'))
-    >>> x.mm(y).names
-    ('N', 'out')
+```
+>>> x = torch.randn(3, 3, names=('N', 'D'))
+>>> y = torch.randn(3, 3, names=('in', 'out'))
+>>> x.mm(y).names
+('N', 'out')
+```
 
 Inherently, a matrix multiplication performs a dot product over two dimensions,
 collapsing them. When two tensors are matrix-multiplied, the contracted dimensions
 disappear and do not show up in the output tensor.
 
-:func:`torch.mv`, :func:`torch.dot` work in a similar way: name inference does not
+{func}`torch.mv`, {func}`torch.dot` work in a similar way: name inference does not
 check input names and removes the dimensions that are involved in the dot product:
 
-::
+```
+>>> x = torch.randn(3, 3, names=('N', 'D'))
+>>> y = torch.randn(3, names=('something',))
+>>> x.mv(y).names
+('N',)
+```
 
-    >>> x = torch.randn(3, 3, names=('N', 'D'))
-    >>> y = torch.randn(3, names=('something',))
-    >>> x.mv(y).names
-    ('N',)
-
-Now, let's take a look at ``torch.matmul(tensor, other)``. Assume that ``tensor.dim() >= 2``
-and ``other.dim() >= 2``.
+Now, let's take a look at `torch.matmul(tensor, other)`. Assume that `tensor.dim() >= 2`
+and `other.dim() >= 2`.
 
 - Check names: Check that the batch dimensions of the inputs are aligned and broadcastable.
-  See :ref:`unifies_names_from_inputs-doc` for what it means for the inputs to be aligned.
+  See {ref}`unifies_names_from_inputs-doc` for what it means for the inputs to be aligned.
 - Propagate names: result names are obtained by unifying the batch dimensions and removing
   the contracted dimensions:
-  ``unify(tensor.names[:-2], other.names[:-2]) + (tensor.names[-2], other.names[-1])``.
-
-Examples::
-
-    # Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].
-    # 'A', 'B' are batch dimensions.
-    >>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))
-    >>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))
-    >>> torch.matmul(x, y).names
-    ('A', 'B', 'C', 'F')
+  `unify(tensor.names[:-2], other.names[:-2]) + (tensor.names[-2], other.names[-1])`.
 
+Examples:
 
-Finally, there are fused ``add`` versions of many matmul functions. i.e., :func:`addmm`
-and :func:`addmv`. These are treated as composing name inference for i.e. :func:`mm` and
-name inference for :func:`add`.
+```
+# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].
+# 'A', 'B' are batch dimensions.
+>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))
+>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))
+>>> torch.matmul(x, y).names
+('A', 'B', 'C', 'F')
+```
 
-.. _factory-doc:
+Finally, there are fused `add` versions of many matmul functions. i.e., {func}`addmm`
+and {func}`addmv`. These are treated as composing name inference for i.e. {func}`mm` and
+name inference for {func}`add`.
 
-Factory functions
-^^^^^^^^^^^^^^^^^
+(factory-doc)=
 
+## Factory functions
 
-Factory functions now take a new :attr:`names` argument that associates a name
+Factory functions now take a new {attr}`names` argument that associates a name
 with each dimension.
 
-::
+```
+>>> torch.zeros(2, 3, names=('N', 'C'))
+tensor([[0., 0., 0.],
+        [0., 0., 0.]], names=('N', 'C'))
+```
 
-    >>> torch.zeros(2, 3, names=('N', 'C'))
-    tensor([[0., 0., 0.],
-            [0., 0., 0.]], names=('N', 'C'))
+(out_function_semantics-doc)=
 
-.. _out_function_semantics-doc:
+## out function and in-place variants
 
-out function and in-place variants
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-
-A tensor specified as an ``out=`` tensor has the following behavior:
+A tensor specified as an `out=` tensor has the following behavior:
 
 - If it has no named dimensions, then the names computed from the operation
   get propagated to it.
@@ -473,13 +471,13 @@ A tensor specified as an ``out=`` tensor has the following behavior:
 All in-place methods modify inputs to have names equal to the computed names
 from name inference. For example:
 
-::
-
-    >>> x = torch.randn(3, 3)
-    >>> y = torch.randn(3, 3, names=('N', 'C'))
-    >>> x.names
-    (None, None)
+```
+>>> x = torch.randn(3, 3)
+>>> y = torch.randn(3, 3, names=('N', 'C'))
+>>> x.names
+(None, None)
 
-    >>> x += y
-    >>> x.names
-    ('N', 'C')
+>>> x += y
+>>> x.names
+('N', 'C')
+```",docs/source/name_inference.rst,md,,.\tmp\github_file_blobs\pytorch\pytorch\d41f62b7a06c51e4a57df4d58e7a2d86d2faa875\after\docs_source_name_inference.md,{}
ea2c058b-6d4c-4608-9f63-871ca97e1b5c,cfbdebe2-77fb-4bc3-b93e-d13ce197d145,docs/source/notes/cuda.rst,modified,5,3,8,"@@ -511,6 +511,8 @@ Available options:
   80% of the total memory allocated to the GPU application). The algorithm prefers
   to free old & unused blocks first to avoid freeing blocks that are actively being
   reused. The threshold value should be between greater than 0.0 and less than 1.0.
+  The default value is set at 1.0.
+
   ``garbage_collection_threshold`` is only meaningful with ``backend:native``.
   With ``backend:cudaMallocAsync``, ``garbage_collection_threshold`` is ignored.
 * ``expandable_segments`` (experimental, default: `False`) If set to `True`, this setting instructs
@@ -546,20 +548,20 @@ Available options:
   appended to the end of the segment. This process does not create as many slivers
   of unusable memory, so it is more likely to succeed at finding this memory.
 
-  `pinned_use_cuda_host_register` option is a boolean flag that determines whether to
+* `pinned_use_cuda_host_register` option is a boolean flag that determines whether to
   use the CUDA API's cudaHostRegister function for allocating pinned memory instead
   of the default cudaHostAlloc. When set to True, the memory is allocated using regular
   malloc and then pages are mapped to the memory before calling cudaHostRegister.
   This pre-mapping of pages helps reduce the lock time during the execution
   of cudaHostRegister.
 
-  `pinned_num_register_threads` option is only valid when pinned_use_cuda_host_register
+* `pinned_num_register_threads` option is only valid when pinned_use_cuda_host_register
   is set to True. By default, one thread is used to map the pages. This option allows
   using more threads to parallelize the page mapping operations to reduce the overall
   allocation time of pinned memory. A good value for this option is 8 based on
   benchmarking results.
 
-  `pinned_use_background_threads` option is a boolean flag to enable background thread
+* `pinned_use_background_threads` option is a boolean flag to enable background thread
   for processing events. This avoids any slow path associated with querying/processing of
   events in the fast allocation path. This feature is disabled by default.
 ",,rst,.\tmp\github_file_blobs\pytorch\pytorch\2908c10259bac21b00e9b36318e364801e0ae910\before\docs_source_notes_cuda.rst,.\tmp\github_file_blobs\pytorch\pytorch\2908c10259bac21b00e9b36318e364801e0ae910\after\docs_source_notes_cuda.rst,{}
f8542ade-9cdb-4ff2-8954-e1fa77671bc6,80374847-ddb0-4fa5-98f8-82f41fd99aea,test/dynamo/test_aot_autograd_cache.py,modified,39,0,39,"@@ -1,5 +1,6 @@
 # Owner(s): [""module: dynamo""]
 
+import copy
 import os
 import shutil
 import unittest
@@ -822,6 +823,44 @@ def fn(a, b):
         self.assertEqual(a.grad, a2.grad)
         self.assertEqual(b.grad, b2.grad)
 
+    @inductor_config.patch(""fx_graph_remote_cache"", False)
+    @inductor_config.patch({""fx_graph_cache"": True})
+    @functorch_config.patch({""enable_autograd_cache"": True})
+    @functorch_config.patch({""strict_autograd_cache"": True})
+    def test_autograd_no_dynamo_trace_backward(self):
+        """"""
+        Test that dynamo does not trace into the backward compiled function,
+        even on cache hit.
+        """"""
+        torch._dynamo.eval_frame.clear_dynamo_tls()
+
+        @torch.compile
+        def fn(x):
+            # Calls x.sum().backward() during forward execution of fn
+            (x_grad,) = torch.autograd.grad(x.sum(), x)
+            return x_grad
+
+        a = torch.randn(10, 10, requires_grad=True, device=""cpu"")
+        result = fn(a)
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_miss""], 1)
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_hit""], 0)
+        # Backward of `sum` will run during execution of graph break
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_saved""], 1)
+        traced_frame_infos = copy.deepcopy(
+            torch._dynamo.eval_frame.dynamo_tls.traced_frame_infos
+        )
+
+        torch._dynamo.reset()
+        torch._dynamo.eval_frame.clear_dynamo_tls()
+        result2 = fn(a)
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_miss""], 1)
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_hit""], 1)
+        self.assertEqual(counters[""aot_autograd""][""autograd_cache_saved""], 1)
+        new_traced_frame_infos = torch._dynamo.eval_frame.dynamo_tls.traced_frame_infos
+        self.assertEqual(result, result2)
+        # Dynamo should trace exactly the same frames on cache hit
+        self.assertEqual(traced_frame_infos, new_traced_frame_infos)
+
     @inductor_config.patch(""fx_graph_remote_cache"", False)
     @inductor_config.patch(""fx_graph_cache"", True)
     @functorch_config.patch({""enable_autograd_cache"": True})",,py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\before\test_dynamo_test_aot_autograd_cache.py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\after\test_dynamo_test_aot_autograd_cache.py,{}
1ebfbb77-bb9a-4563-bc76-4b8fb634f2d1,80374847-ddb0-4fa5-98f8-82f41fd99aea,torch/_dynamo/backends/common.py,modified,4,1,5,"@@ -68,7 +68,10 @@ def __call__(self, gm: torch.fx.GraphModule, example_inputs, **kwargs):
 
         def wrap_bw_compiler(bw_compiler_fn):
             def _wrapped_bw_compiler(*args, **kwargs):
-                # stop TorchDynamo from trying to compile our generated backwards pass
+                # Note [Wrapping bw_compiler in disable]
+                # The two disables here:
+                # - stop TorchDynamo from trying to compile the bw_compiler function itself
+                # - stop TorchDynamo from trying to compile our the generated backwards pass bw_compiler produces
                 return disable(
                     disable(
                         bw_compiler_fn, reason=""do not trace backward compiler function""",,py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\before\torch__dynamo_backends_common.py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\after\torch__dynamo_backends_common.py,{}
66d23e69-91d9-438d-ad35-16aea1ddd719,80374847-ddb0-4fa5-98f8-82f41fd99aea,torch/_functorch/_aot_autograd/autograd_cache.py,modified,17,1,18,"@@ -589,6 +589,15 @@ class CompiledBackward(GenericCompiledBackward[CompiledFxGraph], FxGraphCacheLoa
     def _is_backward(self) -> bool:
         return True
 
+    def post_compile(
+        self, result: CompiledFxGraph, fx_config: _CompileFxKwargs
+    ) -> CompiledFxGraph:
+        compiled_bw = super().post_compile(result, fx_config)
+        # See note [Wrapping bw_compiler in disable]
+        # This is done by _wrapped_bw_compiler in torch/_dynamo/backends/common.py
+        # But since on cache hit we do not call the bw_compiler, we need to reapply the disable
+        return torch._dynamo.disable(compiled_bw, reason=""do not trace generated backwards pass"")  # type: ignore[return-value]
+
 
 # Forward types don't have any extra parameters, so this is just a TypeAlias, in essence
 class BundledCompiledForward(CompiledFxGraphLoadable):
@@ -599,7 +608,14 @@ class BundledCompiledForward(CompiledFxGraphLoadable):
 class BundledCompiledBackward(
     GenericCompiledBackward[CompiledFxGraph], CompiledFxGraphLoadable
 ):
-    pass
+    def post_compile(
+        self, result: CompiledFxGraph, fx_config: _CompileFxKwargs
+    ) -> CompiledFxGraph:
+        compiled_bw = super().post_compile(result, fx_config)
+        # See note [Wrapping bw_compiler in disable]
+        # This is done by _wrapped_bw_compiler in torch/_dynamo/backends/common.py
+        # But since on cache hit we do not call the bw_compiler, we need to reapply the disable
+        return torch._dynamo.disable(compiled_bw, reason=""do not trace generated backwards pass"")  # type: ignore[return-value]
 
 
 TForward = TypeVar(""TForward"", bound=InductorOutput)",,py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\before\torch__functorch__aot_autograd_autograd_cache.py,.\tmp\github_file_blobs\pytorch\pytorch\be2ad70cfa1360da5c23a04ff6ca3480fa02f278\after\torch__functorch__aot_autograd_autograd_cache.py,{}
56ef0fb1-c175-474b-8126-84cac8e40146,4f0c3ba1-740d-4c41-9dfc-a7e1df301629,torch/utils/collect_env.py,modified,4,182,186,"@@ -10,7 +10,6 @@
 import subprocess
 import sys
 import os
-from typing import cast as _cast
 from collections import namedtuple
 
 
@@ -38,7 +37,6 @@
     'nvidia_driver_version',
     'nvidia_gpu_models',
     'cudnn_version',
-    'is_xpu_available',
     'pip_version',  # 'pip' or 'pip3'
     'pip_packages',
     'conda_packages',
@@ -75,30 +73,6 @@
     ""nvtx"",
 ]
 
-ONEAPI_PATTERNS = [
-    ""dpcpp-cpp-rt"",
-    ""intel-cmplr-lib-rt"",
-    ""intel-cmplr-lib-ur"",
-    ""intel-cmplr-lic-rt"",
-    ""intel-opencl-rt"",
-    ""intel-sycl-rt"",
-    ""mkl"",
-    ""onemkl-sycl-blas"",
-    ""onemkl-sycl-dft"",
-    ""onemkl-sycl-lapack"",
-    ""onemkl-sycl-rng"",
-    ""onemkl-sycl-sparse"",
-    ""intel-openmp"",
-    ""tbb"",
-    ""impi-rt"",
-    ""impi-devel"",
-    ""oneccl"",
-    ""oneccl-devel"",
-    ""intel-pti"",
-    ""umf"",
-    ""tcmlib"",
-]
-
 CONDA_PATTERNS = [
     ""cudatoolkit"",
     ""soumith"",
@@ -157,7 +131,7 @@ def run_and_return_first_line(run_lambda, command):
 
 def get_conda_packages(run_lambda, patterns=None):
     if patterns is None:
-        patterns = CONDA_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS
+        patterns = CONDA_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS
     conda = os.environ.get('CONDA_EXE', 'conda')
     out = run_and_read_all(run_lambda, ""{} list"".format(conda))
     if out is None:
@@ -269,149 +243,6 @@ def get_nvidia_smi():
     return smi
 
 
-def _detect_linux_pkg_manager():
-    if get_platform() != ""linux"":
-        return ""N/A""
-    for mgr_name in [""dpkg"", ""dnf"", ""yum"", ""zypper""]:
-        rc, _, _ = run(f""which {mgr_name}"")
-        if rc == 0:
-            return mgr_name
-    return ""N/A""
-
-
-def get_linux_pkg_version(run_lambda, pkg_name):
-    pkg_mgr = _detect_linux_pkg_manager()
-    if pkg_mgr == ""N/A"":
-        return ""N/A""
-
-    grep_version = {
-        ""dpkg"": {
-            ""field_index"": 2,
-            ""command"": ""dpkg -l | grep {}"",
-        },
-        ""dnf"": {
-            ""field_index"": 1,
-            ""command"": ""dnf list | grep {}"",
-        },
-        ""yum"": {
-            ""field_index"": 1,
-            ""command"": ""yum list | grep {}"",
-        },
-        ""zypper"": {
-            ""field_index"": 2,
-            ""command"": ""zypper info {} | grep Version"",
-        },
-    }
-
-    field_index: int = int(_cast(int, grep_version[pkg_mgr][""field_index""]))
-    cmd: str = str(grep_version[pkg_mgr][""command""])
-    cmd = cmd.format(pkg_name)
-    ret = run_and_read_all(run_lambda, cmd)
-    if ret == """":
-        return ""N/A""
-    lst = re.sub("" +"", "" "", ret).split("" "")
-    if len(lst) <= field_index:
-        return ""N/A""
-    return lst[field_index]
-
-
-def get_intel_gpu_driver_version(run_lambda):
-    lst = []
-    platform = get_platform()
-    if platform == ""linux"":
-        pkgs = {  # type: ignore[var-annotated]
-            ""dpkg"": {
-                ""intel-opencl-icd"",
-                ""libze1"",
-            },
-            ""dnf"": {
-                ""intel-opencl"",
-                ""level-zero"",
-            },
-            ""yum"": {
-                ""intel-opencl"",
-                ""level-zero"",
-            },
-            ""zypper"": {
-                ""intel-opencl"",
-                ""level-zero"",
-            },
-        }.get(_detect_linux_pkg_manager(), {})
-        for pkg in pkgs:
-            lst.append(f""* {pkg}:\t{get_linux_pkg_version(run_lambda, pkg)}"")
-    if platform in [""win32"", ""cygwin""]:
-        txt = run_and_read_all(
-            run_lambda,
-            'powershell.exe ""gwmi -Class Win32_PnpSignedDriver | where{$_.DeviceClass -eq \\""DISPLAY\\""\
-            -and $_.Manufacturer -match \\""Intel\\""} | Select-Object -Property DeviceName,DriverVersion,DriverDate\
-            | ConvertTo-Json""',
-        )
-        try:
-            obj = json.loads(txt)
-            if type(obj) is list:
-                for o in obj:
-                    lst.append(
-                        f'* {o[""DeviceName""]}: {o[""DriverVersion""]} ({o[""DriverDate""]})'
-                    )
-            else:
-                lst.append(f'* {obj[""DriverVersion""]} ({obj[""DriverDate""]})')
-        except ValueError as e:
-            lst.append(txt)
-            lst.append(str(e))
-    return ""\n"".join(lst)
-
-
-def get_intel_gpu_onboard(run_lambda):
-    lst: list[str] = []
-    platform = get_platform()
-    if platform == ""linux"":
-        txt = run_and_read_all(run_lambda, ""xpu-smi discovery -j"")
-        if txt:
-            try:
-                obj = json.loads(txt)
-                device_list = obj.get(""device_list"", [])
-                if isinstance(device_list, list) and device_list:
-                    lst.extend(f'* {device[""device_name""]}' for device in device_list)
-                else:
-                    lst.append(""N/A"")
-            except (ValueError, TypeError) as e:
-                lst.append(txt)
-                lst.append(str(e))
-        else:
-            lst.append(""N/A"")
-    if platform in [""win32"", ""cygwin""]:
-        txt = run_and_read_all(
-            run_lambda,
-            'powershell.exe ""gwmi -Class Win32_PnpSignedDriver | where{$_.DeviceClass -eq \\""DISPLAY\\""\
-            -and $_.Manufacturer -match \\""Intel\\""} | Select-Object -Property DeviceName | ConvertTo-Json""',
-        )
-        if txt:
-            try:
-                obj = json.loads(txt)
-                if isinstance(obj, list) and obj:
-                    lst.extend(f'* {device[""DeviceName""]}' for device in obj)
-                else:
-                    lst.append(f'* {obj.get(""DeviceName"", ""N/A"")}')
-            except ValueError as e:
-                lst.append(txt)
-                lst.append(str(e))
-        else:
-            lst.append(""N/A"")
-    return ""\n"".join(lst)
-
-
-def get_intel_gpu_detected(run_lambda):
-    if not TORCH_AVAILABLE or not hasattr(torch, ""xpu""):
-        return ""N/A""
-
-    device_count = torch.xpu.device_count()
-    if device_count == 0:
-        return ""N/A""
-
-    devices = [f""* [{i}] {torch.xpu.get_device_properties(i)}"" for i in range(device_count)]
-    return ""\n"".join(devices)
-
-
 # example outputs of CPU infos
 #  * linux
 #    Architecture:            x86_64
@@ -565,7 +396,7 @@ def get_os(run_lambda):
     from platform import machine
     platform = get_platform()
 
-    if platform in [""win32"", ""cygwin""]:
+    if platform == 'win32' or platform == 'cygwin':
         return get_windows_version(run_lambda)
 
     if platform == 'darwin':
@@ -606,7 +437,7 @@ def get_libc_version():
 def get_pip_packages(run_lambda, patterns=None):
     """"""Return `pip list` output. Note: will also find conda-installed pytorch and numpy packages.""""""
     if patterns is None:
-        patterns = PIP_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS
+        patterns = PIP_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS
 
     pip_version = 'pip3' if sys.version_info.major == 3 else 'pip'
 
@@ -673,13 +504,6 @@ def get_env_info():
         debug_mode_str = str(torch.version.debug)
         cuda_available_str = str(torch.cuda.is_available())
         cuda_version_str = torch.version.cuda
-        xpu_available_str = str(torch.xpu.is_available())
-        if torch.xpu.is_available():
-            xpu_available_str = f'{xpu_available_str}\n' + \
-                                f'XPU used to build PyTorch: {torch.version.xpu}\n' + \
-                                f'Intel GPU driver version:\n{get_intel_gpu_driver_version(run_lambda)}\n' + \
-                                f'Intel GPU models onboard:\n{get_intel_gpu_onboard(run_lambda)}\n' + \
-                                f'Intel GPU models detected:\n{get_intel_gpu_detected(run_lambda)}'
         if not hasattr(torch.version, 'hip') or torch.version.hip is None:  # cuda version
             hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'
         else:  # HIP version
@@ -693,7 +517,7 @@ def get_version_or_na(cfg, prefix):
             cuda_version_str = 'N/A'
             hip_compiled_version = torch.version.hip
     else:
-        version_str = debug_mode_str = cuda_available_str = cuda_version_str = xpu_available_str = 'N/A'
+        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'
         hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'
 
     sys_version = sys.version.replace(""\n"", "" "")
@@ -712,7 +536,6 @@ def get_version_or_na(cfg, prefix):
         nvidia_gpu_models=get_gpu_info(run_lambda),
         nvidia_driver_version=get_nvidia_driver_version(run_lambda),
         cudnn_version=get_cudnn_version(run_lambda),
-        is_xpu_available=xpu_available_str,
         hip_compiled_version=hip_compiled_version,
         hip_runtime_version=hip_runtime_version,
         miopen_runtime_version=miopen_runtime_version,
@@ -749,7 +572,6 @@ def get_version_or_na(cfg, prefix):
 GPU models and configuration: {nvidia_gpu_models}
 Nvidia driver version: {nvidia_driver_version}
 cuDNN version: {cudnn_version}
-Is XPU available: {is_xpu_available}
 HIP runtime version: {hip_runtime_version}
 MIOpen runtime version: {miopen_runtime_version}
 Is XNNPACK available: {is_xnnpack_available}",,py,.\tmp\github_file_blobs\pytorch\pytorch\6fb62931593fc10252e4994cd1a0595ebf8e990f\before\torch_utils_collect_env.py,.\tmp\github_file_blobs\pytorch\pytorch\6fb62931593fc10252e4994cd1a0595ebf8e990f\after\torch_utils_collect_env.py,{}
454033be-7338-45f3-bb5d-c576e574f976,04a97c86-48a4-48cc-b4c2-be4f4bec7341,build_variables.bzl,modified,1,0,1,"@@ -596,6 +596,7 @@ libtorch_nativert_sources = [
     ""torch/nativert/graph/TensorMeta.cpp"",
     ""torch/nativert/executor/Placement.cpp"",
     ""torch/nativert/executor/PlacementUtils.cpp"",
+    ""torch/nativert/executor/Weights.cpp"",
     ""torch/nativert/executor/memory/FunctionSchema.cpp"",
     ""torch/nativert/common/FileUtil.cpp"",
 ]",,bzl,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\before\build_variables.bzl,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\after\build_variables.bzl,{}
16076903-6629-447d-bf1d-23cd0579cdc6,04a97c86-48a4-48cc-b4c2-be4f4bec7341,test/cpp/nativert/CMakeLists.txt,modified,1,0,1,"@@ -9,6 +9,7 @@ set(NATIVERT_TEST_SRCS
   ${TORCH_ROOT}/torch/nativert/graph/Graph.cpp
   ${TORCH_ROOT}/torch/nativert/graph/GraphSignature.cpp
   ${TORCH_ROOT}/torch/nativert/executor/PlacementUtils.cpp
+  ${TORCH_ROOT}/torch/nativert/executor/Weights.cpp
   ${TORCH_ROOT}/torch/nativert/common/FileUtil.cpp
   ${TORCH_ROOT}/torch/nativert/executor/memory/FunctionSchema.cpp
 )",,txt,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\before\test_cpp_nativert_CMakeLists.txt,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\after\test_cpp_nativert_CMakeLists.txt,{}
6129b476-4c02-43b4-9a7b-60af0fe64d39,04a97c86-48a4-48cc-b4c2-be4f4bec7341,test/cpp/nativert/test_weights.cpp,added,92,0,92,"@@ -0,0 +1,92 @@
+#include <gtest/gtest.h>
+#include <torch/csrc/jit/serialization/pickle.h>
+#include <torch/custom_class.h>
+#include <torch/torch.h>
+#include <memory>
+
+#include <torch/nativert/executor/Placement.h>
+#include <torch/nativert/executor/Weights.h>
+#include <torch/nativert/graph/Graph.h>
+
+namespace torch::nativert {
+class WeightsTest : public ::testing::Test {
+ protected:
+  void SetUp() override {
+    static constexpr std::string_view source =
+        R""(graph(%foo, %bar, %baz):
+%o1, %o2 = aten.foo(self=%foo, target=%bar, alpha=0.1)
+return(%o2, %baz)
+)"";
+    graph = stringToGraph(source);
+    placement = std::make_unique<Placement>(c10::Device(c10::DeviceType::CPU));
+  }
+  std::shared_ptr<Graph> graph;
+  std::unique_ptr<Placement> placement;
+};
+TEST_F(WeightsTest, ConstructEmptyStateDict) {
+  std::unordered_map<std::string, c10::IValue> stateDict;
+  Weights weights(graph.get(), stateDict, *placement);
+  // Check that weights are initialized correctly
+  EXPECT_TRUE(weights.parameters().empty());
+  EXPECT_TRUE(weights.buffers().empty());
+  EXPECT_FALSE(weights.contains(""non_existent_weight""));
+}
+TEST_F(WeightsTest, SetAndGetValue) {
+  std::unordered_map<std::string, c10::IValue> stateDict;
+  Weights weights(graph.get(), stateDict, *placement);
+  at::Tensor tensor = at::ones({2, 2});
+  weights.setValue(""added_weight"", tensor);
+  EXPECT_TRUE(weights.contains(""added_weight""));
+  EXPECT_EQ(weights.at(""added_weight"").sizes(), tensor.sizes());
+}
+
+} // namespace torch::nativert
+
+using namespace ::testing;
+struct ContainsTensorDict : torch::CustomClassHolder {
+  explicit ContainsTensorDict(at::Tensor t) : t_(t) {}
+
+  explicit ContainsTensorDict(c10::Dict<std::string, at::Tensor> dict) {
+    t_ = dict.at(std::string(""init_tensor""));
+  }
+
+  c10::Dict<std::string, at::Tensor> serialize() const {
+    c10::Dict<std::string, at::Tensor> dict;
+    dict.insert(std::string(""init_tensor""), t_);
+    return dict;
+  }
+
+  at::Tensor t_;
+};
+
+static auto reg =
+    torch::class_<ContainsTensorDict>(""testing"", ""ContainsTensorDict"")
+        .def(torch::init<at::Tensor>())
+        .def_pickle(
+            // __getstate__
+            [](const c10::intrusive_ptr<ContainsTensorDict>& self)
+                -> c10::Dict<std::string, at::Tensor> {
+              return self->serialize();
+            },
+            // __setstate__
+            [](c10::Dict<std::string, at::Tensor> data)
+                -> c10::intrusive_ptr<ContainsTensorDict> {
+              return c10::make_intrusive<ContainsTensorDict>(std::move(data));
+            });
+
+TEST(CustomWeightsTest, TestCustomObjWithContainedTensor) {
+  // Save
+  auto customObj =
+      c10::make_intrusive<ContainsTensorDict>(torch::tensor({1, 2, 3}));
+  const auto bytes = torch::jit::pickle_save(c10::IValue(std::move(customObj)));
+
+  // Load
+  const auto loadedCustomObj =
+      torch::jit::pickle_load_obj(std::string{bytes.begin(), bytes.end()});
+  EXPECT_TRUE(loadedCustomObj.isObject());
+  EXPECT_EQ(
+      loadedCustomObj.to<c10::intrusive_ptr<ContainsTensorDict>>()
+          ->t_[0]
+          .item<int>(),
+      1);
+}",,cpp,,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\after\test_cpp_nativert_test_weights.cpp,{}
df9f0ee2-78a1-4bb6-aa51-64b656f44596,04a97c86-48a4-48cc-b4c2-be4f4bec7341,torch/nativert/executor/Weights.cpp,added,439,0,439,"@@ -0,0 +1,439 @@
+
+#include <c10/util/Logging.h>
+#include <utility>
+
+#include <torch/csrc/export/pt2_archive_constants.h>
+#include <torch/csrc/jit/serialization/import.h>
+#include <torch/csrc/jit/serialization/import_read.h>
+#include <torch/csrc/jit/serialization/pickle.h>
+#include <torch/nativert/executor/Weights.h>
+
+#ifndef AT_PER_OPERATOR_HEADERS
+#include <ATen/Functions.h>
+#else
+#include <ATen/ops/empty.h>
+#include <ATen/ops/empty_strided.h>
+#include <ATen/ops/scalar_tensor.h>
+#endif
+
+#include <c10/util/string_view.h>
+#include <caffe2/serialize/inline_container.h>
+
+namespace torch::nativert {
+
+WeightVersion Weights::globalVersion_ = 0;
+
+Weights::Weights(
+    const Graph* graph,
+    const std::optional<std::unordered_map<std::string, c10::IValue>>&
+        stateDict,
+    Placement placement)
+    : graph_(graph),
+      weightsMeta_(graph->weightsMeta()),
+      placement_(std::move(placement)),
+      version_(globalVersion_++) {
+  if (stateDict.has_value()) {
+    loadStateDict(stateDict.value());
+  }
+}
+
+Weights::Weights(
+    const Graph* graph,
+    std::shared_ptr<caffe2::serialize::PyTorchStreamReader> pytorchStreamReader,
+    const std::unordered_map<std::string, std::string>& stateDictPaths,
+    std::string_view stateDictPathPrefix,
+    const std::unordered_map<std::string, std::string>& constantPaths,
+    std::string_view constantPathPrefix,
+    Placement placement,
+    std::function<bool(const std::string&)> skipSizeCheck,
+    std::function<bool(const std::string&)> skipDtypeCheck)
+    : graph_(graph),
+      weightsMeta_(graph->weightsMeta()),
+      placement_(std::move(placement)),
+      version_(globalVersion_++),
+      skipSizeCheck_(std::move(skipSizeCheck)),
+      skipDtypeCheck_(std::move(skipDtypeCheck)) {
+  auto loadAndInsert =
+      [&](const std::string& tensorName,
+          std::string_view pathPrefix,
+          const std::unordered_map<std::string, std::string>& tensorPaths,
+          bool isUsed) {
+        auto pathIt = tensorPaths.find(tensorName);
+        TORCH_CHECK(
+            pathIt != tensorPaths.end(),
+            ""Couldn't find "",
+            tensorName,
+            "" in tensorPaths"");
+
+        const std::string tensorPath = std::string{pathPrefix} + pathIt->second;
+        VLOG(1) << ""Loading weight from: "" << tensorPath;
+        TORCH_CHECK(
+            pytorchStreamReader->hasRecord(tensorPath),
+            tensorPath,
+            "" not found"");
+
+        auto [tensorData, tensorDataSize] =
+            pytorchStreamReader->getRecord(tensorPath);
+
+        // TODO: We now have two copies of metadata for weights, one in
+        // model definition /models/<model_name>.json, another in
+        // /extra/xl_weights/<model_name>_model_param_config.json
+        // Currently, we only use the metadata from model definition.
+        std::optional<TensorMeta> tensorMeta;
+        if (weightsMeta_.find(tensorName) != weightsMeta_.end()) {
+          tensorMeta = weightsMeta_.at(tensorName);
+        } else {
+          TORCH_CHECK(false, ""Tensor meta not found for: "", tensorName);
+        }
+
+        if (tensorDataSize == 0 && tensorMeta->numel() > 0) {
+          VLOG(1) << ""Tensor "" << tensorName
+                  << "" does not have data and create on Meta device"";
+          allValues_[tensorName] = at::empty_strided(
+              tensorMeta->sizes(),
+              tensorMeta->strides(),
+              tensorMeta->asTensorOptions().device(at::kMeta));
+          return;
+        }
+
+        if (!isUsed) {
+          VLOG(1) << ""Tensor "" << tensorName << "" is not used during inference"";
+          auto targetDevice = placement_.getMappedDevice(tensorMeta->device());
+          allValues_[tensorName] =
+              at::scalar_tensor(0, at::TensorOptions().device(targetDevice));
+          return;
+        }
+
+        size_t bytesPerEntry =
+            c10::scalarTypeToTypeMeta(tensorMeta->dtype()).itemsize();
+        auto device = tensorData.device();
+        auto storage = c10::Storage(
+            c10::Storage::use_byte_size_t(),
+            at::detail::computeStorageNbytes(
+                tensorMeta->sizes(), tensorMeta->strides(), bytesPerEntry),
+            std::move(tensorData), // ownership is transferred
+            nullptr,
+            false);
+        const auto tensorOptions = at::TensorOptions(device)
+                                       .dtype(tensorMeta->dtype())
+                                       .requires_grad(false);
+        auto tensor =
+            at::empty({0}, tensorOptions)
+                .set_(storage, 0, tensorMeta->sizes(), tensorMeta->strides());
+
+        auto targetDevice = placement_.getMappedDevice(tensorMeta->device());
+        VLOG(1) << ""Loading weight "" << tensorName << "" on "" << targetDevice;
+        if (!isSameDevice(targetDevice, tensor.device())) {
+          tensor = tensor.to(targetDevice);
+        }
+
+        allValues_[tensorName] = tensor;
+      };
+
+  auto loadAndInsertParamsBuffers = [&](const auto& tensorName, bool isUsed) {
+    return loadAndInsert(
+        std::string(tensorName), stateDictPathPrefix, stateDictPaths, isUsed);
+  };
+
+  size_t weightIndex = 0;
+  bool isUsed = true;
+  const auto& weightValues = graph->weightValues();
+
+  for (const auto& tensorName : graph->signature().parameters()) {
+    isUsed = !weightValues[weightIndex]->users().empty();
+    if (!isUsed) {
+      unusedWeights_.insert(std::string(tensorName));
+    }
+    loadAndInsertParamsBuffers(tensorName, isUsed);
+    weightIndex++;
+  }
+  for (const auto& tensorName : graph->signature().buffers()) {
+    isUsed = !weightValues[weightIndex]->users().empty();
+    if (!isUsed) {
+      unusedWeights_.insert(std::string(tensorName));
+    }
+    loadAndInsertParamsBuffers(tensorName, isUsed);
+    weightIndex++;
+  }
+
+  // Load tensor constants and custom object constants, they are both stored
+  // in the same directory in the archive, i.e. ""extra/constants/"" tensor
+  // constants are prefixed with ""tensor_"" custom objects are prefixed with
+  // ""custom_obj_""
+  auto loadConstants = [&](const auto& constants) {
+    for (const auto& constantName : constants) {
+      auto pathIt = constantPaths.find(std::string(constantName));
+      TORCH_CHECK(
+          pathIt != constantPaths.end(),
+          ""Couldn't find "",
+          constantName,
+          "" in constantPaths"");
+      auto& fileName = pathIt->second;
+
+      if (c10::starts_with(
+              fileName,
+              torch::_export::archive_spec::TENSOR_CONSTANT_FILENAME_PREFIX)) {
+        // tensor constants
+        isUsed = !weightValues[weightIndex]->users().empty();
+        if (!isUsed) {
+          unusedWeights_.insert(std::string(constantName));
+        }
+        loadAndInsert(
+            std::string(constantName),
+            constantPathPrefix,
+            constantPaths,
+            isUsed);
+        weightIndex++;
+      } else {
+        TORCH_CHECK(false, ""Unknown constant path: "", fileName);
+      }
+    }
+  };
+  loadConstants(graph->signature().nonPersistentBuffers());
+  loadConstants(graph->signature().tensorConstants());
+
+  // custom object constants
+  for (const auto& customObjName : graph->signature().customObjs()) {
+    auto pathIt = constantPaths.find(std::string(customObjName));
+    TORCH_CHECK(
+        pathIt != constantPaths.end(),
+        ""Couldn't find "",
+        customObjName,
+        "" in constantPaths"");
+    auto& fileName = pathIt->second;
+
+    if (!c10::starts_with(
+            fileName,
+            torch::_export::archive_spec::CUSTOM_OBJ_FILENAME_PREFIX)) {
+      TORCH_CHECK(false, ""Unknown constant path: "", fileName);
+    }
+    std::string customObjPath = std::string{constantPathPrefix} + fileName;
+    LOG(INFO) << ""Loading custom object from: "" << customObjPath;
+
+    TORCH_CHECK(
+        pytorchStreamReader->hasRecord(customObjPath),
+        customObjPath,
+        "" not found"");
+
+    const auto& [customObjData, customObjDataSize] =
+        pytorchStreamReader->getRecord(customObjPath);
+
+    const char* customObjDataPtr =
+        reinterpret_cast<const char*>(customObjData.get());
+    std::string customObjBytes(
+        customObjDataPtr, customObjDataPtr + customObjDataSize);
+
+    c10::IValue customObj = torch::jit::pickle_load_obj(customObjBytes);
+    TORCH_CHECK(
+        customObj.isCustomClass(), ""Custom object is not a custom class"");
+    TORCH_CHECK(!customObj.isNone(), ""Custom object is None"");
+    customObjs_[std::string(customObjName)] = std::move(customObj);
+    customObjsPaths_[customObjPath] = std::string(customObjName);
+  }
+}
+
+std::unordered_map<std::string, at::Tensor> Weights::parameters() const {
+  std::unordered_map<std::string, at::Tensor> result;
+  for (const auto& name : graph_->signature().parameters()) {
+    result.emplace(name, allValues_.at(std::string(name)));
+  }
+  return result;
+}
+
+std::unordered_map<std::string, at::Tensor> Weights::buffers() const {
+  std::unordered_map<std::string, at::Tensor> result;
+  for (const auto& name : graph_->signature().buffers()) {
+    result.emplace(name, allValues_.at(std::string(name)));
+  }
+  return result;
+}
+
+std::unordered_map<std::string, at::Tensor> Weights::attributes() const {
+  return allValues_;
+}
+
+at::Tensor Weights::at(const std::string& name) const {
+  auto it = allValues_.find(name);
+  if (it != allValues_.end()) {
+    return it->second;
+  }
+
+  TORCH_CHECK(false, name, "" not found in Weights "", toString());
+}
+
+at::Tensor& Weights::at(const std::string& name) {
+  auto it = allValues_.find(name);
+  if (it != allValues_.end()) {
+    return it->second;
+  }
+
+  TORCH_CHECK(false, name, "" not found in Weights "", toString());
+}
+
+bool Weights::contains(const std::string& name) const {
+  return allValues_.find(name) != allValues_.end();
+}
+
+c10::IValue Weights::getCustomObj(const std::string& name) const {
+  auto it = customObjs_.find(name);
+  if (it != customObjs_.end()) {
+    return it->second;
+  }
+
+  TORCH_CHECK(false, ""Custom objects "", name, "" not found in Weights"");
+}
+
+c10::IValue Weights::getCustomObjByFileName(const std::string& name) const {
+  auto it = customObjsPaths_.find(name);
+  TORCH_CHECK(
+      it != customObjsPaths_.end(),
+      ""Custom objects with file name "",
+      name,
+      "" not found in Weights"");
+  const std::string obj_name = it->second;
+  return getCustomObj(obj_name);
+}
+
+void Weights::loadStateDict(
+    const std::unordered_map<std::string, c10::IValue>& stateDict) {
+  auto validateAndInsert = [&](const std::string& name) {
+    auto stateDictIt = stateDict.find(name);
+    TORCH_CHECK(
+        stateDictIt != stateDict.end(),
+        ""Couldn't find "",
+        name,
+        "" in stateDict"");
+
+    // Verify that the tensor matches the tensorMeta
+    auto it = weightsMeta_.find(name);
+    TORCH_CHECK(
+        it != weightsMeta_.end(), ""Couldn't find "", name, "" in weightsMeta"");
+
+    auto targetDevice = placement_.getMappedDevice(it->second.device());
+    auto tensor = stateDictIt->second.toTensor().to(targetDevice);
+
+    TORCH_CHECK(tensor.sizes() == it->second.sizes());
+    TORCH_CHECK(tensor.dtype() == it->second.dtype());
+
+    allValues_.emplace(name, tensor);
+  };
+
+  for (const auto& name : graph_->signature().parameters()) {
+    validateAndInsert(std::string(name));
+  }
+  for (const auto& name : graph_->signature().buffers()) {
+    validateAndInsert(std::string(name));
+  }
+  // TensorConstants_ not filled !!
+}
+
+void Weights::validateValue(const std::string& name, const at::Tensor& newValue)
+    const {
+  auto& weightMeta = weightsMeta_.at(name);
+
+  TORCH_CHECK(
+      weightMeta.sizes() == newValue.sizes() ||
+          (skipSizeCheck_ && skipSizeCheck_(name)) ||
+          unusedWeights_.find(name) != unusedWeights_.end(),
+      ""Mismatched sizes for "",
+      name,
+      "": "",
+      weightMeta.sizes(),
+      "" vs "",
+      newValue.sizes());
+  TORCH_CHECK(
+      weightMeta.dtype() == newValue.dtype() ||
+          (skipDtypeCheck_ && skipDtypeCheck_(name)) ||
+          unusedWeights_.find(name) != unusedWeights_.end(),
+      ""Mismatched dtype for "",
+      name,
+      "": "",
+      weightMeta.dtype(),
+      "" vs "",
+      newValue.dtype());
+
+  auto targetDevice = placement_.getMappedDevice(weightMeta.device());
+  if (targetDevice.is_cpu() && targetDevice.has_index()) {
+    LOG(WARNING) << ""Target device is cpu but has index: "" << targetDevice;
+  }
+  TORCH_CHECK(
+      isSameDevice(targetDevice, newValue.device()),
+      ""Mismatched device for "",
+      name,
+      "": "",
+      targetDevice,
+      "" vs "",
+      newValue.device());
+}
+
+void Weights::setValue(const std::string& name, const at::Tensor& newValue) {
+  if (allValues_.find(name) != allValues_.end()) {
+    validateValue(name, newValue);
+  } else {
+    LOG(WARNING) << name << "" is not found in the registered weights"";
+  }
+
+  allValues_[name] = newValue;
+}
+
+void Weights::updateValue(const std::string& name, const at::Tensor& newValue) {
+  auto it = allValues_.find(name);
+  TORCH_CHECK(
+      it != allValues_.end(), name, "" not found in Weights "", toString());
+  validateValue(name, newValue);
+
+  it->second.copy_(newValue);
+}
+
+void Weights::updateValues(
+    const std::unordered_map<std::string, at::Tensor>& newValues) {
+  for (auto& [name, newValue] : newValues) {
+    updateValue(name, newValue);
+  }
+}
+
+std::string Weights::toString() const {
+  std::stringstream ss;
+  ss << '[';
+  for (const auto& [name, _] : allValues_) {
+    ss << name << "", "";
+  }
+  ss << ']';
+  ss << '[';
+  for (const auto& [name, _] : customObjs_) {
+    ss << name << "", "";
+  }
+  ss << ']';
+  return ss.str();
+}
+
+void Weights::validateAllWeightsLoaded() {
+  auto checkNames = [&](const auto& names) {
+    for (const auto& name : names) {
+      if (unusedWeights_.find(std::string(name)) != unusedWeights_.end()) {
+        continue;
+      }
+      auto it = allValues_.find(std::string(name));
+      TORCH_CHECK(it != allValues_.end(), ""Missing weight: "", name);
+      TORCH_CHECK(it->second.defined(), ""Weight not defined: "", name);
+      if (it->second.device().is_meta()) {
+        LOG(WARNING) << ""Weight is on meta device: "" << name;
+      }
+    }
+  };
+  checkNames(graph_->signature().parameters());
+  checkNames(graph_->signature().buffers());
+  checkNames(graph_->signature().nonPersistentBuffers());
+  checkNames(graph_->signature().tensorConstants());
+}
+
+void Weights::updateFoldedConst(std::string_view name, c10::IValue tensor) {
+  foldedConstsMap_[std::string{name}] = std::move(tensor);
+}
+
+const std::unordered_map<std::string, c10::IValue>& Weights::getFoldedConsts()
+    const {
+  return foldedConstsMap_;
+}
+
+} // namespace torch::nativert",,cpp,,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\after\torch_nativert_executor_Weights.cpp,{}
8cc4cf96-2b96-4ada-94ba-72a41391de00,04a97c86-48a4-48cc-b4c2-be4f4bec7341,torch/nativert/executor/Weights.h,added,145,0,145,"@@ -0,0 +1,145 @@
+#pragma once
+
+#include <unordered_map>
+
+#include <c10/util/FbcodeMaps.h>
+#include <c10/util/Logging.h>
+#include <caffe2/serialize/inline_container.h>
+#include <torch/nativert/executor/Placement.h>
+
+#include <torch/nativert/graph/Graph.h>
+
+namespace torch::nativert {
+
+using WeightVersion = int;
+/**
+ * @brief A class that manages the weights of a graph, providing functionality
+ * to load, access, and manipulate them.
+ *
+ * It is responsible for handling the parameters, buffers, and constants
+ * associated with a graph It provides mechanisms to load weights from
+ * serialized data, access and modify them, and performs necessary validation
+ * checks.
+ */
+class Weights {
+ public:
+  explicit Weights(
+      const Graph* graph,
+      const std::optional<std::unordered_map<std::string, c10::IValue>>&
+          stateDict = std::nullopt,
+      Placement placement = Placement());
+
+  // Arguments
+  // - pytorchStreamReader: the reader for the model archive
+  // - stateDictPath: a map from parameter/buffer/constant name to file path in
+  // the archive
+  // - stateDictPathPrefix: a prefix that will be prepended to paths in
+  // stateDictPathPrefix
+  // - constantPaths: a map from constant name to file path in the archive
+  // - constantPathPrefix: a prefix that will be prepended to paths in
+  // constantPathPrefix
+  // - placement: the device placement of the weights, default to follow the
+  //   original device in the weight's metadata
+  explicit Weights(
+      const Graph* graph,
+      std::shared_ptr<caffe2::serialize::PyTorchStreamReader>
+          pytorchStreamReader,
+      const std::unordered_map<std::string, std::string>& stateDictPaths,
+      std::string_view stateDictPathPrefix,
+      const std::unordered_map<std::string, std::string>& constantPaths,
+      std::string_view constantPathPrefix,
+      Placement placement = Placement(),
+      std::function<bool(const std::string&)> skipSizeCheck = {},
+      std::function<bool(const std::string&)> skipDtypeCheck = {});
+
+  at::Tensor at(const std::string& name) const;
+  at::Tensor& at(const std::string& name);
+  bool contains(const std::string& name) const;
+  c10::IValue getCustomObj(const std::string& name) const;
+  c10::IValue getCustomObjByFileName(const std::string& name) const;
+
+  std::unordered_map<std::string, at::Tensor> parameters() const;
+
+  std::unordered_map<std::string, at::Tensor> buffers() const;
+
+  std::unordered_map<std::string, at::Tensor> attributes() const;
+
+  void loadStateDict(
+      const std::unordered_map<std::string, c10::IValue>& stateDict);
+
+  /*
+   * Replace the value stored at the weight with name ""name"".
+   */
+  void setValue(const std::string& name, const at::Tensor& newValue);
+
+  /*
+   * Update the value stored at the weight with name ""name"".
+   * This is done in-place.
+   */
+  void updateValue(const std::string& name, const at::Tensor& newValue);
+
+  void updateValues(
+      const std::unordered_map<std::string, at::Tensor>& newValues);
+
+  void validateValue(const std::string& name, const at::Tensor& newValue) const;
+
+  void validateAllWeightsLoaded();
+
+  void updateFoldedConst(std::string_view name, c10::IValue tensor);
+
+  const std::unordered_map<std::string, c10::IValue>& getFoldedConsts() const;
+
+  C10_ALWAYS_INLINE const c10::FastMap<std::string, c10::IValue>&
+  getConstFoldedValues() const {
+    return constFoldedValues_;
+  }
+
+  C10_ALWAYS_INLINE void setConstFoldedValue(
+      const std::string& n,
+      c10::IValue iv) {
+    constFoldedValues_.insert_or_assign(n, std::move(iv));
+  }
+
+  std::string toString() const;
+
+  WeightVersion version() const {
+    return version_;
+  }
+
+ private:
+  const Graph* graph_;
+  const std::unordered_map<std::string, TensorMeta>& weightsMeta_;
+  Placement placement_;
+
+  // keys are parameter/buffer/constant names, not graph input names!
+  std::unordered_map<std::string, at::Tensor> allValues_;
+
+  std::unordered_map<std::string, c10::IValue> customObjs_;
+
+  // contains CustomClassHolder map from a file name to an arbitray
+  // key in customObjs_ that hold the loaded content of the file.
+  // This is used in AOTIDelegateExecutor.
+  std::unordered_map<std::string, std::string> customObjsPaths_;
+
+  // The liftcycle of folded consts should be tied with the weights from which
+  // it was derived. The ordering of the constant should be consistent with
+  // the output order of const graph.
+  std::vector<c10::IValue> foldedConsts_;
+  std::unordered_map<std::string, c10::IValue> foldedConstsMap_;
+
+  c10::FastMap<std::string, c10::IValue> constFoldedValues_;
+
+  // unique version number for this instance of weight
+  const WeightVersion version_;
+
+  // every instance of Weight has a unique version number
+  static WeightVersion globalVersion_;
+
+  std::function<bool(const std::string&)> skipSizeCheck_ = {};
+  std::function<bool(const std::string&)> skipDtypeCheck_ = {};
+
+  // save the names of unused weights
+  std::unordered_set<std::string> unusedWeights_;
+};
+
+} // namespace torch::nativert",,h,,.\tmp\github_file_blobs\pytorch\pytorch\9b4a748e29a720d0fade7e1298a68cc36cfd5f5e\after\torch_nativert_executor_Weights.h,{}
040a70c5-2f28-4197-831f-1ed09ac7bfad,e80cec77-911e-4a92-8dc7-26c456f8b464,test/distributed/_composable/fsdp/test_fully_shard_compile.py,modified,10,1,11,"@@ -543,7 +543,16 @@ def test_compiled():
                 )
                 if fwd_fullgraph:
                     self.assertEqual(len(counters[""graph_break""]), 1)
-                    self.assertIn(""Tensor.backward"", counters[""graph_break""])
+                    self.assertExpectedInline(
+                        next(iter(counters[""graph_break""].keys())),
+                        """"""\
+Unsupported Tensor.backward() call
+  Explanation: Dynamo currently does not support tracing `Tensor.backward()`.
+  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.
+
+  Developer debug context: call_method TensorVariable() backward () {}
+"""""",  # noqa: B950
+                    )
                 else:
                     self.assertGreater(len(counters[""graph_break""]), 1)
                 return res",,py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\before\test_distributed__composable_fsdp_test_fully_shard_compile.py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\after\test_distributed__composable_fsdp_test_fully_shard_compile.py,{}
153dfc71-5c98-4a43-a269-a626ba050fcd,e80cec77-911e-4a92-8dc7-26c456f8b464,test/dynamo/test_error_messages.py,modified,6,1,7,"@@ -96,7 +96,12 @@ def fn(x):
                 torch.Tensor([1])
             ),
             """"""\
-Tensor.item
+Unsupported Tensor.item() call with capture_scalar_outputs=False
+  Explanation: Dynamo does not support tracing `Tensor.item()` with config.capture_scalar_outputs=False.
+  Hint: Set `torch._dynamo.config.capture_scalar_outputs = True` or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` to include these operations in the captured graph.
+
+  Developer debug context: call_method TensorVariable() item () {}
+
 
 from user code:
    File ""test_error_messages.py"", line N, in fn",,py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\before\test_dynamo_test_error_messages.py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\after\test_dynamo_test_error_messages.py,{}
c697429d-c763-457d-b858-9f4bfdc840c7,e80cec77-911e-4a92-8dc7-26c456f8b464,test/dynamo/test_reorder_logs.py,modified,10,1,11,"@@ -202,7 +202,16 @@ def f(x):
 
         graph_break_key = counters[""graph_break""].keys()
         self.assertEqual(len(graph_break_key), 1)
-        self.assertEqual(next(iter(graph_break_key)), ""Tensor.item"")
+        self.assertExpectedInline(
+            next(iter(graph_break_key)),
+            """"""\
+Unsupported Tensor.item() call with capture_scalar_outputs=False
+  Explanation: Dynamo does not support tracing `Tensor.item()` with config.capture_scalar_outputs=False.
+  Hint: Set `torch._dynamo.config.capture_scalar_outputs = True` or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` to include these operations in the captured graph.
+
+  Developer debug context: call_method TensorVariable() item () {}
+"""""",  # noqa: B950
+        )
 
 
 if __name__ == ""__main__"":",,py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\before\test_dynamo_test_reorder_logs.py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\after\test_dynamo_test_reorder_logs.py,{}
2a6b9d58-0f90-48bf-b080-9bb27870d507,e80cec77-911e-4a92-8dc7-26c456f8b464,torch/_dynamo/variables/tensor.py,modified,162,26,188,"@@ -45,7 +45,6 @@
 from .. import config, graph_break_hints, variables
 from .._trace_wrapped_higher_order_op import trace_wrapped
 from ..exc import (
-    unimplemented,
     unimplemented_v2,
     UnknownPropertiesDuringBackwardTrace,
     UserError,
@@ -376,7 +375,12 @@ def method_attr_is_nested(self, tx):
             return ConstantVariable.create(self.is_nested)
 
     def method_attr_retain_grad(self, tx):
-        unimplemented(""retain_grad does not work with AOTDispatcher"")
+        unimplemented_v2(
+            gb_type=""Tensor.retain_grad() with AOTDispatcher"",
+            context=f""var_getattr {self} retain_grad"",
+            explanation=""`Tensor.retain_grad()` does not work with AOTDispatcher."",
+            hints=[],
+        )
 
     def method_attr_data(self, tx):
         return variables.TorchInGraphFunctionVariable(
@@ -385,7 +389,12 @@ def method_attr_data(self, tx):
 
     def method_attr_grad_fn(self, tx):
         if self.has_grad_fn:
-            unimplemented(""TensorVariable has a grad_fn"")
+            unimplemented_v2(
+                gb_type=""Tensor with grad_fn()"",
+                context=f""var_getattr {self} grad_fn"",
+                explanation=""Dynamo does not support tracing tensors with a grad_fn directly."",
+                hints=[],
+            )
         else:
             return variables.ConstantVariable(None)
 
@@ -427,8 +436,14 @@ def call_obj_hasattr(self, tx: ""InstructionTranslator"", name):
     def var_getattr(self, tx: ""InstructionTranslator"", name):
         if self.is_strict_mode(tx):
             if name in self._strict_mode_banned_ops():
-                unimplemented(
-                    f""Getattr invocation {name} in strict mode is not supported""
+                unimplemented_v2(
+                    gb_type=""Strict mode banned op"",
+                    context=f""var_getattr {self} {name}"",
+                    explanation=f""Getattr invocation '{name}' in strict mode is not supported."",
+                    hints=[
+                        f""Remove `{name}` from the list of banned ops by ""
+                        ""setting `torch._dynamo.config._autograd_backward_strict_mode_banned_ops`."",
+                    ],
                 )
             elif name in self._strict_mode_conditional_banned_ops():
                 raise UnknownPropertiesDuringBackwardTrace(
@@ -511,17 +526,34 @@ def try_generic_attr_handling():
 
     def call_id(self, tx):
         if not self.source:
-            unimplemented(""call_id not supported for sourceless TensorVariable"")
+            unimplemented_v2(
+                gb_type=""Unsupported call_id() without source"",
+                context=f""call_id {self}"",
+                explanation=""call_id() not supported for sourceless TensorVariable."",
+                hints=[],
+            )
 
         # For local source, we associate the real value. We use this real value
         scope = {""L"": tx.output.local_scope, ""G"": tx.output.global_scope}
         try:
             _input_associated_real_value = eval(self.source.name(), scope)
         except Exception as exc:
-            unimplemented(f""error getting associated real value: {exc}"")
+            unimplemented_v2(
+                gb_type=""Error getting associated real value"",
+                context=f""call_id {self}"",
+                explanation=""Dynamo encountered an error while trying to ""
+                ""get the associated real value."",
+                hints=[],
+                from_exc=exc,
+            )
 
         if _input_associated_real_value is None:
-            unimplemented(""call_id without associated real value"")
+            unimplemented_v2(
+                gb_type=""call_id() without associated real value"",
+                context=f""call_id {self}"",
+                explanation=""Dynamo could not find an associated real value for the tensor."",
+                hints=[],
+            )
 
         install_guard(self.source.make_guard(GuardBuilder.ID_MATCH))
         id_value = id(_input_associated_real_value)
@@ -592,7 +624,13 @@ def call_method(
         from .torch_function import can_dispatch_torch_function, dispatch_torch_function
 
         if self.is_strict_mode(tx) and name in self._strict_mode_banned_ops():
-            unimplemented(f""Illegal method invocation {name} in strict mode"")
+            unimplemented_v2(
+                gb_type=""Illegal method invocation in strict mode"",
+                context=f""call_method {self} {name} {args} {kwargs}"",
+                explanation=""Dynamo currently does not support this method ""
+                f""({name}) invocation in strict mode."",
+                hints=[],
+            )
 
         # Only override builtin tensor methods
         # The user can manually add override handling
@@ -660,7 +698,14 @@ def call_method(
                 if result:
                     return result
             except TypeError as e:
-                unimplemented(f""unhandled args for {name}: {e}"")
+                unimplemented_v2(
+                    gb_type=""Unhandled args for method"",
+                    context=f""call_method {self} {name} {args} {kwargs}"",
+                    explanation=""Dynamo encountered an error while calling ""
+                    f""the method `{name}`."",
+                    hints=[],
+                    from_exc=e,
+                )
 
         from .builder import wrap_fx_proxy
 
@@ -850,9 +895,26 @@ def method_element_size(self):
 
     def method_numpy(self, *, force=False):
         if not config.trace_numpy:
-            unimplemented(""Tensor.numpy(). config.trace_numpy is False"")
+            unimplemented_v2(
+                gb_type=""Tensor.numpy() with trace_numpy=False"",
+                context=f""call_method {self} numpy"",
+                explanation=""`Tensor.numpy()` was called, but the `trace_numpy` ""
+                ""configuration was manually disabled."",
+                hints=[
+                    ""Set `torch._dynamo.config.trace_numpy = True` to allow ""
+                    ""Dynamo to trace through NumPy."",
+                ],
+            )
         if not np:
-            unimplemented(""Tensor.numpy(). NumPy is not available"")
+            unimplemented_v2(
+                gb_type=""Tensor.numpy() without NumPy installed"",
+                context=f""call_method {self} numpy"",
+                explanation=""`Tensor.numpy()` was called, but the NumPy library ""
+                ""is not available in the current environment."",
+                hints=[
+                    ""Ensure NumPy is installed in your Python environment."",
+                ],
+            )
         if self.layout != torch.strided:
             raise TypeError(
                 f""can't convert {self.layout} layout tensor to numpy. Use Tensor.to_dense() first""
@@ -898,7 +960,16 @@ def wrap(i, sub_proxy):
                 torch.int32,
                 torch.int64,
             ]:
-                unimplemented(""Input tensor for tolist must be an integer tensor"")
+                unimplemented_v2(
+                    gb_type=""Tensor.tolist() with non-integer tensor"",
+                    context=f""call_method {self} to_list"",
+                    explanation=""Dynamo currently does not support tracing ""
+                    ""`tolist()` on non-integer tensors."",
+                    hints=[
+                        ""Ensure the input tensor to `tolist()` is an integer ""
+                        ""type (e.g., int8, int16, int32, int64).""
+                    ],
+                )
 
             if tensor.dim() == 0:
                 return wrap(tensor, sub_proxy)
@@ -916,15 +987,30 @@ def wrap(i, sub_proxy):
         return VariableTracker.build(tx, out)
 
     def method_backward(self, *args, **kwargs):
-        unimplemented(""Tensor.backward"")
+        unimplemented_v2(
+            gb_type=""Unsupported Tensor.backward() call"",
+            context=f""call_method {self} backward {args} {kwargs}"",
+            explanation=""Dynamo currently does not support tracing `Tensor.backward()`."",
+            hints=[*graph_break_hints.FUNDAMENTAL],
+        )
 
     def method_data_ptr(self, *args, **kwargs):
         return DataPtrVariable(self)
 
     def method_item(self, *args, **kwargs):
         if not config.capture_scalar_outputs:
             self._warn_capture_scalar_outputs()
-            unimplemented(""Tensor.item"")
+            unimplemented_v2(
+                gb_type=""Unsupported Tensor.item() call with capture_scalar_outputs=False"",
+                context=f""call_method {self} item {args} {kwargs}"",
+                explanation=""Dynamo does not support tracing `Tensor.item()` ""
+                ""with config.capture_scalar_outputs=False."",
+                hints=[
+                    ""Set `torch._dynamo.config.capture_scalar_outputs = True` ""
+                    ""or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` ""
+                    ""to include these operations in the captured graph."",
+                ],
+            )
 
     def method___getitem__(self, *args, **kwargs):
         from ..symbolic_convert import InstructionTranslator
@@ -1013,16 +1099,36 @@ def has_bool_key(v):
         return ConstantVariable.create(None)
 
     def method_resize_(self, *args, **kwargs):
-        unimplemented(""Tensor.resize_"")
+        unimplemented_v2(
+            gb_type=""Unsupported Tensor.resize_() call"",
+            context=f""call_method {self} resize_ {args} {kwargs}"",
+            explanation=""Dynamo currently does not support tracing `Tensor.resize_()`."",
+            hints=[],
+        )
 
     def method_resize_as_(self, *args, **kwargs):
-        unimplemented(""Tensor.resize_as_"")
+        unimplemented_v2(
+            gb_type=""Unsupported Tensor.resize_as_() call"",
+            context=f""call_method {self} resize_as_ {args} {kwargs}"",
+            explanation=""Dynamo currently does not support tracing `Tensor.resize_as_()`."",
+            hints=[],
+        )
 
     def method_sparse_resize_(self, *args, **kwargs):
-        unimplemented(""Tensor.sparse_resize_"")
+        unimplemented_v2(
+            gb_type=""Unsupported Tensor.sparse_resize_() call"",
+            context=f""call_method {self} sparse_resize_ {args} {kwargs}"",
+            explanation=""Dynamo currently does not support tracing `Tensor.sparse_resize_()`."",
+            hints=[],
+        )
 
     def method_sparse_resize_and_clear_(self, *args, **kwargs):
-        unimplemented(""Tensor.sparse_resize_and_clear_"")
+        unimplemented_v2(
+            gb_type=""Unsupported Tensor.sparse_resize_and_clear_() call"",
+            context=f""call_method {self} sparse_resize_and_clear_ {args} {kwargs}"",
+            explanation=""Dynamo currently does not support tracing `Tensor.sparse_resize_and_clear_()`."",
+            hints=[],
+        )
 
     def method_set_(self, *args, **kwargs):
         if len(args) > 1:
@@ -1032,7 +1138,13 @@ def method_set_(self, *args, **kwargs):
             # overload and is used by FSDP.
             # graph-breaking on aten::set_source_Tensor_storage_offset for now,
             # unless we find that we need to make it work.
-            unimplemented(""Tensor.set_.source_Tensor_storage_offset"")
+            unimplemented_v2(
+                gb_type=""Unsupported Tensor.set_() call"",
+                context=f""call_method {self} set_ {args} {kwargs}"",
+                explanation=""Dynamo currently does not support tracing `Tensor.set_()` ""
+                ""overloads that include more than one argument."",
+                hints=[*graph_break_hints.SUPPORTABLE],
+            )
 
     def method_add_(self, other, *, alpha=None):
         if alpha is not None:
@@ -1158,8 +1270,11 @@ def _method_register_hook(self, name: str, hook: VariableTracker):
                 # would have no recourse - their forward traces just fine, but will fail at backwards unless
                 # compiled_autograd is enabled. If compiled_autograd fails (there are a lot of failures today)
                 # then they have nothing they can do except disable compile.
-                unimplemented(
-                    ""Compilation of intermediate hooks requires compiled autograd""
+                unimplemented_v2(
+                    gb_type=""Compilation of intermediate hooks requires compiled autograd"",
+                    context=f""var_getattr {self} {name}"",
+                    explanation=""Dynamo must be in compiled_autograd to register hooks."",
+                    hints=[],
                 )
 
             hook_name, bw_state_proxy = tx.output.add_backward_state_hook(hook)
@@ -1205,7 +1320,13 @@ def method_requires_grad_(self, requires_grad=True):
             requires_grad = requires_grad.as_python_constant()
 
         if self.as_proxy().node.meta[""example_value""].requires_grad != requires_grad:
-            unimplemented(""Tensor.requires_grad_"")
+            unimplemented_v2(
+                gb_type=""Unsupported Tensor.requires_grad_() call"",
+                context=f""call_method {self} requires_grad_"",
+                explanation=""Dynamo does not support changes to a Tensor's ""
+                ""`requires_grad` through calling `requires_grad_()`."",
+                hints=[],
+            )
         else:
             return self
 
@@ -1391,9 +1512,19 @@ def insert_into_graph():
                 return ConstantVariable.create(int(r))
             return insert_into_graph()
         elif name in [""base"", ""flags"", ""dtype""]:
-            unimplemented(f""TODO: add support for ndarray.{name}"")
+            unimplemented_v2(
+                gb_type=""Unsupported ndarray attribute access"",
+                context=f""var_getattr {self} {name}"",
+                explanation=f""Dynamo currently does not support tracing `ndarray.{name}`."",
+                hints=[],
+            )
         elif name in [""__version__""]:
-            unimplemented(""delegate np.__version__ to NumPy"")
+            unimplemented_v2(
+                gb_type=""Unsupported ndarray.__version__ access"",
+                context=f""var_getattr {self} {name}"",
+                explanation=f""Dynamo currently does not support tracing `ndarray.{name}`."",
+                hints=[],
+            )
         if result is None:
             raise NotImplementedError
         return result
@@ -1420,7 +1551,12 @@ def call_method(
             # delegate back to TensorVariable
             return super().call_method(tx, name, args, kwargs)
         if name in (""tostring"", ""tobytes"", ""__delattr__""):
-            unimplemented(f""{name} is not modelled in torch._numpy"")
+            unimplemented_v2(
+                gb_type=""Unsupported ndarray method call"",
+                context=f""call_method {self} {name} {args} {kwargs}"",
+                explanation=f""`ndarray.{name}()` is not modelled in `torch._numpy`."",
+                hints=[],
+            )
         proxy = tx.output.create_proxy(
             ""call_function"",
             numpy_method_wrapper(name),",,py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\before\torch__dynamo_variables_tensor.py,.\tmp\github_file_blobs\pytorch\pytorch\9968c854b6a38857d05fb1da7bea797fbf23c880\after\torch__dynamo_variables_tensor.py,{}
028e5546-0ba6-4467-8ab1-0ab9c11a3b9b,e456da74-5ded-4603-87ae-c8327be73bfa,torch/distributed/elastic/agent/server/api.py,modified,3,1,4,"@@ -71,7 +71,8 @@ class WorkerSpec:
         tee: tees the specified std stream(s) to console + file,
              selectively tee for a particular local rank by passing a map,
              takes precedence over ``redirects`` settings.
-
+        event_log_handler: name of the event logging handler as registered in
+          `elastic/events/handlers.py <https://docs.pytorch.org/docs/stable/elastic/events.html>`_.
     """"""
 
     role: str
@@ -86,6 +87,7 @@ class WorkerSpec:
     master_port: Optional[int] = None
     master_addr: Optional[str] = None
     local_addr: Optional[str] = None
+    event_log_handler: str = ""null""
 
     def __post_init__(self):
         assert self.local_world_size > 0",,py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\before\torch_distributed_elastic_agent_server_api.py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\after\torch_distributed_elastic_agent_server_api.py,{}
4b075ef1-69a9-46de-9165-8f022da9afc6,e456da74-5ded-4603-87ae-c8327be73bfa,torch/distributed/launcher/api.py,modified,19,12,31,"@@ -64,6 +64,9 @@ class LaunchConfig:
         local_addr: address of the local node if any. If not set, a lookup on the local
                 machine's FQDN will be performed.
         local_ranks_filter: ranks for which to show logs in console. If not set, show from all.
+        event_log_handler: name of the event logging handler as registered in
+          `elastic/events/handlers.py <https://docs.pytorch.org/docs/stable/elastic/events.html>`_.
+
 
     .. note::
         `rdzv_timeout` is a legacy argument that will be removed in future.
@@ -87,6 +90,7 @@ class LaunchConfig:
     log_line_prefix_template: Optional[str] = None
     metrics_cfg: dict[str, str] = field(default_factory=dict)
     local_addr: Optional[str] = None
+    event_log_handler: str = ""null""
 
     def __post_init__(self):
         default_timeout = 900
@@ -194,18 +198,19 @@ def launch_agent(
 
     logger.info(
         ""Starting elastic_operator with launch configs:\n""
-        ""  entrypoint       : %(entrypoint)s\n""
-        ""  min_nodes        : %(min_nodes)s\n""
-        ""  max_nodes        : %(max_nodes)s\n""
-        ""  nproc_per_node   : %(nproc_per_node)s\n""
-        ""  run_id           : %(run_id)s\n""
-        ""  rdzv_backend     : %(rdzv_backend)s\n""
-        ""  rdzv_endpoint    : %(rdzv_endpoint)s\n""
-        ""  rdzv_configs     : %(rdzv_configs)s\n""
-        ""  max_restarts     : %(max_restarts)s\n""
-        ""  monitor_interval : %(monitor_interval)s\n""
-        ""  log_dir          : %(log_dir)s\n""
-        ""  metrics_cfg      : %(metrics_cfg)s\n"",
+        ""  entrypoint         : %(entrypoint)s\n""
+        ""  min_nodes          : %(min_nodes)s\n""
+        ""  max_nodes          : %(max_nodes)s\n""
+        ""  nproc_per_node     : %(nproc_per_node)s\n""
+        ""  run_id             : %(run_id)s\n""
+        ""  rdzv_backend       : %(rdzv_backend)s\n""
+        ""  rdzv_endpoint      : %(rdzv_endpoint)s\n""
+        ""  rdzv_configs       : %(rdzv_configs)s\n""
+        ""  max_restarts       : %(max_restarts)s\n""
+        ""  monitor_interval   : %(monitor_interval)s\n""
+        ""  log_dir            : %(log_dir)s\n""
+        ""  metrics_cfg        : %(metrics_cfg)s\n""
+        ""  event_log_handler  : %(event_log_handler)s\n"",
         {
             ""entrypoint"": entrypoint_name,
             ""min_nodes"": config.min_nodes,
@@ -219,6 +224,7 @@ def launch_agent(
             ""monitor_interval"": config.monitor_interval,
             ""log_dir"": config.logs_specs.root_log_dir,  # type: ignore[union-attr]
             ""metrics_cfg"": config.metrics_cfg,
+            ""event_log_handler"": config.event_log_handler,
         },
     )
 
@@ -245,6 +251,7 @@ def launch_agent(
         master_addr=master_addr,
         master_port=master_port,
         local_addr=config.local_addr,
+        event_log_handler=config.event_log_handler,
     )
 
     agent = LocalElasticAgent(",,py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\before\torch_distributed_launcher_api.py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\after\torch_distributed_launcher_api.py,{}
0e3cd495-905c-4b02-8dff-49fe8cef4496,e456da74-5ded-4603-87ae-c8327be73bfa,torch/distributed/run.py,modified,9,0,9,"@@ -486,6 +486,14 @@ def get_args_parser() -> ArgumentParser:
         choices=[""spawn"", ""fork"", ""forkserver""],
         help=""Multiprocessing start method to use when creating workers."",
     )
+    parser.add_argument(
+        ""--event-log-handler"",
+        ""--event_log_handler"",
+        action=env,
+        type=str,
+        default=""null"",
+        help=""name of a registered event logging handler (see: https://docs.pytorch.org/docs/stable/elastic/events.html)"",
+    )
     parser.add_argument(
         ""--role"",
         action=env,
@@ -817,6 +825,7 @@ def config_from_args(args) -> tuple[LaunchConfig, Union[Callable, str], list[str
         log_line_prefix_template=log_line_prefix_template,
         local_addr=args.local_addr,
         logs_specs=logs_specs,
+        event_log_handler=args.event_log_handler,
     )
 
     with_python = not args.no_python",,py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\before\torch_distributed_run.py,.\tmp\github_file_blobs\pytorch\pytorch\e15848669f84d3767bfca724a29a6a6dde3308b9\after\torch_distributed_run.py,{}
82ac2fd5-547d-4683-b46c-a1a8c54b6451,847202ce-06c3-44a4-a35f-c6fb174ad3b3,torch/onnx/_internal/exporter/_core.py,modified,7,0,7,"@@ -619,10 +619,17 @@ def _handle_call_function_node_with_lowering(
         node_name_to_values[node.name] = outputs
         for i, output in enumerate(outputs):
             output.name = f""{node.name}__{i}""
+            # Set the name of the producing node using the value name for correspondence
+            producer = output.producer()
+            if producer is not None:
+                producer.name = f""node_{output.name}""
     else:
         _set_shape_type(outputs, node.meta[""val""], complex_to_float=True)
         node_name_to_values[node.name] = outputs
         outputs.name = node.name
+        producer = outputs.producer()
+        if producer is not None:
+            producer.name = f""node_{outputs.name}""
 
     for ir_node in onnx_nodes:
         ir_node.meta[""node""] = node",,py,.\tmp\github_file_blobs\pytorch\pytorch\79aef141695f2daea4a9aeb0f385726c5794a242\before\torch_onnx__internal_exporter__core.py,.\tmp\github_file_blobs\pytorch\pytorch\79aef141695f2daea4a9aeb0f385726c5794a242\after\torch_onnx__internal_exporter__core.py,{}
e348d532-3c2f-433a-a3f8-518377f252d1,a7a28780-63bb-42f7-9103-66771c103b12,test/profiler/test_execution_trace.py,modified,0,7,7,"@@ -15,7 +15,6 @@
 
 import json
 import os
-import sys
 import tempfile
 import unittest
 from typing import Any
@@ -365,9 +364,6 @@ def test_execution_trace_env_disabled(self, device):
         self.assertTrue(p.execution_trace_observer is None)
 
     @unittest.skipIf(IS_WINDOWS, ""torch.compile does not support WINDOWS"")
-    @unittest.skipIf(
-        sys.version_info >= (3, 12), ""torch.compile is not supported on python 3.12+""
-    )
     @unittest.skipIf(
         (not has_triton()) or (not TEST_CUDA and not TEST_XPU),
         ""need triton and device(CUDA or XPU) availability to run"",
@@ -419,9 +415,6 @@ def fn(a, b, c):
         assert found_captured_triton_kernel_node
 
     @unittest.skipIf(IS_WINDOWS, ""torch.compile does not support WINDOWS"")
-    @unittest.skipIf(
-        sys.version_info >= (3, 12), ""torch.compile is not supported on python 3.12+""
-    )
     @unittest.skipIf(
         (not has_triton()) or (not TEST_CUDA and not TEST_XPU),
         ""need triton and device(CUDA or XPU) availability to run"",",,py,.\tmp\github_file_blobs\pytorch\pytorch\b9b84d8011b08ac62cabf9100043c65863372fea\before\test_profiler_test_execution_trace.py,.\tmp\github_file_blobs\pytorch\pytorch\b9b84d8011b08ac62cabf9100043c65863372fea\after\test_profiler_test_execution_trace.py,{}
9e7a2de7-5ffb-461c-b0d5-fa4a30113cbd,a7a28780-63bb-42f7-9103-66771c103b12,torch/csrc/profiler/standalone/execution_trace_observer.cpp,modified,39,2,41,"@@ -113,6 +113,41 @@ struct TORCH_API ExecutionTraceObserver { // NOLINT
   // Uses the underlying TensorImpl object pointer as the key and map to its
   // unique id.
   std::map<const void*, ID> objectId{};
+
+  using weak_storage_ptr = c10::weak_intrusive_ptr<StorageImpl>;
+  std::unordered_map<const void*, ID> data_ptr_to_storage_id{};
+  std::unordered_map<const void*, weak_storage_ptr>
+      data_ptr_to_weak_storage_ptr{};
+
+  ID get_tensor_storage_ID(const c10::Storage& t_storage) {
+    const std::lock_guard<std::recursive_mutex> lock(gMutex);
+
+    const void* raw_data_ptr = t_storage.data();
+    auto iter = data_ptr_to_weak_storage_ptr.find(raw_data_ptr);
+    if (iter == data_ptr_to_weak_storage_ptr.end()) {
+      ID id = storage_id_++;
+      data_ptr_to_storage_id.emplace(raw_data_ptr, id);
+      data_ptr_to_weak_storage_ptr.emplace(
+          raw_data_ptr, t_storage.getWeakStorageImpl());
+      return id;
+    } else {
+      // check if the storage is still alive
+      if (iter->second.expired()) {
+        ID id = storage_id_++;
+        // std::unorder_map does not change if the key is already in the map.
+        // So we need to remove the key and insert the key with the new value.
+        data_ptr_to_storage_id.erase(raw_data_ptr);
+        data_ptr_to_storage_id[raw_data_ptr] = id;
+        data_ptr_to_weak_storage_ptr.erase(raw_data_ptr);
+        data_ptr_to_weak_storage_ptr.emplace(
+            raw_data_ptr, t_storage.getWeakStorageImpl());
+        return id;
+      } else {
+        return data_ptr_to_storage_id[raw_data_ptr];
+      }
+    }
+  }
+
   // Observer run state.
   enum class RunState { uninitialized, disabled, enabled };
 
@@ -175,6 +210,8 @@ struct TORCH_API ExecutionTraceObserver { // NOLINT
   // 1 -> root ID
   // 2 ... -> regular node ID
   std::atomic<ID> id_{2};
+
+  std::atomic<ID> storage_id_{1};
 };
 
 // Using a singleton manager here to allow init and delete the observer object.
@@ -445,8 +482,8 @@ convertIValue(
     // symbolic sizes/strides implies t->storage_offset() will fail
     if (tensor_impl->has_storage() &&
         !tensor_impl->has_symbolic_sizes_strides()) {
-      auto& t_storage = tensor_impl->storage();
-      storage_id = getObjectID(ob, t_storage.data());
+      const c10::Storage& t_storage = tensor_impl->storage();
+      storage_id = ob.get_tensor_storage_ID(t_storage);
       offset = tensor_impl->storage_offset();
       numel = tensor_impl->numel();
       itemsize = tensor_impl->itemsize();",,cpp,.\tmp\github_file_blobs\pytorch\pytorch\b9b84d8011b08ac62cabf9100043c65863372fea\before\torch_csrc_profiler_standalone_execution_trace_observer.cpp,.\tmp\github_file_blobs\pytorch\pytorch\b9b84d8011b08ac62cabf9100043c65863372fea\after\torch_csrc_profiler_standalone_execution_trace_observer.cpp,{}
4b12ebcc-34c0-425c-8913-434f6a354c40,56953a6b-8003-4930-a55f-d0500a0e4382,third_party/xpu.txt,modified,1,1,2,"@@ -1 +1 @@
-4e027f1e1c560d7dc7db7eb41e48bdee5fc00707
+a3a196ccdbcbc399e157b6bcf8f5611e6561b6d6",,txt,.\tmp\github_file_blobs\pytorch\pytorch\4a4cac0cefea3661cc69cfdafdba64832ee0841a\before\third_party_xpu.txt,.\tmp\github_file_blobs\pytorch\pytorch\4a4cac0cefea3661cc69cfdafdba64832ee0841a\after\third_party_xpu.txt,{}
55612ddd-6a46-4fb6-ab88-f9bb4012de4c,7405ad25-de98-4420-a21e-60d8bd7aaaeb,torch/_dynamo/exc.py,modified,1,0,1,"@@ -404,6 +404,7 @@ def handle_observed_exception(tx: Any) -> None:
     torch._subclasses.fake_tensor.DynamicOutputShapeException,
     torch._subclasses.fake_tensor.UnsupportedOperatorException,
     torch._subclasses.fake_tensor.UnsupportedFakeTensorException,
+    torch._subclasses.fake_tensor.UnsupportedMutationAliasingException,
 )
 
 ",,py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\before\torch__dynamo_exc.py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\after\torch__dynamo_exc.py,{}
d54ac89a-9658-453d-bda3-82b84007ec50,7405ad25-de98-4420-a21e-60d8bd7aaaeb,torch/_functorch/_aot_autograd/input_output_analysis.py,modified,15,0,15,"@@ -300,6 +300,21 @@ def compute_overlapping_inputs(aot_config, fwd_inputs, aliased_input_indices):
         ]
     )
 
+    if torch._inductor.config.is_fbcode():
+        if symbolic and num_aliases > 400:
+            from torch._subclasses.fake_tensor import (
+                UnsupportedMutationAliasingException,
+            )
+            from torch._utils_internal import justknobs_check
+
+            msg = f""Encountered {num_aliases} dynamic, aliased/mutated inputs, consider setting dynamic=False""
+
+            if justknobs_check(
+                ""pytorch/compiler:aliased_inputs_with_mutation_and_dyn_shapes_killswitch"",
+                False,
+            ):
+                raise UnsupportedMutationAliasingException(msg)
+
     with maybe_suppress_guards():
         aliased_fwd_inputs = [fwd_inputs[i] for i in aliased_input_indices]
         actual_aliased_indices = {",,py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\before\torch__functorch__aot_autograd_input_output_analysis.py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\after\torch__functorch__aot_autograd_input_output_analysis.py,{}
9a6c5d9a-54ff-4da7-bd9a-71d3134609f2,7405ad25-de98-4420-a21e-60d8bd7aaaeb,torch/_subclasses/fake_tensor.py,modified,5,0,5,"@@ -121,6 +121,11 @@ class UnsupportedOperatorException(RuntimeError):
     func: OpOverload
 
 
+@dataclass
+class UnsupportedMutationAliasingException(RuntimeError):
+    reason: str
+
+
 @dataclass
 class MetadataMismatchError(RuntimeError):
     reason: str",,py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\before\torch__subclasses_fake_tensor.py,.\tmp\github_file_blobs\pytorch\pytorch\6c05f2fca049344fbbb55eed7a6c866f34c8a477\after\torch__subclasses_fake_tensor.py,{}
eb68778d-84e7-42eb-8872-53263ffc685b,64838e6c-ff70-4a8c-beac-288f5dfbfc83,test/functorch/test_aotdispatch.py,modified,37,0,37,"@@ -7779,6 +7779,43 @@ def run(f):
 
         self.assertEqual(out, optout)
 
+    def test_mutations_in_bw_detached_from_tangent(self):
+        class AF(torch.autograd.Function):
+            @staticmethod
+            def forward(ctx, dummy, inplace_tensor):
+                ctx.inplace_tensor = inplace_tensor
+                return dummy.clone()
+
+            @staticmethod
+            def backward(ctx, grad_output):
+                inplace_tensor = ctx.inplace_tensor
+                gradient_attachment = grad_output * 0 + 1
+                inplace_tensor.add_(1 * gradient_attachment)
+                return grad_output, None, None
+
+        def fn(dummy, inplace_tensor):
+            return AF.apply(dummy, inplace_tensor)
+
+        def _inps():
+            dummy = torch.zeros((2,), requires_grad=True)
+            inplace_tensor = torch.zeros((2,), requires_grad=False)
+            return dummy, inplace_tensor
+
+        inps = _inps()
+        out = fn(*inps)
+        ref_inps_after_fw = [x.clone().detach() for x in inps]
+        out.sum().backward()
+        ref_inps_after_bw = [x.clone().detach() for x in inps]
+
+        inps = _inps()
+        out = torch.compile(fn, backend=""aot_eager"", fullgraph=True)(*inps)
+        inps_after_fw = [x.clone().detach() for x in inps]
+        out.sum().backward()
+        inps_after_bw = [x.clone().detach() for x in inps]
+
+        self.assertEqual(ref_inps_after_fw, inps_after_fw)
+        self.assertEqual(ref_inps_after_bw, inps_after_bw)
+
 
 class MockFXGraphCache:
     """"""",,py,.\tmp\github_file_blobs\pytorch\pytorch\0083032e7559dc8f02483ba60373adfcdaf9dae6\before\test_functorch_test_aotdispatch.py,.\tmp\github_file_blobs\pytorch\pytorch\0083032e7559dc8f02483ba60373adfcdaf9dae6\after\test_functorch_test_aotdispatch.py,{}
60c86052-de42-4b49-a5ca-d0d09c1bd16c,64838e6c-ff70-4a8c-beac-288f5dfbfc83,torch/_functorch/partitioners.py,modified,5,0,5,"@@ -1141,6 +1141,11 @@ def insert_node_in_graph(node):
         return gm
 
     # Build the graph op-by-op by starting from the node all the way to the end
+    # copy_ can be not using tangents at all, we must copy it.
+    for node in list(gm.graph.nodes)[: order[first_node_in_bwd]]:
+        if node.op == ""call_function"" and node.target == torch.ops.aten.copy_.default:
+            insert_node_in_graph(node)
+
     for node in list(gm.graph.nodes)[order[first_node_in_bwd] :]:
         insert_node_in_graph(node)
 ",,py,.\tmp\github_file_blobs\pytorch\pytorch\0083032e7559dc8f02483ba60373adfcdaf9dae6\before\torch__functorch_partitioners.py,.\tmp\github_file_blobs\pytorch\pytorch\0083032e7559dc8f02483ba60373adfcdaf9dae6\after\torch__functorch_partitioners.py,{}
52a99f69-2a94-4adf-b77b-bf271b323a77,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,test/inductor/test_codecache.py,modified,1,42,43,"@@ -27,11 +27,7 @@
     TensorMetadata,
     TensorMetadataAndValues,
 )
-from torch._inductor.custom_graph_pass import (
-    CustomGraphModulePass,
-    CustomGraphPass,
-    get_hash_for_files,
-)
+from torch._inductor.custom_graph_pass import CustomGraphPass, get_hash_for_files
 from torch._inductor.graph import GraphLowering
 from torch._inductor.mock_cache import global_stats, PatchCaches, Stats
 from torch._inductor.runtime.runtime_utils import cache_dir
@@ -57,7 +53,6 @@
     HAS_GPU,
     HAS_MULTIGPU,
     HAS_TRITON,
-    patch_inductor_backend,
     requires_gpu,
     requires_triton,
 )
@@ -2188,42 +2183,6 @@ def uuid(self) -> Optional[Union[bytes, str]]:
                 pickler.dumps(details3),
             )
 
-    def test_hash_custom_backend_pass(self):
-        """"""
-        Test CustomGraphModulePass usage.
-        """"""
-
-        class TestCustomGraphModulePass(CustomGraphModulePass):
-            def __init__(self):
-                self._uuid = None
-
-            def __call__(self, gm: torch.fx.GraphModule) -> None:
-                return None
-
-            def uuid(self) -> Optional[Union[bytes, str]]:
-                return self._uuid
-
-        custom_pass = TestCustomGraphModulePass()
-        with patch_inductor_backend(""cpu"", custom_pass=custom_pass):
-            custom_pass._uuid = ""1""
-            details1 = FxGraphHashDetails(None, [], {}, [])
-            details2 = FxGraphHashDetails(None, [], {}, [])
-
-            custom_pass._uuid = ""2""
-            details3 = FxGraphHashDetails(None, [], {}, [])
-
-            gm = torch.fx.GraphModule({}, torch.fx.Graph())
-            pickler = FxGraphCachePickler(gm)
-
-            self.assertEqual(
-                pickler.dumps(details1),
-                pickler.dumps(details2),
-            )
-            self.assertNotEqual(
-                pickler.dumps(details1),
-                pickler.dumps(details3),
-            )
-
     def test_bypass_unsupported(self):
         """"""
         Test _reduce_unsupported",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\test_inductor_test_codecache.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\test_inductor_test_codecache.py,{}
bf19256d-121d-47aa-bd29-cebc42e39144,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,test/inductor/test_custom_post_grad_passes.py,modified,2,36,38,"@@ -8,17 +8,12 @@
 import torch.fx as fx
 from torch._dynamo.utils import counters
 from torch._inductor import config
-from torch._inductor.codegen.common import get_custom_backend_pass_for_device
-from torch._inductor.custom_graph_pass import (
-    CustomGraphModulePass,
-    CustomGraphPass,
-    get_hash_for_files,
-)
+from torch._inductor.custom_graph_pass import CustomGraphPass, get_hash_for_files
 from torch._inductor.lowering import lowerings as L
 from torch._inductor.pattern_matcher import Arg, CallFunction, PatternMatcherPass
 from torch._inductor.test_case import run_tests, TestCase
 from torch.testing._internal.common_utils import IS_LINUX
-from torch.testing._internal.inductor_utils import HAS_CPU, patch_inductor_backend
+from torch.testing._internal.inductor_utils import HAS_CPU
 
 
 @config.patch({""freezing"": True})
@@ -269,35 +264,6 @@ def f(W, nested_seqs):
 
         inner_test()
 
-    def test_custom_backend_pass(self):
-        class CustomBackendPass(CustomGraphModulePass):
-            def __init__(self, existing_pass: CustomGraphModulePass = None):
-                super().__init__()
-                self.existing_pass = existing_pass
-
-            def __call__(self, gm: fx.GraphModule) -> None:
-                if self.existing_pass:
-                    self.existing_pass(gm)
-
-                change_cos_pass(gm.graph)
-
-            def uuid(self) -> bytes:
-                return get_hash_for_files((__file__,))
-
-        custom_backend_pass = CustomBackendPass(
-            get_custom_backend_pass_for_device(""cpu"")
-        )
-        with patch_inductor_backend(""cpu"", custom_pass=custom_backend_pass):
-
-            def g(x):
-                return x.sin().sin().sin()
-
-            def f(x):
-                return x.cos().cos().cos()
-
-            x = torch.randn(8, dtype=torch.float32)
-            torch.testing.assert_close(torch.compile(f)(x), g(x))
-
 
 if __name__ == ""__main__"":
     if IS_LINUX and HAS_CPU and torch.backends.mkldnn.is_available():",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\test_inductor_test_custom_post_grad_passes.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\test_inductor_test_custom_post_grad_passes.py,{}
b7ba9429-010f-4596-8572-d50b20fe2065,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,test/inductor/test_torchinductor_dynamic_shapes.py,modified,14,7,21,"@@ -12,6 +12,8 @@
 import torch.library
 from torch._dynamo.testing import CompileCounterWithBackend, make_test_cls_with_patches
 from torch._inductor import metrics
+from torch._inductor.codegen.common import device_codegens, register_backend_for_device
+from torch._inductor.codegen.cpp import CppScheduling
 from torch._inductor.codegen.wrapper import PythonWrapperCodegen
 from torch._inductor.test_case import TestCase
 from torch._inductor.utils import run_and_get_code
@@ -32,12 +34,7 @@
     TEST_WITH_ASAN,
     TEST_WITH_ROCM,
 )
-from torch.testing._internal.inductor_utils import (
-    GPU_TYPE,
-    HAS_CPU,
-    HAS_GPU,
-    patch_inductor_backend,
-)
+from torch.testing._internal.inductor_utils import GPU_TYPE, HAS_CPU, HAS_GPU
 
 
 # Make the helper files in test/ importable
@@ -935,13 +932,23 @@ def generate(self, is_inference, *args, **kwargs):
                 _test_wrapper_codegen_statically_known_int_or_none_in_context()
                 return super().generate(is_inference, *args, **kwargs)
 
-        with patch_inductor_backend(""cpu"", python_wrapper_codegen=TestWrapperCodegen):
+        if ""cpu"" not in device_codegens:
+            register_backend_for_device(""cpu"", CppScheduling, PythonWrapperCodegen)
+        orig_cpu_codegens = device_codegens[""cpu""]
+        try:
+            register_backend_for_device(
+                ""cpu"", orig_cpu_codegens.scheduling, TestWrapperCodegen
+            )
             # Compile each of the functions above, with an example input
             # that has 5 in the first dimension, but is marked as dynamic
 
             torch.compile(backend=""inductor"", dynamic=None)(fn_1)(_x)
             torch.compile(backend=""inductor"", dynamic=None)(fn_2)(_x)
             torch.compile(backend=""inductor"", dynamic=None)(fn_3)(_x)
+        finally:
+            register_backend_for_device(
+                ""cpu"", orig_cpu_codegens.scheduling, orig_cpu_codegens.wrapper_codegen
+            )
 
     @torch._dynamo.config.patch(capture_scalar_outputs=True)
     def test_item_unbacked_stride_nobreak(self, device):",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\test_inductor_test_torchinductor_dynamic_shapes.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\test_inductor_test_torchinductor_dynamic_shapes.py,{}
5a361331-88cd-4166-8e82-51a020a3cae9,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/codecache.py,modified,3,12,15,"@@ -51,7 +51,6 @@
 from torch._dynamo.exc import SkipFrame
 from torch._dynamo.utils import CompileEventLogger, counters, dynamo_timed
 from torch._inductor import config, exc, metrics
-from torch._inductor.codegen.common import custom_backend_passes
 from torch._inductor.codegen.cuda import cuda_env
 from torch._inductor.codegen.rocm.compile_command import (
     rocm_compile_command,
@@ -73,11 +72,7 @@
     normalize_path_separator,
 )
 from torch._inductor.cpu_vec_isa import pick_vec_isa
-from torch._inductor.custom_graph_pass import (
-    CustomGraphModulePass,
-    CustomGraphPass,
-    CustomGraphPassType,
-)
+from torch._inductor.custom_graph_pass import CustomGraphPass, CustomGraphPassType
 from torch._inductor.freezing_utils import has_frozen_params, is_frozen_param
 from torch._inductor.runtime.compile_tasks import _reload_python_module
 from torch._inductor.runtime.runtime_utils import cache_dir, default_cache_dir
@@ -896,16 +891,12 @@ def __init__(
             config.post_grad_custom_post_pass
         )
 
-        self.custom_backend_passes = tuple(
-            map(self._get_custom_pass_detail, custom_backend_passes.values())
-        )
-
     def _get_custom_pass_detail(
-        self, custom_pass: Union[CustomGraphPassType, CustomGraphModulePass]
+        self, custom_pass: CustomGraphPassType
     ) -> Optional[Any]:
         if not custom_pass:
             return None
-        assert isinstance(custom_pass, (CustomGraphPass, CustomGraphModulePass))
+        assert isinstance(custom_pass, CustomGraphPass)
         return custom_pass.uuid()
 
 ",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_codecache.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_codecache.py,{}
0ab81a03-6e28-423b-a1a4-b969bda0b7ab,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/codegen/common.py,modified,0,8,8,"@@ -65,7 +65,6 @@
 
     from torch.fx import GraphModule
 
-    from ..custom_graph_pass import CustomGraphModulePass
     from ..ir import Buffer, ChoiceCaller, FixedLayout, IRNode
     from ..loop_body import LoopBody
     from ..scheduler import BaseScheduling, Scheduler, SchedulerNode
@@ -352,7 +351,6 @@ def cpp_global_scratch(self, idx: int) -> Optional[tuple[str, str]]:
 
 
 device_op_overrides_dict: dict[str, DeviceOpOverrides] = {}
-custom_backend_passes: dict[str, Optional[CustomGraphModulePass]] = {}
 
 
 # The code generated by Inductor consists of two main parts: kernel code and wrapper code.
@@ -381,12 +379,10 @@ def register_backend_for_device(
     device_scheduling: SchedulingConstructor,
     device_wrapper_codegen: WrapperConstructor,
     device_cpp_wrapper_codegen: Optional[WrapperConstructor] = None,
-    device_custom_pass: Optional[CustomGraphModulePass] = None,
 ) -> None:
     device_codegens[device] = DeviceCodegen(
         device_scheduling, device_wrapper_codegen, device_cpp_wrapper_codegen
     )
-    custom_backend_passes[device] = device_custom_pass
 
 
 class BackendFeature(Enum):
@@ -445,10 +441,6 @@ def get_wrapper_codegen_for_device(
     return None
 
 
-def get_custom_backend_pass_for_device(device: str) -> Optional[CustomGraphModulePass]:
-    return custom_backend_passes[device] if device in custom_backend_passes else None
-
-
 @functools.lru_cache(None)
 def init_backend_registration() -> None:
     from .cpp import CppScheduling",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_codegen_common.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_codegen_common.py,{}
a395f7ad-a57a-4261-9743-0bb980439d25,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/compile_fx.py,modified,16,1,17,"@@ -76,7 +76,6 @@
     BoxedBool,
     count_tangents,
     fresh_inductor_cache,
-    get_all_devices,
     InputType,
     is_gpu,
     should_assume_input_aligned,
@@ -1910,6 +1909,22 @@ def get_cpp_wrapper_config() -> dict[str, object]:
     }
 
 
+def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]:
+    placeholder_nodes = gm.graph.find_nodes(op=""placeholder"")
+    input_devices: OrderedSet[torch.device] = OrderedSet(
+        node.meta[""val""].device
+        for node in placeholder_nodes
+        if isinstance(node.meta.get(""val""), torch.Tensor)
+    )
+
+    out_devices: OrderedSet[torch.device] = OrderedSet(
+        arg.meta[""val""].device
+        for arg in output_node(gm).args[0]  # type: ignore[union-attr]
+        if isinstance(arg, fx.Node) and isinstance(arg.meta.get(""val""), torch.Tensor)
+    )
+    return input_devices | out_devices
+
+
 def get_cuda_device_context(gm: torch.fx.GraphModule) -> AbstractContextManager[None]:
     """"""
     Returns a cuda device context manager if there is a single device in the graph",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_compile_fx.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_compile_fx.py,{}
28645d49-6175-4eb8-bff0-7e33347aa64a,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/custom_graph_pass.py,modified,0,32,32,"@@ -53,38 +53,6 @@ def uuid(self) -> Optional[Any]:
         """"""
 
 
-class CustomGraphModulePass(ABC):
-    """"""
-    Implement this interface for custom Graph passes:
-
-    1) The __call__() method contains the implementation of the custom pass.
-
-    2) The uuid() method enables inductor to cache compiled graphs when your custom
-    passes are applied. This method can return any identifier as long as it uniquely
-    identifies your implementation (and can be pickled). The caching logic includes this
-    identifier in its key calculation, i.e., any new value will effectively invalidate
-    existing entries. We expect custom passes would typically depend purely on the
-    textual reprensentation of the implementation. In that case, we recommend using the
-    'get_hash_for_files' helper below to compute a unique hash from the contents of a
-    static list of source files, i.e., the source(s) containing the custom pass
-    implementation. That approach ensures that any change to the implementation will
-    mean a new uuid.
-    """"""
-
-    @abstractmethod
-    def __call__(self, gm: torch.fx.GraphModule) -> None:
-        """"""
-        Implementation of the custom pass.
-        """"""
-
-    @abstractmethod
-    def uuid(self) -> Optional[Any]:
-        """"""
-        Return an ID to uniquely identify your custom pass implementation. Return None
-        to skip inductor code caching entirely.
-        """"""
-
-
 CustomGraphPassType: TypeAlias = Optional[
     Union[CustomGraphPass, Callable[[torch.fx.graph.Graph], None]]
 ]",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_custom_graph_pass.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_custom_graph_pass.py,{}
f9774d7a-a123-4cf6-b24b-9d76d0125a39,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/fx_passes/post_grad.py,modified,0,9,9,"@@ -22,7 +22,6 @@
 from torch.utils._ordered_set import OrderedSet
 
 from .. import config, ir, pattern_matcher
-from ..codegen.common import custom_backend_passes
 from ..comms import remove_fsdp2_unsharded_param_graph_input_usage
 from ..fx_utils import FakeTensorUpdater, get_fake_args_kwargs, get_node_storage
 from ..lowering import lowerings as L
@@ -49,7 +48,6 @@
 )
 from ..utils import (
     decode_device,
-    get_all_devices,
     get_gpu_type,
     is_gpu,
     is_pointwise_use,
@@ -184,13 +182,6 @@ def post_grad_passes(gm: torch.fx.GraphModule, is_inference: bool):
 
     fake_tensor_updater.incremental_update()
 
-    for device, custom_backend_pass in custom_backend_passes.items():
-        if custom_backend_pass is not None:
-            gm_devices = [d.type for d in get_all_devices(gm)]
-            if device in gm_devices:
-                pass_name = ""custom_backend_passes_"" + device
-                GraphTransformObserver(gm, pass_name).apply_gm_pass(custom_backend_pass)
-
     # Keep these last, since they introduces mutation. Look at
     # ./fx_passes/README.md for a discussion of mutation invariants.
     GraphTransformObserver(gm, ""reinplace_inplaceable_ops"").apply_graph_pass(",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_fx_passes_post_grad.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_fx_passes_post_grad.py,{}
7e3c5066-6252-4801-bd80-b87cc9e1788e,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/_inductor/utils.py,modified,0,19,19,"@@ -994,25 +994,6 @@ def output_node(gm: torch.fx.GraphModule) -> Node:
     return last_node
 
 
-def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]:
-    placeholder_nodes = gm.graph.find_nodes(op=""placeholder"")
-    input_devices: OrderedSet[torch.device] = OrderedSet(
-        node.meta[""val""].device
-        for node in placeholder_nodes
-        if isinstance(node.meta.get(""val""), torch.Tensor)
-    )
-
-    out_arg = output_node(gm).args[0]  # type: ignore[union-attr]
-    out_args = out_arg if isinstance(out_arg, tuple) else (out_arg,)
-    out_devices: OrderedSet[torch.device] = OrderedSet(
-        arg.meta[""val""].device
-        for arg in out_args
-        if isinstance(arg, torch.fx.Node)
-        and isinstance(arg.meta.get(""val""), torch.Tensor)
-    )
-    return input_devices | out_devices
-
-
 _registered_caches: list[Any] = []
 
 ",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch__inductor_utils.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch__inductor_utils.py,{}
698eee48-a82f-4c10-a9c3-6837883a9a1b,9eda1e82-6747-42f6-afaf-43ff12ef0dd6,torch/testing/_internal/inductor_utils.py,modified,0,47,47,"@@ -14,15 +14,6 @@
 from torch._inductor.graph import GraphLowering
 from torch._inductor.compile_fx import shape_env_from_inputs
 from torch._inductor.codecache import CppCodeCache
-from torch._inductor.custom_graph_pass import CustomGraphModulePass
-from torch._inductor.codegen.common import (
-    get_custom_backend_pass_for_device,
-    get_scheduling_for_device,
-    get_wrapper_codegen_for_device,
-    init_backend_registration,
-    register_backend_for_device
-)
-from torch._inductor.codegen.wrapper import PythonWrapperCodegen
 from torch._inductor.utils import get_gpu_shared_memory, is_big_gpu
 from torch._inductor.utils import GPU_TYPES, get_gpu_type, is_gpu
 from torch.utils._triton import has_triton
@@ -299,41 +290,3 @@ def _quantize_rowwise(x: Tensor, float8_dtype: torch.dtype):
     x_fp8 = _to_fp8_saturated(x * scale, float8_dtype)
     inverse_scale = scale.reciprocal()
     return x_fp8, inverse_scale
-
-@contextlib.contextmanager
-def patch_inductor_backend(
-    device: str,
-    python_wrapper_codegen: PythonWrapperCodegen = None,
-    custom_pass: CustomGraphModulePass = None
-):
-    """"""
-    Patch the inductor backend for a specific device.
-    """"""
-    # Make sure the backend is already registered
-    init_backend_registration()
-
-    # Get the original registration parameters
-    original_scheduling = get_scheduling_for_device(device)
-    original_python_wrapper = get_wrapper_codegen_for_device(device, False)
-    original_cpp_wrapper = get_wrapper_codegen_for_device(device, True)
-    original_custom_pass = get_custom_backend_pass_for_device(device)
-
-    try:
-        # Register modified backend for the device
-        register_backend_for_device(
-            device,
-            original_scheduling,
-            python_wrapper_codegen if python_wrapper_codegen is not None else original_python_wrapper,
-            original_cpp_wrapper,
-            custom_pass if custom_pass is not None else original_custom_pass
-        )
-        yield
-    finally:
-        # Restore the original backend
-        register_backend_for_device(
-            device,
-            original_scheduling,
-            original_python_wrapper,
-            original_cpp_wrapper,
-            original_custom_pass
-        )",,py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\before\torch_testing__internal_inductor_utils.py,.\tmp\github_file_blobs\pytorch\pytorch\79bdafe5b61f6f073dcd276e135076ff3277a951\after\torch_testing__internal_inductor_utils.py,{}
